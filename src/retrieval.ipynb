{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !curl -X GET \"nersc-tbn-6.testbed100.es.net:9200/iperf3*/_search\" | jq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "**\n",
    "** Project Lead: Eashan Adhikarla\n",
    "** Mentor: Ezra Kissel\n",
    "** \n",
    "** Date Created: June 17' 2021\n",
    "** Last Modified: June 22' 2021 \n",
    "**\n",
    "'''\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Create the elasticsearch client\n",
    "HOST = 'nersc-tbn-6.testbed100.es.net'\n",
    "PORT = 9200\n",
    "\n",
    "es = Elasticsearch(host=HOST, port=PORT, timeout=50)\n",
    "\n",
    "\n",
    "class clr:\n",
    "    \"\"\"\n",
    "    Defining colors for the print syntax coloring\n",
    "    \"\"\"\n",
    "    H   = '\\033[35m' # Header\n",
    "    B   = '\\033[94m' # Blue\n",
    "    G   = '\\033[36m' # Green\n",
    "    W   = '\\033[93m' # Warning\n",
    "    F   = '\\033[91m' # Fail\n",
    "    E   = '\\033[0m'  # End \n",
    "    BD  = '\\033[1m'  # Bold\n",
    "    UL  = '\\033[4m'  # Underline\n",
    "\n",
    "\n",
    "class GETTER:\n",
    "    \"\"\"\n",
    "    Get class to get current information about different Indexs\n",
    "    \"\"\"\n",
    "    def __init__(self, term, sum=0):\n",
    "        self.term = term\n",
    "        self.sum = sum\n",
    "\n",
    "    def getIndexList(self, term):\n",
    "        idx=[]\n",
    "        indices_dict = es.indices.get_alias(self.term)\n",
    "        if isinstance(indices_dict, dict) and indices_dict is not None:\n",
    "            print(f\"'{clr.G}{len(indices_dict)}{clr.E}' indexes found!\")\n",
    "            for k,v in indices_dict.items():\n",
    "                idx.append(k)\n",
    "            return idx\n",
    "        else:\n",
    "            print (f\"{clr.F}Empty dict!{clr.E}\")\n",
    "\n",
    "\n",
    "    def getIndexedDetails(self, indexes):\n",
    "        for i in range(1): # len(indexes)):\n",
    "            try:\n",
    "                # Given a index name, finds all the documents in the index\n",
    "                # index: Index name as a string\n",
    "                # body: Empty equivalent to '*' to get all the values\n",
    "                # size: To some random max value for getting all the documents\n",
    "                result = es.search(index=indexes[i],\n",
    "                                body={\"query\":{\"match_all\":{}}},\n",
    "                                size=1000,\n",
    "                                )\n",
    "                print(f\"{indexes[i]} ---> {clr.G}{result['hits']['total']['value']}{clr.E} documents\\n\")\n",
    "                \n",
    "                documents = [doc for doc in result['hits']['hits']]\n",
    "                \n",
    "                for j in range(len(documents)):\n",
    "                    # ---------------------\n",
    "                    # For each job/document\n",
    "                    # ---------------------\n",
    "                    # Format: dict_keys(['end', '@version', 'start', 'uuid', 'intervals', '@timestamp'])\n",
    "                    # uuid\n",
    "                    uuid = documents[j]['_source']['uuid']\n",
    "                    # @timestamp\n",
    "                    timestamp = documents[j]['_source']['@timestamp']\n",
    "                    # start (Format: dict_keys(['cookie', 'test_start', 'tcp_mss_default', 'version', 'connected', 'sndbuf_actual',\n",
    "                    #                           'rcvbuf_actual', 'sock_bufsize', 'system_info', 'timestamp', 'connecting_to']))\n",
    "                    start_dict = documents[j]['_source']['start']\n",
    "                    num_streams = start_dict['test_start']['num_streams']\n",
    "\n",
    "                    # Congestion Type\n",
    "                    sender_congestion = documents[j]['_source']['end']['sender_tcp_congestion']\n",
    "                    receiver_congestion = documents[j]['_source']['end']['receiver_tcp_congestion']\n",
    "\n",
    "\n",
    "                    # print(f\"uuid: {uuid}\\ntimestamp: {timestamp}\\nnum_streams: {num_streams}\\n\\n\")\n",
    "\n",
    "                    # --------------------- \n",
    "                    # For each stream/flow\n",
    "                    # ---------------------\n",
    "                    # Intervals\n",
    "                    intervals_list = documents[j]['_source']['intervals']\n",
    "                    for k in range(len(intervals_list)):\n",
    "                        for m in range(num_streams):\n",
    "                            # Format: dict_keys(['receiver', 'sender'])\n",
    "                            # 'sender': dict_keys(['retransmits', 'max_rtt', 'sender', 'start', 'bytes', 'mean_rtt', 'end'\n",
    "                            #                      'max_snd_cwnd', 'bits_per_second', 'socket', 'seconds', 'min_rtt'])\n",
    "                            # 'receiver': dict_keys(['end', 'bits_per_second', 'sender', 'start', 'socket', 'seconds', 'bytes'])\n",
    "                            \n",
    "                            sender_start = documents[j]['_source']['end']['streams'][m]['sender']['start']\n",
    "                            sender_end = documents[j]['_source']['end']['streams'][m]['sender']['end']\n",
    "                            sender_retransmits = documents[j]['_source']['end']['streams'][m]['sender']['retransmits']\n",
    "                            sender_bytes = documents[j]['_source']['end']['streams'][m]['sender']['bytes']\n",
    "                            sender_min_rtt = documents[j]['_source']['end']['streams'][m]['sender']['min_rtt']\n",
    "                            sender_max_rtt = documents[j]['_source']['end']['streams'][m]['sender']['max_rtt']\n",
    "                            sender_mean_rtt = documents[j]['_source']['end']['streams'][m]['sender']['mean_rtt']\n",
    "                            sender_bps = documents[j]['_source']['end']['streams'][m]['sender']['bits_per_second']\n",
    "\n",
    "                            receiver_start = documents[j]['_source']['end']['streams'][m]['receiver']['start']\n",
    "                            receiver_end = documents[j]['_source']['end']['streams'][m]['receiver']['end']\n",
    "                            receiver_seconds = documents[j]['_source']['end']['streams'][m]['receiver']['seconds']\n",
    "                            receiver_bytes = documents[j]['_source']['end']['streams'][m]['receiver']['bytes']\n",
    "                            receiver_bps = documents[j]['_source']['end']['streams'][m]['receiver']['bits_per_second']\n",
    "\n",
    "\n",
    "                            print(f\"uuid: {uuid}\\ntimestamp: {timestamp}\\nnum_streams: {num_streams}\\nsender_congestion: {sender_congestion}\\nreceiver_congestion: {receiver_congestion}\\nsender_start: {sender_start}\\n sender_end: {sender_end}\\n sender_retransmits: {sender_retransmits}\\nsender_bytes: {sender_bytes}\\n sender_min_rtt: {sender_min_rtt}\\n sender_max_rtt: {sender_max_rtt}\\nsender_mean_rtt: {sender_mean_rtt}\\n sender_bps: {sender_bps}\\n receiver_start: {receiver_start}\\nreceiver_end: {receiver_end}\\n receiver_seconds: {receiver_seconds}\\n receiver_bytes: {receiver_bytes}\\nreceiver_bps: {receiver_bps}\\n\\n\")\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "            # print(\"\\nTotal docs found: \", self.sum)\n",
    "        \n",
    "        return None\n",
    "\n",
    "indexTypes = [\"*\", \"iperf*\", \"jobmeta*\", \"bbrmon*\"]\n",
    "term_ = indexTypes[0]\n",
    "\n",
    "get = GETTER(term_)\n",
    "indexes = get.getIndexList(term_)\n",
    "for e,i in enumerate(indexes):\n",
    "    print(f\"{e}: {i}\")\n",
    "\n",
    "# index_response = get.getIndexedDetails(indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_ind = sorted(indexes)\n",
    "sorted_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iperf3 = [i for i in sorted_ind if i.startswith(\"iperf3\")]\n",
    "jobmeta = [i for i in sorted_ind if i.startswith(\"jobmeta\")]\n",
    "ss = [i for i in sorted_ind if i.startswith(\"ss\")]\n",
    "bbrmon_pscheduler = [i for i in sorted_ind if i.startswith(\"bbrmon-pscheduler\")]\n",
    "bbrmon_jobmeta = [i for i in sorted_ind if i.startswith(\"bbrmon-jobmeta\")]\n",
    "bbrmon_ss = [i for i in sorted_ind if i.startswith(\"bbrmon-ss\")]\n",
    "bbrmon_tcptrace = [i for i in sorted_ind if i.startswith(\"bbrmon-tcptrace\")]\n",
    "miscellaneous = [i for i in sorted_ind if i not in (iperf3+jobmeta+ss+bbrmon_jobmeta+bbrmon_pscheduler+bbrmon_ss+bbrmon_tcptrace)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(iperf3+jobmeta+ss+bbrmon_jobmeta+bbrmon_pscheduler+bbrmon_ss+bbrmon_tcptrace+miscellaneous) == len(sorted_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "result = es.search(index=jobmeta[i],\n",
    "                                body={\"query\":{\"match_all\":{}}},\n",
    "                                size=10000,\n",
    "                                )\n",
    "data = [doc for doc in result['hits']['hits']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['hits'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['hits']['total'])\n",
    "print(result['hits']['max_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['hits']['hits'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['hits']['hits'][0]['_index']) #.keys()\n",
    "print(result['hits']['hits'][0]['_type'])\n",
    "print(result['hits']['hits'][0]['_id'])\n",
    "print(result['hits']['hits'][0]['_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['hits']['hits'][0]['_source'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['hits']['hits'][0]['_source']['iter_uuids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['hits']['hits'][0]['_source']['end'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['hits']['hits'][0]['_source']['@version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['hits']['hits'][0]['_source']['start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['hits']['hits'][0]['_source']['uuid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['hits']['hits'][0]['_source']['intervals'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['hits']['hits'][0]['_source']['@timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(data)):\n",
    "#     print(len(data[i]['_source']['end']['streams'])) # .keys()#['bits_per_second'] # [].keys()\n",
    "# data[0]['_source']['end']['streams'][0] #['sender'].keys()\n",
    "\n",
    "x = data[0]['_source']['end']['streams'][:]\n",
    "# x['sender']['start']\n",
    "a[:] = x[:]['sender']['start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es.get(index=\"iperf3-2021.06.06\", id=\"uhKs33kBkImc33Nl6K8P\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = es.search(index=indexes[0], body={\"query\":{\"match_all\":{}}})\n",
    "# indexes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = es.search(index=indexes[0], body={\"query\":{\"match_all\":{}}})\n",
    "\n",
    "# hitsList=[]\n",
    "# print(f\"Found {len(result.items())} keys found..\\n\")\n",
    "\n",
    "print(f\"Found '{clr.B}{clr.BD}{result['hits']['total']['value']}{clr.E}{clr.E}' documents in the current index'{clr.B}{clr.BD}{indexes[0]}{clr.E}{clr.E}'\")\n",
    "\n",
    "data = [doc for doc in result['hits']['hits']]\n",
    "\n",
    "i=0\n",
    "print(data[i]['_source'].keys())\n",
    "print(\"\\n\")\n",
    "print(data[i]['_source']['end'])\n",
    "# print(data[i]['_source']['@version'])\n",
    "# print(data[i]['_source']['start'])\n",
    "# print(data[i]['_source']['uuid'])\n",
    "# print(data[i]['_source']['@timestamp'])\n",
    "print()\n",
    "\n",
    "i=1\n",
    "print(data[i]['_source'].keys())\n",
    "print(\"\\n\")\n",
    "print(data[i]['_source']['end'])\n",
    "# print(data[i]['_source']['@version'])\n",
    "# print(data[i]['_source']['start'])\n",
    "# print(data[i]['_source']['uuid'])\n",
    "# print(data[i]['_source']['@timestamp'])\n",
    "print()\n",
    "\n",
    "i=2\n",
    "print(data[i]['_source'].keys())\n",
    "print(\"\\n\")\n",
    "print(data[i]['_source']['end'])\n",
    "# print(data[i]['_source']['@version'])\n",
    "# print(data[i]['_source']['start'])\n",
    "# print(data[i]['_source']['uuid'])\n",
    "# print(data[i]['_source']['@timestamp'])\n",
    "print()\n",
    "\n",
    "# for doc in data:\n",
    "#     print(doc['_source'].keys())\n",
    "#     print(doc['_source']['start'])\n",
    "#     print(doc['_source']['uuid'])\n",
    "#     print(doc['_source']['@timestamp'])\n",
    "#     break\n",
    "    # print(f\"{doc['_id']}, {doc['_source']}\"), {doc['_source']}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# **\n",
    "# ** Project Lead: Eashan Adhikarla\n",
    "# ** Mentor: Ezra Kissel\n",
    "# ** \n",
    "# ** Date Created: June 17' 2021\n",
    "# ** Last Modified: June 22' 2021 \n",
    "# **\n",
    "# '''\n",
    "\n",
    "# from elasticsearch import Elasticsearch\n",
    "# from elasticsearch.helpers import scan\n",
    "\n",
    "# import pandas as pd\n",
    "# import requests\n",
    "# import json\n",
    "\n",
    "# # Create the elasticsearch client\n",
    "# HOST = 'nersc-tbn-6.testbed100.es.net'\n",
    "# PORT = 9200\n",
    "\n",
    "# es = Elasticsearch(host=HOST, port=PORT)\n",
    "\n",
    "\n",
    "# class clr:\n",
    "#     \"\"\"\n",
    "#     Defining colors for the print syntax coloring\n",
    "#     \"\"\"\n",
    "#     H   = '\\033[35m' # Header\n",
    "#     B   = '\\033[94m' # Blue\n",
    "#     G   = '\\033[36m' # Green\n",
    "#     W   = '\\033[93m' # Warning\n",
    "#     F   = '\\033[91m' # Fail\n",
    "#     E   = '\\033[0m'  # End \n",
    "#     BD  = '\\033[1m'  # Bold\n",
    "#     UL  = '\\033[4m'  # Underline\n",
    "\n",
    "\n",
    "# class GETTER:\n",
    "#     \"\"\"\n",
    "#     Get class to get current information about different Indexs\n",
    "#     \"\"\"\n",
    "#     def __init__(self, term, sum=0):\n",
    "#         self.term = term\n",
    "#         self.sum = sum\n",
    "\n",
    "#     def getIndexList(self, term):\n",
    "#         idx=[]\n",
    "#         indices_dict = es.indices.get_alias(self.term)\n",
    "#         if isinstance(indices_dict, dict) and indices_dict is not None:\n",
    "#             print(f\"'{clr.G}{len(indices_dict)}{clr.E}' indexes found!\")\n",
    "#             for k,v in indices_dict.items():\n",
    "#                 idx.append(k)\n",
    "#             return idx\n",
    "#         else:\n",
    "#             print (f\"{clr.F}Empty dict!{clr.E}\")\n",
    "\n",
    "\n",
    "#     def getIndexedDetails(self, indexes):\n",
    "#         for i in range(1): # len(indexes)):\n",
    "#             try:\n",
    "#                 # Given a index name, finds all the documents in the index\n",
    "#                 # index: Index name as a string\n",
    "#                 # body: Empty equivalent to '*' to get all the values\n",
    "#                 # size: To some random max value for getting all the documents\n",
    "#                 result = es.search(index=indexes[i],\n",
    "#                                 body={\"query\":{\"match_all\":{}}},\n",
    "#                                 size=10000,\n",
    "#                                 )\n",
    "#                 print(f\"{indexes[i]} ---> {clr.G}{result['hits']['total']['value']}{clr.E} documents\\n\")\n",
    "                \n",
    "#                 documents = [doc for doc in result['hits']['hits']]\n",
    "                \n",
    "#                 for j in range(len(documents)):\n",
    "#                     # ---------------------\n",
    "#                     # For each job/document\n",
    "#                     # ---------------------\n",
    "#                     # Format: dict_keys(['end', '@version', 'start', 'uuid', 'intervals', '@timestamp'])\n",
    "#                     # uuid\n",
    "#                     uuid = documents[j]['_source']['uuid']\n",
    "#                     # @timestamp\n",
    "#                     timestamp = documents[j]['_source']['@timestamp']\n",
    "#                     # start (Format: dict_keys(['cookie', 'test_start', 'tcp_mss_default', 'version', 'connected', 'sndbuf_actual',\n",
    "#                     #                           'rcvbuf_actual', 'sock_bufsize', 'system_info', 'timestamp', 'connecting_to']))\n",
    "#                     start_dict = documents[j]['_source']['start']\n",
    "#                     num_streams = start_dict['test_start']['num_streams']\n",
    "\n",
    "#                     # --------------------- \n",
    "#                     # For each stream/flow\n",
    "#                     # ---------------------\n",
    "#                     # Intervals\n",
    "#                     intervals_list = documents[j]['_source']['intervals']\n",
    "#                     for k in range(len(intervals_list)):\n",
    "#                         for m in range(num_streams):\n",
    "#                             # Format: dict_keys(['receiver', 'sender'])\n",
    "#                             # 'receiver': dict_keys(['end', 'bits_per_second', 'sender', 'start', 'socket', 'seconds', 'bytes'])\n",
    "#                             # 'sender': dict_keys(['retransmits', 'max_rtt', 'sender', 'start', 'bytes', 'mean_rtt', 'end'\n",
    "#                             #                      'max_snd_cwnd', 'bits_per_second', 'socket', 'seconds', 'min_rtt'])\n",
    "#                             data[0]['_source']['end']['streams']\n",
    "                            \n",
    "#                             # Format: dict_keys(['retransmits', 'omitted', 'start', 'bytes', 'snd_cwnd', 'pmtu', 'rttvar', 'sender', 'end', 'bits_per_second', 'socket', 'seconds', 'rtt'])\n",
    "#                             # retransmits = documents[j]['_source']['intervals'][k]['streams'][m]['retransmits']\n",
    "#                             # rttvar      = documents[j]['_source']['intervals'][k]['streams'][m]['rttvar']\n",
    "#                             # rtt         = documents[j]['_source']['intervals'][k]['streams'][m]['rtt']\n",
    "#                             # bits_per_second = documents[j]['_source']['intervals'][k]['streams'][m]['bits_per_second']\n",
    "                    \n",
    "#                     print(f\"uuid: {uuid}\\ntimestamp: {timestamp}\\nnum_streams: {num_streams}\\nretransmits: {retransmits}\\nrttvar: {rttvar}\\nrtt: {rtt}\\nbps: {bits_per_second}\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "                \n",
    "#                 # self.sum += result['hits']['total']['value']\n",
    "                \n",
    "#                 # # iterate the nested dictionaries inside the [\"hits\"][\"hits\"] list\n",
    "#                 # for num, doc in enumerate(data):\n",
    "#                 #     # print (\"DOC ID:\", doc[\"_id\"], \"--->\", doc, type(doc), \"\\n\")\n",
    "                    \n",
    "#                 #     # Use 'iteritems()` instead of 'items()' if using Python 2\n",
    "#                 #     for key, value in doc.items():\n",
    "#                 #         print (f\"{clr.B}{key}{clr.E}   ---> {value}\")\n",
    "\n",
    "#                 #     # print a few spaces between each doc for readability\n",
    "#                 #     print (\"\\n\\n\")\n",
    "#                 #     break\n",
    "#             except:\n",
    "#                 pass\n",
    "#             # print(\"\\nTotal docs found: \", self.sum)\n",
    "        \n",
    "#         return None\n",
    "\n",
    "# indexTypes = [\"*\", \"iperf*\", \"jobmeta*\", \"bbrmon*\"]\n",
    "# term_ = indexTypes[1]\n",
    "\n",
    "# get = GETTER(term_)\n",
    "# indexes = get.getIndexList(term_)\n",
    "# # for e,i in enumerate(indexes):\n",
    "# #     print(f\"{e}: {i}\")\n",
    "\n",
    "# index_response = get.getIndexedDetails(indexes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k2,v2 in hitsList[0][0].items():\n",
    "    print(k2)\n",
    "    if k2==\"_score\":\n",
    "        for v in value:\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(host='nersc-tbn-6.testbed100.es.net', port=9200)\n",
    "res = es.search(index=\"iperf3*\",\n",
    "                # doc_type=\"articles\",\n",
    "                body={\"query\": \n",
    "                        {\"match\": \n",
    "                                {\"content\": \"\"}\n",
    "                        }\n",
    "                      })\n",
    "print(\"res: \", res)\n",
    "\n",
    "print(f\"{res['hits']['total']} documents found\")\n",
    "\n",
    "for doc in res['hits']['hits']:\n",
    "    print(\"%s) %s\" % (doc['_id'], doc['_source']['content']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying elasticsearch via elasticsearch-py module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data_from_elastic():\n",
    "    \n",
    "# query: The elasticsearch query.\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"symbols.keyword\": \"\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Scan function to get all the data. \n",
    "rel = scan( client=es,             \n",
    "            query=query,                                     \n",
    "            scroll='1m',\n",
    "            index='iperf3*',\n",
    "            raise_on_error=True,\n",
    "            preserve_order=True,\n",
    "            clear_scroll=True)\n",
    "\n",
    "# Keep response in a list.\n",
    "if not rel==None:\n",
    "    result = list(rel)\n",
    "\n",
    "print(result)\n",
    "\n",
    "# temp = []\n",
    "\n",
    "# # We need only '_source', which has all the fields required.\n",
    "# # This elimantes the elasticsearch metdata like _id, _type, _index.\n",
    "# for hit in result:\n",
    "#     temp.append(hit['_source'])\n",
    "\n",
    "# # Create a dataframe.\n",
    "# df = pd.DataFrame(temp)\n",
    "\n",
    "# return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying elasticsearch via REST api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(uri, term):\n",
    "    \"\"\"\n",
    "    Simple Elasticsearch Query\n",
    "    \"\"\"\n",
    "    query = json.dumps({\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"content\": term\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    response = requests.get(uri, data=query)\n",
    "    results = json.loads(response.text)\n",
    "    return results\n",
    "\n",
    "def format_results(results):\n",
    "    \"\"\"\n",
    "    Print results nicely:\n",
    "    doc_id) content\n",
    "    \"\"\"\n",
    "    data = [doc for doc in results['hits']['hits']]\n",
    "    for doc in data:\n",
    "        print(\"%s) %s\" % (doc['_id'], doc['_source']['content']))\n",
    "\n",
    "def create_doc(uri, doc_data={}):\n",
    "    \"\"\"\n",
    "    Create new document.\n",
    "    \"\"\"\n",
    "    query = json.dumps(doc_data)\n",
    "    response = requests.post(uri, data=query)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = get_data_from_elastic()\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    uri_search = 'http://192.168.120.46:9200/'\n",
    "    # uri_search = 'http://192.168.120.46:9200/test/articles/_search'\n",
    "    # uri_create = 'http://localhost:9200/test/articles/'\n",
    "\n",
    "    results = search(uri_search, \"_source\")\n",
    "    print(results)\n",
    "    # format_results(results)\n",
    "\n",
    "    #create_doc(uri_create, {\"content\": \"The fox!\"})\n",
    "    #results = search(uri_search, \"fox\")\n",
    "    #format_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !curl -XGET 192.168.120.46:9200/iperf3*/_search | jq >> sample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "list_files = subprocess.run([\"ls\", \"-al\"])\n",
    "print(\"The exit code was: %d\" % list_files.returncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan\n",
    "\n",
    "# Create the elasticsearch client\n",
    "HOST = 'nersc-tbn-6.testbed100.es.net'\n",
    "PORT = 9200\n",
    "\n",
    "es = Elasticsearch(host=HOST, port=PORT, timeout=30)\n",
    "# all = es.indices.get_alias(\"*\").keys()\n",
    "jobmeta = es.indices.get_alias(\"jobmeta-**\").keys()\n",
    "iperf3 = es.indices.get_alias(\"iperf3*\").keys()\n",
    "print(len(jobmeta))#, len(iperf3))\n",
    "# print(jobmeta, iperf3)\n",
    "\n",
    "iperf3 = sorted(iperf3)\n",
    "jobmeta = sorted(jobmeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobmeta-2021.06.22\n",
      "jobmeta-2021.06.24\n",
      "jobmeta-2021.06.28\n"
     ]
    }
   ],
   "source": [
    "jobmeta = jobmeta[5:]\n",
    "for j in jobmeta:\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------> jobmeta-2021.06.22\n",
      "4\n",
      "hostB\n",
      "dict_keys(['profile', 'profile_settings', 'parent', '@version', 'hostname', 'iter_uuids', 'alias', '@timestamp'])\n",
      "\n",
      "hostC\n",
      "dict_keys(['profile', 'profile_settings', 'parent', '@version', 'hostname', 'iter_uuids', 'alias', '@timestamp'])\n",
      "\n",
      "None\n",
      "dict_keys(['dst', 'src', 'parent', 'src_cmd', 'start_time', 'outdir', 'name', 'vector_element', 'iters', 'hostlist', 'user', 'enabled', 'post_dst_cmd', 'tcpdump_filt', 'pre_dst_cmd', 'post', 'util_path', 'archive', 'pacing', 'uuid', 'limit_sweep', 'pre_src_cmd', 'loss', 'nic', 'lat_sweep', 'profile_file', 'hostname', 'tcptrace', 'hosts', 'alias', 'tcpdump', 'profile', 'type', '@timestamp', 'dst_cmd', 'dst_cmd_once', 'ss', 'lat', 'instrument', 'limit', 'pre', 'src_cmd_once', 'post_src_cmd', '@version', 'param_sweep'])\n",
      "\n",
      "hostA\n",
      "dict_keys(['profile', 'profile_settings', 'parent', '@version', 'hostname', 'iter_uuids', 'alias', '@timestamp'])\n",
      "\n",
      "------> jobmeta-2021.06.24\n",
      "4\n",
      "None\n",
      "dict_keys(['dst', 'src', 'parent', 'src_cmd', 'start_time', 'outdir', 'name', 'vector_element', 'iters', 'hostlist', 'user', 'enabled', 'post_dst_cmd', 'tcpdump_filt', 'pre_dst_cmd', 'post', 'util_path', 'archive', 'pacing', 'uuid', 'limit_sweep', 'pre_src_cmd', 'loss', 'nic', 'lat_sweep', 'profile_file', 'hostname', 'tcptrace', 'hosts', 'alias', 'tcpdump', 'profile', 'type', '@timestamp', 'dst_cmd', 'dst_cmd_once', 'ss', 'lat', 'instrument', 'limit', 'pre', 'src_cmd_once', 'post_src_cmd', '@version', 'param_sweep'])\n",
      "\n",
      "hostA\n",
      "dict_keys(['profile', 'profile_settings', 'parent', '@version', 'hostname', 'iter_uuids', 'alias', '@timestamp'])\n",
      "\n",
      "hostA\n",
      "dict_keys(['profile', 'profile_settings', 'parent', '@version', 'hostname', 'iter_uuids', 'alias', '@timestamp'])\n",
      "\n",
      "hostA\n",
      "dict_keys(['profile', 'profile_settings', 'parent', '@version', 'hostname', 'iter_uuids', 'alias', '@timestamp'])\n",
      "\n",
      "------> jobmeta-2021.06.28\n",
      "4\n",
      "['1gbit', '2gbit', '3gbit', '4gbit', '5gbit', '6gbit']\n",
      "dict_keys(['dst', 'src', 'parent', 'src_cmd', 'start_time', 'outdir', 'name', 'vector_element', 'iters', 'hostlist', 'user', 'enabled', 'post_dst_cmd', 'tcpdump_filt', 'pre_dst_cmd', 'post', 'util_path', 'archive', 'pacing', 'uuid', 'limit_sweep', 'pre_src_cmd', 'loss', 'nic', 'lat_sweep', 'profile_file', 'hostname', 'tcptrace', 'hosts', 'alias', 'tcpdump', 'profile', 'type', '@timestamp', 'dst_cmd', 'dst_cmd_once', 'ss', 'lat', 'instrument', 'limit', 'pre', 'src_cmd_once', 'post_src_cmd', '@version', 'param_sweep'])\n",
      "\n",
      "['1gbit', '2gbit', '3gbit', '4gbit', '5gbit', '6gbit']\n",
      "dict_keys(['dst', 'NIC_speed', 'src', 'parent', 'src_cmd', 'start_time', 'outdir', 'name', 'vector_element', 'iters', 'hostlist', 'user', 'enabled', 'post_dst_cmd', 'tcpdump_filt', 'pre_dst_cmd', 'post', 'util_path', 'archive', 'pacing', 'uuid', 'limit_sweep', 'pre_src_cmd', 'loss', 'nic', 'lat_sweep', 'profile_file', 'hostname', 'MTU', 'tcptrace', 'hosts', 'alias', 'tcpdump', 'profile', 'type', '@timestamp', 'dst_cmd', 'dst_cmd_once', 'ss', 'lat', 'instrument', 'limit', 'pre', 'src_cmd_once', 'post_src_cmd', '@version', 'NIC', 'param_sweep'])\n",
      "\n",
      "1gbit\n",
      "dict_keys(['profile', 'profile_settings', 'parent', '@version', 'pacing', 'hostname', 'iter_uuids', 'alias', '@timestamp'])\n",
      "\n",
      "['1gbit', '2gbit', '3gbit', '4gbit', '5gbit', '6gbit']\n",
      "dict_keys(['dst', 'NIC_speed', 'src', 'parent', 'src_cmd', 'start_time', 'outdir', 'name', 'vector_element', 'iters', 'hostlist', 'user', 'enabled', 'post_dst_cmd', 'tcpdump_filt', 'pre_dst_cmd', 'post', 'util_path', 'archive', 'pacing', 'uuid', 'limit_sweep', 'pre_src_cmd', 'loss', 'nic', 'lat_sweep', 'profile_file', 'hostname', 'MTU', 'tcptrace', 'hosts', 'alias', 'tcpdump', 'profile', 'type', '@timestamp', 'dst_cmd', 'dst_cmd_once', 'ss', 'lat', 'instrument', 'limit', 'pre', 'src_cmd_once', 'post_src_cmd', '@version', 'NIC', 'param_sweep'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "key = 'start_time'\n",
    "\n",
    "for j in jobmeta:\n",
    "    print(\"------>\", j)\n",
    "    jobmeta_result = es.search( index=j,\n",
    "                                body={\"query\":{\"match_all\":{}}},\n",
    "                                size=10000,\n",
    "                                )\n",
    "    print(len(jobmeta_result))\n",
    "    for i in range(len(jobmeta_result)):\n",
    "        try:\n",
    "            print(jobmeta_result['hits']['hits'][i]['_source']['pacing'])\n",
    "        except:\n",
    "            print(jobmeta_result['hits']['hits'][i]['_source']['alias'])\n",
    "        print(jobmeta_result['hits']['hits'][i]['_source'].keys())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(jobmeta_result)):\n",
    "    key = 'start_time'\n",
    "    # print(jobmeta_result['hits']['hits'][i]['_source'].keys())\n",
    "    if key in jobmeta_result['hits']['hits'][i].keys():\n",
    "        print(jobmeta_result['hits']['hits'][i]['_source']['start_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['start_time', 'parent', 'user', 'post', 'outdir', 'name', 'dst_cmd_once', 'tcpdump_filt', 'hostlist', 'post_src_cmd', 'instrument', '_hosts', 'pre_dst_cmd', 'src', 'tcptrace', 'tcpdump', 'hostname', 'iters', 'dst', 'src_cmd_once', 'pre', 'ss', 'iter_uuids', 'type', 'src_cmd', '@timestamp', 'pre_src_cmd', 'enabled', '@version', 'dst_cmd', 'uuid', 'post_dst_cmd'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobmeta_result['hits']['hits'][0]['_source'].keys() #['alias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in jobmeta:\n",
    "    jobmeta_result = es.search( index=x,\n",
    "                                body={\"query\":{\"match_all\":{}}},\n",
    "                                size=1000,\n",
    "                                )\n",
    "    jobmeta_documents = [doc for doc in jobmeta_result['hits']['hits']]\n",
    "    print(f\"{len(jobmeta_documents)} found in index {x}\")\n",
    "\n",
    "    for i in iperf3:\n",
    "        iperf3_results = es.search( index=i,\n",
    "                                    body={\"query\":{\"match_all\":{}}},\n",
    "                                    size=1000,\n",
    "                                   )\n",
    "        iperf3_documents = [doc for doc in iperf3_results['hits']['hits']]\n",
    "        for doc in iperf3_documents:\n",
    "            uuid = doc['_source']['uuid']\n",
    "            for jobdoc in jobmeta_documents:\n",
    "                try:\n",
    "                    iter_uuids = jobdoc[\"_source\"][\"iter_uuids\"]\n",
    "                except:\n",
    "                    pass\n",
    "                if uuid in iter_uuids:\n",
    "                    timestamp = doc['_source']['@timestamp']\n",
    "                    start_dict = doc['_source']['start']\n",
    "                    num_streams = start_dict['test_start']['num_streams']\n",
    "\n",
    "                    print(\"uuid: \", uuid)\n",
    "                    print(\"timestamp: \", timestamp)\n",
    "                    print(\"num_streams: \", num_streams)\n",
    "                    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.201.1.2'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobdoc['_source']['hostname']#.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, datetime\n",
    "\n",
    "for a in all:\n",
    "    try:\n",
    "        match = re.search('\\d{4}.\\d{2}.\\d{2}', a)\n",
    "        date = datetime.datetime.strptime(match.group(), '%Y.%m.%d').date()\n",
    "        # print(f\"{a}  --->   {date}\")\n",
    "    except:\n",
    "        pass\n",
    "        # print(f\"{a}  ---->   Not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-20 00:00:00 2021-06-29 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from_date = datetime.datetime(date.year, date.month, date.day)\n",
    "curr = datetime.datetime.now()\n",
    "to_date = datetime.datetime(curr.year, curr.month, curr.day)\n",
    "\n",
    "print(from_date, to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = datetime.datetime(2021,5,15)\n",
    "from_date < date < to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# curr = datetime.date.today()\n",
    "curr = datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-6-29'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{curr.year}-{curr.month}-{curr.day}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iperf3-2021.06.28'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iperf3[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "iperf3_result = es.search( index=iperf3[5],\n",
    "                            body={\"query\":{\"match_all\":{}}},\n",
    "                            size=10000,\n",
    "                            )\n",
    "iperf3_doc = [doc for doc in iperf3_result['hits']['hits']]\n",
    "print(len(iperf3_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['end', '@version', 'start', 'uuid', 'intervals', '@timestamp'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(iperf3_doc)\n",
    "iperf3_doc[0]['_source'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['receiver_tcp_congestion', 'sum_received', 'streams', 'sum_sent', 'cpu_utilization_percent', 'sender_tcp_congestion'])\n",
      "dict_keys(['cookie', 'test_start', 'tcp_mss_default', 'version', 'connected', 'sndbuf_actual', 'rcvbuf_actual', 'sock_bufsize', 'system_info', 'timestamp', 'connecting_to'])\n"
     ]
    }
   ],
   "source": [
    "print(iperf3_doc[0]['_source']['end'].keys())\n",
    "print(iperf3_doc[0]['_source']['start'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['streams', 'sum'])\n"
     ]
    }
   ],
   "source": [
    "print(iperf3_doc[0]['_source']['intervals'][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'receiver': {'end': 60.06053,\n",
       "   'bits_per_second': 1623276822.0993054,\n",
       "   'sender': True,\n",
       "   'start': 0,\n",
       "   'socket': 5,\n",
       "   'seconds': 60.000067,\n",
       "   'bytes': 12186858284},\n",
       "  'sender': {'retransmits': 1535,\n",
       "   'max_rtt': 60529,\n",
       "   'sender': True,\n",
       "   'start': 0,\n",
       "   'bytes': 12227868368,\n",
       "   'mean_rtt': 60371,\n",
       "   'end': 60.000067,\n",
       "   'max_snd_cwnd': 14334696,\n",
       "   'bits_per_second': 1630380628.4749649,\n",
       "   'socket': 5,\n",
       "   'seconds': 60.000067,\n",
       "   'min_rtt': 60124}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iperf3_doc[0]['_source']['end']['streams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'retransmits': 1535,\n",
       " 'max_rtt': 60529,\n",
       " 'sender': True,\n",
       " 'start': 0,\n",
       " 'bytes': 12227868368,\n",
       " 'mean_rtt': 60371,\n",
       " 'end': 60.000067,\n",
       " 'max_snd_cwnd': 14334696,\n",
       " 'bits_per_second': 1630380628.4749649,\n",
       " 'socket': 5,\n",
       " 'seconds': 60.000067,\n",
       " 'min_rtt': 60124}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iperf3_doc[0]['_source']['end']['streams'][0]['sender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6e2000e74d83ab38a7a73c8353776f8595191be383670aed524d2a15b88663d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('bbrv2': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "f6e2000e74d83ab38a7a73c8353776f8595191be383670aed524d2a15b88663d"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}