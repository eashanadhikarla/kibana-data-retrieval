{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# !curl -X GET \"nersc-tbn-6.testbed100.es.net:9200/iperf3*/_search\" | jq "
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''\n",
    "**\n",
    "** Project Lead: Eashan Adhikarla\n",
    "** Mentor: Ezra Kissel\n",
    "** \n",
    "** Date Created: June 17' 2021\n",
    "** Last Modified: June 22' 2021 \n",
    "**\n",
    "'''\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Create the elasticsearch client\n",
    "HOST = 'nersc-tbn-6.testbed100.es.net'\n",
    "PORT = 9200\n",
    "\n",
    "es = Elasticsearch(host=HOST, port=PORT, timeout=50)\n",
    "\n",
    "\n",
    "class clr:\n",
    "    \"\"\"\n",
    "    Defining colors for the print syntax coloring\n",
    "    \"\"\"\n",
    "    H   = '\\033[35m' # Header\n",
    "    B   = '\\033[94m' # Blue\n",
    "    G   = '\\033[36m' # Green\n",
    "    W   = '\\033[93m' # Warning\n",
    "    F   = '\\033[91m' # Fail\n",
    "    E   = '\\033[0m'  # End \n",
    "    BD  = '\\033[1m'  # Bold\n",
    "    UL  = '\\033[4m'  # Underline\n",
    "\n",
    "\n",
    "class GETTER:\n",
    "    \"\"\"\n",
    "    Get class to get current information about different Indexs\n",
    "    \"\"\"\n",
    "    def __init__(self, term, sum=0):\n",
    "        self.term = term\n",
    "        self.sum = sum\n",
    "\n",
    "    def getIndexList(self, term):\n",
    "        idx=[]\n",
    "        indices_dict = es.indices.get_alias(self.term)\n",
    "        if isinstance(indices_dict, dict) and indices_dict is not None:\n",
    "            print(f\"'{clr.G}{len(indices_dict)}{clr.E}' indexes found!\")\n",
    "            for k,v in indices_dict.items():\n",
    "                idx.append(k)\n",
    "            return idx\n",
    "        else:\n",
    "            print (f\"{clr.F}Empty dict!{clr.E}\")\n",
    "\n",
    "\n",
    "    def getIndexedDetails(self, indexes):\n",
    "        for i in range(1): # len(indexes)):\n",
    "            try:\n",
    "                # Given a index name, finds all the documents in the index\n",
    "                # index: Index name as a string\n",
    "                # body: Empty equivalent to '*' to get all the values\n",
    "                # size: To some random max value for getting all the documents\n",
    "                result = es.search(index=indexes[i],\n",
    "                                body={\"query\":{\"match_all\":{}}},\n",
    "                                size=1000,\n",
    "                                )\n",
    "                print(f\"{indexes[i]} ---> {clr.G}{result['hits']['total']['value']}{clr.E} documents\\n\")\n",
    "                \n",
    "                documents = [doc for doc in result['hits']['hits']]\n",
    "                \n",
    "                for j in range(len(documents)):\n",
    "                    # ---------------------\n",
    "                    # For each job/document\n",
    "                    # ---------------------\n",
    "                    # Format: dict_keys(['end', '@version', 'start', 'uuid', 'intervals', '@timestamp'])\n",
    "                    # uuid\n",
    "                    uuid = documents[j]['_source']['uuid']\n",
    "                    # @timestamp\n",
    "                    timestamp = documents[j]['_source']['@timestamp']\n",
    "                    # start (Format: dict_keys(['cookie', 'test_start', 'tcp_mss_default', 'version', 'connected', 'sndbuf_actual',\n",
    "                    #                           'rcvbuf_actual', 'sock_bufsize', 'system_info', 'timestamp', 'connecting_to']))\n",
    "                    start_dict = documents[j]['_source']['start']\n",
    "                    num_streams = start_dict['test_start']['num_streams']\n",
    "\n",
    "                    # Congestion Type\n",
    "                    sender_congestion = documents[j]['_source']['end']['sender_tcp_congestion']\n",
    "                    receiver_congestion = documents[j]['_source']['end']['receiver_tcp_congestion']\n",
    "\n",
    "\n",
    "                    # print(f\"uuid: {uuid}\\ntimestamp: {timestamp}\\nnum_streams: {num_streams}\\n\\n\")\n",
    "\n",
    "                    # --------------------- \n",
    "                    # For each stream/flow\n",
    "                    # ---------------------\n",
    "                    # Intervals\n",
    "                    intervals_list = documents[j]['_source']['intervals']\n",
    "                    for k in range(len(intervals_list)):\n",
    "                        for m in range(num_streams):\n",
    "                            # Format: dict_keys(['receiver', 'sender'])\n",
    "                            # 'sender': dict_keys(['retransmits', 'max_rtt', 'sender', 'start', 'bytes', 'mean_rtt', 'end'\n",
    "                            #                      'max_snd_cwnd', 'bits_per_second', 'socket', 'seconds', 'min_rtt'])\n",
    "                            # 'receiver': dict_keys(['end', 'bits_per_second', 'sender', 'start', 'socket', 'seconds', 'bytes'])\n",
    "                            \n",
    "                            sender_start = documents[j]['_source']['end']['streams'][m]['sender']['start']\n",
    "                            sender_end = documents[j]['_source']['end']['streams'][m]['sender']['end']\n",
    "                            sender_retransmits = documents[j]['_source']['end']['streams'][m]['sender']['retransmits']\n",
    "                            sender_bytes = documents[j]['_source']['end']['streams'][m]['sender']['bytes']\n",
    "                            sender_min_rtt = documents[j]['_source']['end']['streams'][m]['sender']['min_rtt']\n",
    "                            sender_max_rtt = documents[j]['_source']['end']['streams'][m]['sender']['max_rtt']\n",
    "                            sender_mean_rtt = documents[j]['_source']['end']['streams'][m]['sender']['mean_rtt']\n",
    "                            sender_bps = documents[j]['_source']['end']['streams'][m]['sender']['bits_per_second']\n",
    "\n",
    "                            receiver_start = documents[j]['_source']['end']['streams'][m]['receiver']['start']\n",
    "                            receiver_end = documents[j]['_source']['end']['streams'][m]['receiver']['end']\n",
    "                            receiver_seconds = documents[j]['_source']['end']['streams'][m]['receiver']['seconds']\n",
    "                            receiver_bytes = documents[j]['_source']['end']['streams'][m]['receiver']['bytes']\n",
    "                            receiver_bps = documents[j]['_source']['end']['streams'][m]['receiver']['bits_per_second']\n",
    "\n",
    "\n",
    "                            print(f\"uuid: {uuid}\\ntimestamp: {timestamp}\\nnum_streams: {num_streams}\\nsender_congestion: {sender_congestion}\\nreceiver_congestion: {receiver_congestion}\\nsender_start: {sender_start}\\n sender_end: {sender_end}\\n sender_retransmits: {sender_retransmits}\\nsender_bytes: {sender_bytes}\\n sender_min_rtt: {sender_min_rtt}\\n sender_max_rtt: {sender_max_rtt}\\nsender_mean_rtt: {sender_mean_rtt}\\n sender_bps: {sender_bps}\\n receiver_start: {receiver_start}\\nreceiver_end: {receiver_end}\\n receiver_seconds: {receiver_seconds}\\n receiver_bytes: {receiver_bytes}\\nreceiver_bps: {receiver_bps}\\n\\n\")\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "            # print(\"\\nTotal docs found: \", self.sum)\n",
    "        \n",
    "        return None\n",
    "\n",
    "indexTypes = [\"*\", \"iperf*\", \"jobmeta*\", \"bbrmon*\"]\n",
    "term_ = indexTypes[0]\n",
    "\n",
    "get = GETTER(term_)\n",
    "indexes = get.getIndexList(term_)\n",
    "for e,i in enumerate(indexes):\n",
    "    print(f\"{e}: {i}\")\n",
    "\n",
    "# index_response = get.getIndexedDetails(indexes)\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sorted_ind = sorted(indexes)\n",
    "sorted_ind"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "iperf3 = [i for i in sorted_ind if i.startswith(\"iperf3\")]\n",
    "jobmeta = [i for i in sorted_ind if i.startswith(\"jobmeta\")]\n",
    "ss = [i for i in sorted_ind if i.startswith(\"ss\")]\n",
    "bbrmon_pscheduler = [i for i in sorted_ind if i.startswith(\"bbrmon-pscheduler\")]\n",
    "bbrmon_jobmeta = [i for i in sorted_ind if i.startswith(\"bbrmon-jobmeta\")]\n",
    "bbrmon_ss = [i for i in sorted_ind if i.startswith(\"bbrmon-ss\")]\n",
    "bbrmon_tcptrace = [i for i in sorted_ind if i.startswith(\"bbrmon-tcptrace\")]\n",
    "miscellaneous = [i for i in sorted_ind if i not in (iperf3+jobmeta+ss+bbrmon_jobmeta+bbrmon_pscheduler+bbrmon_ss+bbrmon_tcptrace)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(iperf3+jobmeta+ss+bbrmon_jobmeta+bbrmon_pscheduler+bbrmon_ss+bbrmon_tcptrace+miscellaneous) == len(sorted_ind)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "i=0\n",
    "result = es.search(index=jobmeta[i],\n",
    "                                body={\"query\":{\"match_all\":{}}},\n",
    "                                size=10000,\n",
    "                                )\n",
    "data = [doc for doc in result['hits']['hits']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data[0].keys()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result['hits'].keys()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(result['hits']['total'])\n",
    "print(result['hits']['max_score'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result['hits']['hits'][0].keys()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(result['hits']['hits'][0]['_index']) #.keys()\n",
    "print(result['hits']['hits'][0]['_type'])\n",
    "print(result['hits']['hits'][0]['_id'])\n",
    "print(result['hits']['hits'][0]['_score'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result['hits']['hits'][0]['_source'].keys()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result['hits']['hits'][0]['_source']['iter_uuids']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result['hits']['hits'][0]['_source']['end'].keys()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result['hits']['hits'][0]['_source']['@version']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result['hits']['hits'][0]['_source']['start']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result['hits']['hits'][0]['_source']['uuid']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result['hits']['hits'][0]['_source']['intervals'][0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result['hits']['hits'][0]['_source']['@timestamp']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for i in range(len(data)):\n",
    "#     print(len(data[i]['_source']['end']['streams'])) # .keys()#['bits_per_second'] # [].keys()\n",
    "# data[0]['_source']['end']['streams'][0] #['sender'].keys()\n",
    "\n",
    "x = data[0]['_source']['end']['streams'][:]\n",
    "# x['sender']['start']\n",
    "a[:] = x[:]['sender']['start']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# es.get(index=\"iperf3-2021.06.06\", id=\"uhKs33kBkImc33Nl6K8P\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# result = es.search(index=indexes[0], body={\"query\":{\"match_all\":{}}})\n",
    "# indexes[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result = es.search(index=indexes[0], body={\"query\":{\"match_all\":{}}})\n",
    "\n",
    "# hitsList=[]\n",
    "# print(f\"Found {len(result.items())} keys found..\\n\")\n",
    "\n",
    "print(f\"Found '{clr.B}{clr.BD}{result['hits']['total']['value']}{clr.E}{clr.E}' documents in the current index'{clr.B}{clr.BD}{indexes[0]}{clr.E}{clr.E}'\")\n",
    "\n",
    "data = [doc for doc in result['hits']['hits']]\n",
    "\n",
    "i=0\n",
    "print(data[i]['_source'].keys())\n",
    "print(\"\\n\")\n",
    "print(data[i]['_source']['end'])\n",
    "# print(data[i]['_source']['@version'])\n",
    "# print(data[i]['_source']['start'])\n",
    "# print(data[i]['_source']['uuid'])\n",
    "# print(data[i]['_source']['@timestamp'])\n",
    "print()\n",
    "\n",
    "i=1\n",
    "print(data[i]['_source'].keys())\n",
    "print(\"\\n\")\n",
    "print(data[i]['_source']['end'])\n",
    "# print(data[i]['_source']['@version'])\n",
    "# print(data[i]['_source']['start'])\n",
    "# print(data[i]['_source']['uuid'])\n",
    "# print(data[i]['_source']['@timestamp'])\n",
    "print()\n",
    "\n",
    "i=2\n",
    "print(data[i]['_source'].keys())\n",
    "print(\"\\n\")\n",
    "print(data[i]['_source']['end'])\n",
    "# print(data[i]['_source']['@version'])\n",
    "# print(data[i]['_source']['start'])\n",
    "# print(data[i]['_source']['uuid'])\n",
    "# print(data[i]['_source']['@timestamp'])\n",
    "print()\n",
    "\n",
    "# for doc in data:\n",
    "#     print(doc['_source'].keys())\n",
    "#     print(doc['_source']['start'])\n",
    "#     print(doc['_source']['uuid'])\n",
    "#     print(doc['_source']['@timestamp'])\n",
    "#     break\n",
    "    # print(f\"{doc['_id']}, {doc['_source']}\"), {doc['_source']}\" )"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# '''\n",
    "# **\n",
    "# ** Project Lead: Eashan Adhikarla\n",
    "# ** Mentor: Ezra Kissel\n",
    "# ** \n",
    "# ** Date Created: June 17' 2021\n",
    "# ** Last Modified: June 22' 2021 \n",
    "# **\n",
    "# '''\n",
    "\n",
    "# from elasticsearch import Elasticsearch\n",
    "# from elasticsearch.helpers import scan\n",
    "\n",
    "# import pandas as pd\n",
    "# import requests\n",
    "# import json\n",
    "\n",
    "# # Create the elasticsearch client\n",
    "# HOST = 'nersc-tbn-6.testbed100.es.net'\n",
    "# PORT = 9200\n",
    "\n",
    "# es = Elasticsearch(host=HOST, port=PORT)\n",
    "\n",
    "\n",
    "# class clr:\n",
    "#     \"\"\"\n",
    "#     Defining colors for the print syntax coloring\n",
    "#     \"\"\"\n",
    "#     H   = '\\033[35m' # Header\n",
    "#     B   = '\\033[94m' # Blue\n",
    "#     G   = '\\033[36m' # Green\n",
    "#     W   = '\\033[93m' # Warning\n",
    "#     F   = '\\033[91m' # Fail\n",
    "#     E   = '\\033[0m'  # End \n",
    "#     BD  = '\\033[1m'  # Bold\n",
    "#     UL  = '\\033[4m'  # Underline\n",
    "\n",
    "\n",
    "# class GETTER:\n",
    "#     \"\"\"\n",
    "#     Get class to get current information about different Indexs\n",
    "#     \"\"\"\n",
    "#     def __init__(self, term, sum=0):\n",
    "#         self.term = term\n",
    "#         self.sum = sum\n",
    "\n",
    "#     def getIndexList(self, term):\n",
    "#         idx=[]\n",
    "#         indices_dict = es.indices.get_alias(self.term)\n",
    "#         if isinstance(indices_dict, dict) and indices_dict is not None:\n",
    "#             print(f\"'{clr.G}{len(indices_dict)}{clr.E}' indexes found!\")\n",
    "#             for k,v in indices_dict.items():\n",
    "#                 idx.append(k)\n",
    "#             return idx\n",
    "#         else:\n",
    "#             print (f\"{clr.F}Empty dict!{clr.E}\")\n",
    "\n",
    "\n",
    "#     def getIndexedDetails(self, indexes):\n",
    "#         for i in range(1): # len(indexes)):\n",
    "#             try:\n",
    "#                 # Given a index name, finds all the documents in the index\n",
    "#                 # index: Index name as a string\n",
    "#                 # body: Empty equivalent to '*' to get all the values\n",
    "#                 # size: To some random max value for getting all the documents\n",
    "#                 result = es.search(index=indexes[i],\n",
    "#                                 body={\"query\":{\"match_all\":{}}},\n",
    "#                                 size=10000,\n",
    "#                                 )\n",
    "#                 print(f\"{indexes[i]} ---> {clr.G}{result['hits']['total']['value']}{clr.E} documents\\n\")\n",
    "                \n",
    "#                 documents = [doc for doc in result['hits']['hits']]\n",
    "                \n",
    "#                 for j in range(len(documents)):\n",
    "#                     # ---------------------\n",
    "#                     # For each job/document\n",
    "#                     # ---------------------\n",
    "#                     # Format: dict_keys(['end', '@version', 'start', 'uuid', 'intervals', '@timestamp'])\n",
    "#                     # uuid\n",
    "#                     uuid = documents[j]['_source']['uuid']\n",
    "#                     # @timestamp\n",
    "#                     timestamp = documents[j]['_source']['@timestamp']\n",
    "#                     # start (Format: dict_keys(['cookie', 'test_start', 'tcp_mss_default', 'version', 'connected', 'sndbuf_actual',\n",
    "#                     #                           'rcvbuf_actual', 'sock_bufsize', 'system_info', 'timestamp', 'connecting_to']))\n",
    "#                     start_dict = documents[j]['_source']['start']\n",
    "#                     num_streams = start_dict['test_start']['num_streams']\n",
    "\n",
    "#                     # --------------------- \n",
    "#                     # For each stream/flow\n",
    "#                     # ---------------------\n",
    "#                     # Intervals\n",
    "#                     intervals_list = documents[j]['_source']['intervals']\n",
    "#                     for k in range(len(intervals_list)):\n",
    "#                         for m in range(num_streams):\n",
    "#                             # Format: dict_keys(['receiver', 'sender'])\n",
    "#                             # 'receiver': dict_keys(['end', 'bits_per_second', 'sender', 'start', 'socket', 'seconds', 'bytes'])\n",
    "#                             # 'sender': dict_keys(['retransmits', 'max_rtt', 'sender', 'start', 'bytes', 'mean_rtt', 'end'\n",
    "#                             #                      'max_snd_cwnd', 'bits_per_second', 'socket', 'seconds', 'min_rtt'])\n",
    "#                             data[0]['_source']['end']['streams']\n",
    "                            \n",
    "#                             # Format: dict_keys(['retransmits', 'omitted', 'start', 'bytes', 'snd_cwnd', 'pmtu', 'rttvar', 'sender', 'end', 'bits_per_second', 'socket', 'seconds', 'rtt'])\n",
    "#                             # retransmits = documents[j]['_source']['intervals'][k]['streams'][m]['retransmits']\n",
    "#                             # rttvar      = documents[j]['_source']['intervals'][k]['streams'][m]['rttvar']\n",
    "#                             # rtt         = documents[j]['_source']['intervals'][k]['streams'][m]['rtt']\n",
    "#                             # bits_per_second = documents[j]['_source']['intervals'][k]['streams'][m]['bits_per_second']\n",
    "                    \n",
    "#                     print(f\"uuid: {uuid}\\ntimestamp: {timestamp}\\nnum_streams: {num_streams}\\nretransmits: {retransmits}\\nrttvar: {rttvar}\\nrtt: {rtt}\\nbps: {bits_per_second}\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "                \n",
    "#                 # self.sum += result['hits']['total']['value']\n",
    "                \n",
    "#                 # # iterate the nested dictionaries inside the [\"hits\"][\"hits\"] list\n",
    "#                 # for num, doc in enumerate(data):\n",
    "#                 #     # print (\"DOC ID:\", doc[\"_id\"], \"--->\", doc, type(doc), \"\\n\")\n",
    "                    \n",
    "#                 #     # Use 'iteritems()` instead of 'items()' if using Python 2\n",
    "#                 #     for key, value in doc.items():\n",
    "#                 #         print (f\"{clr.B}{key}{clr.E}   ---> {value}\")\n",
    "\n",
    "#                 #     # print a few spaces between each doc for readability\n",
    "#                 #     print (\"\\n\\n\")\n",
    "#                 #     break\n",
    "#             except:\n",
    "#                 pass\n",
    "#             # print(\"\\nTotal docs found: \", self.sum)\n",
    "        \n",
    "#         return None\n",
    "\n",
    "# indexTypes = [\"*\", \"iperf*\", \"jobmeta*\", \"bbrmon*\"]\n",
    "# term_ = indexTypes[1]\n",
    "\n",
    "# get = GETTER(term_)\n",
    "# indexes = get.getIndexList(term_)\n",
    "# # for e,i in enumerate(indexes):\n",
    "# #     print(f\"{e}: {i}\")\n",
    "\n",
    "# index_response = get.getIndexedDetails(indexes)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for k2,v2 in hitsList[0][0].items():\n",
    "    print(k2)\n",
    "    if k2==\"_score\":\n",
    "        for v in value:\n",
    "            print(v)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(host='nersc-tbn-6.testbed100.es.net', port=9200)\n",
    "res = es.search(index=\"iperf3*\",\n",
    "                # doc_type=\"articles\",\n",
    "                body={\"query\": \n",
    "                        {\"match\": \n",
    "                                {\"content\": \"\"}\n",
    "                        }\n",
    "                      })\n",
    "print(\"res: \", res)\n",
    "\n",
    "print(f\"{res['hits']['total']} documents found\")\n",
    "\n",
    "for doc in res['hits']['hits']:\n",
    "    print(\"%s) %s\" % (doc['_id'], doc['_source']['content']))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Querying elasticsearch via elasticsearch-py module"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# def get_data_from_elastic():\n",
    "    \n",
    "# query: The elasticsearch query.\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"symbols.keyword\": \"\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Scan function to get all the data. \n",
    "rel = scan( client=es,             \n",
    "            query=query,                                     \n",
    "            scroll='1m',\n",
    "            index='iperf3*',\n",
    "            raise_on_error=True,\n",
    "            preserve_order=True,\n",
    "            clear_scroll=True)\n",
    "\n",
    "# Keep response in a list.\n",
    "if not rel==None:\n",
    "    result = list(rel)\n",
    "\n",
    "print(result)\n",
    "\n",
    "# temp = []\n",
    "\n",
    "# # We need only '_source', which has all the fields required.\n",
    "# # This elimantes the elasticsearch metdata like _id, _type, _index.\n",
    "# for hit in result:\n",
    "#     temp.append(hit['_source'])\n",
    "\n",
    "# # Create a dataframe.\n",
    "# df = pd.DataFrame(temp)\n",
    "\n",
    "# return df\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Querying elasticsearch via REST api"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def search(uri, term):\n",
    "    \"\"\"\n",
    "    Simple Elasticsearch Query\n",
    "    \"\"\"\n",
    "    query = json.dumps({\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"content\": term\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    response = requests.get(uri, data=query)\n",
    "    results = json.loads(response.text)\n",
    "    return results\n",
    "\n",
    "def format_results(results):\n",
    "    \"\"\"\n",
    "    Print results nicely:\n",
    "    doc_id) content\n",
    "    \"\"\"\n",
    "    data = [doc for doc in results['hits']['hits']]\n",
    "    for doc in data:\n",
    "        print(\"%s) %s\" % (doc['_id'], doc['_source']['content']))\n",
    "\n",
    "def create_doc(uri, doc_data={}):\n",
    "    \"\"\"\n",
    "    Create new document.\n",
    "    \"\"\"\n",
    "    query = json.dumps(doc_data)\n",
    "    response = requests.post(uri, data=query)\n",
    "    print(response)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# df = get_data_from_elastic()\n",
    "# print(df.head())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if __name__ == '__main__':\n",
    "    uri_search = 'http://192.168.120.46:9200/'\n",
    "    # uri_search = 'http://192.168.120.46:9200/test/articles/_search'\n",
    "    # uri_create = 'http://localhost:9200/test/articles/'\n",
    "\n",
    "    results = search(uri_search, \"_source\")\n",
    "    print(results)\n",
    "    # format_results(results)\n",
    "\n",
    "    #create_doc(uri_create, {\"content\": \"The fox!\"})\n",
    "    #results = search(uri_search, \"fox\")\n",
    "    #format_results(results)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# !curl -XGET 192.168.120.46:9200/iperf3*/_search | jq >> sample.txt"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import subprocess\n",
    "\n",
    "list_files = subprocess.run([\"ls\", \"-al\"])\n",
    "print(\"The exit code was: %d\" % list_files.returncode)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan\n",
    "\n",
    "# Create the elasticsearch client\n",
    "HOST = 'nersc-tbn-6.testbed100.es.net'\n",
    "PORT = 9200\n",
    "\n",
    "es = Elasticsearch(host=HOST, port=PORT, timeout=30)\n",
    "\n",
    "# all = es.indices.get_alias(\"*\").keys()\n",
    "\n",
    "jobmeta = es.indices.get_alias(\"jobmeta-**\").keys()\n",
    "iperf3 = es.indices.get_alias(\"iperf3*\").keys()\n",
    "bbrmon_pscheduler = es.indices.get_alias(\"bbrmon-pscheduler*\").keys()\n",
    "bbrmon_jobmeta = es.indices.get_alias(\"bbrmon-jobmeta*\").keys()\n",
    "\n",
    "# print(len(jobmeta)), len(iperf3))\n",
    "# print(jobmeta, iperf3)\n",
    "print( len(bbrmon_pscheduler), len(bbrmon_jobmeta) )\n",
    "\n",
    "# iperf3 = sorted(iperf3)\n",
    "# jobmeta = sorted(jobmeta)\n",
    "bbrmon_pscheduler = sorted(bbrmon_pscheduler)\n",
    "bbrmon_jobmeta = sorted(bbrmon_jobmeta)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "jobmeta = jobmeta[5:]\n",
    "for j in jobmeta:\n",
    "    print(j)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "key = 'start_time'\n",
    "\n",
    "for j in jobmeta:\n",
    "    print(\"------>\", j)\n",
    "    jobmeta_result = es.search( index=j,\n",
    "                                body={\"query\":{\"match_all\":{}}},\n",
    "                                size=10000,\n",
    "                                )\n",
    "    print(len(jobmeta_result))\n",
    "    for i in range(len(jobmeta_result)):\n",
    "        try:\n",
    "            print(jobmeta_result['hits']['hits'][i]['_source']['pacing'])\n",
    "            print(jobmeta_result['hits']['hits'][i]['_source']['alias'])\n",
    "        except:\n",
    "            print(jobmeta_result['hits']['hits'][i]['_source']['alias'])\n",
    "        print(jobmeta_result['hits']['hits'][i]['_source'].keys())\n",
    "        print()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in range(len(jobmeta_result)):\n",
    "    key = 'start_time'\n",
    "    # print(jobmeta_result['hits']['hits'][i]['_source'].keys())\n",
    "    if key in jobmeta_result['hits']['hits'][i].keys():\n",
    "        print(jobmeta_result['hits']['hits'][i]['_source']['start_time'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "jobmeta_result['hits']['hits'][0]['_source'].keys() #['alias']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for x in jobmeta:\n",
    "    jobmeta_result = es.search( index=x,\n",
    "                                body={\"query\":{\"match_all\":{}}},\n",
    "                                size=1000,\n",
    "                                )\n",
    "    jobmeta_documents = [doc for doc in jobmeta_result['hits']['hits']]\n",
    "    print(f\"{len(jobmeta_documents)} found in index {x}\")\n",
    "\n",
    "    for i in iperf3:\n",
    "        iperf3_results = es.search( index=i,\n",
    "                                    body={\"query\":{\"match_all\":{}}},\n",
    "                                    size=1000,\n",
    "                                   )\n",
    "        iperf3_documents = [doc for doc in iperf3_results['hits']['hits']]\n",
    "        for doc in iperf3_documents:\n",
    "            uuid = doc['_source']['uuid']\n",
    "            for jobdoc in jobmeta_documents:\n",
    "                try:\n",
    "                    iter_uuids = jobdoc[\"_source\"][\"iter_uuids\"]\n",
    "                except:\n",
    "                    pass\n",
    "                if uuid in iter_uuids:\n",
    "                    timestamp = doc['_source']['@timestamp']\n",
    "                    start_dict = doc['_source']['start']\n",
    "                    num_streams = start_dict['test_start']['num_streams']\n",
    "\n",
    "                    print(\"uuid: \", uuid)\n",
    "                    print(\"timestamp: \", timestamp)\n",
    "                    print(\"num_streams: \", num_streams)\n",
    "                    print()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "jobdoc['_source']['hostname']#.keys()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re, datetime\n",
    "\n",
    "for a in all:\n",
    "    try:\n",
    "        match = re.search('\\d{4}.\\d{2}.\\d{2}', a)\n",
    "        date = datetime.datetime.strptime(match.group(), '%Y.%m.%d').date()\n",
    "        # print(f\"{a}  --->   {date}\")\n",
    "    except:\n",
    "        pass\n",
    "        # print(f\"{a}  ---->   Not found!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from_date = datetime.datetime(date.year, date.month, date.day)\n",
    "curr = datetime.datetime.now()\n",
    "to_date = datetime.datetime(curr.year, curr.month, curr.day)\n",
    "\n",
    "print(from_date, to_date)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "date = datetime.datetime(2021,5,15)\n",
    "from_date < date < to_date"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import datetime\n",
    "# curr = datetime.date.today()\n",
    "curr = datetime.datetime.now()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f\"{curr.year}-{curr.month}-{curr.day}\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "iperf3[5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "iperf3_result = es.search( index=iperf3[5],\n",
    "                            body={\"query\":{\"match_all\":{}}},\n",
    "                            size=10000,\n",
    "                            )\n",
    "iperf3_doc = [doc for doc in iperf3_result['hits']['hits']]\n",
    "print(len(iperf3_doc))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(iperf3_doc)\n",
    "iperf3_doc[0]['_source'].keys()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(iperf3_doc[0]['_source']['end'].keys())\n",
    "print(iperf3_doc[0]['_source']['start'].keys())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(iperf3_doc[0]['_source']['intervals'][0].keys())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "iperf3_doc[0]['_source']['end']['streams']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "iperf3_doc[0]['_source']['end']['streams'][0]['sender']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in range(len(bbrmon_jobmeta)):\n",
    "    try:\n",
    "        bbrmon_jobmeta_result = es.search(index=bbrmon_jobmeta[i], \n",
    "                                        body={\"query\":{\"match_all\":{}}},\n",
    "                                        size=10000)\n",
    "        # bbrmon_jobmeta_documents = [doc for doc in bbrmon_jobmeta_result['hits']['hits']]\n",
    "        print(\"Index: \",i)\n",
    "        print(bbrmon_jobmeta_result)\n",
    "    except:\n",
    "        pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan\n",
    "\n",
    "# Create the elasticsearch client\n",
    "HOST = 'nersc-tbn-6.testbed100.es.net'\n",
    "PORT = 9200\n",
    "\n",
    "es = Elasticsearch(host=HOST, port=PORT, timeout=30)\n",
    "\n",
    "bbrmon_pscheduler = es.indices.get_alias(\"bbrmon-pscheduler*\").keys()\n",
    "bbrmon_jobmeta = es.indices.get_alias(\"bbrmon-jobmeta*\").keys()\n",
    "bbrmon_ss = es.indices.get_alias(\"bbrmon-ss*\").keys()\n",
    "\n",
    "bbrmon_pscheduler = sorted(bbrmon_pscheduler)\n",
    "bbrmon_jobmeta = sorted(bbrmon_jobmeta)\n",
    "bbrmon_ss = sorted(bbrmon_ss)\n",
    "\n",
    "print( len(bbrmon_pscheduler), len(bbrmon_jobmeta), len(bbrmon_ss) )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "164 54 69\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# for i in range(len()):\n",
    "# try:\n",
    "#     bbrmon_jobmeta_result = es.search(index=bbrmon_jobmeta[0],\n",
    "#                                     body={\"query\":{\"match_all\":{}}},\n",
    "#                                     size=10000,\n",
    "#                                     )\n",
    "#     bbrmon_jobmeta_result\n",
    "\n",
    "bbrmon_pscheduler_result = es.search(index=bbrmon_pscheduler[0],\n",
    "                                body={\"query\":{\"match_all\":{}}},\n",
    "                                size=10000,\n",
    "                                )\n",
    "bbrmon_pscheduler_documents = [doc for doc in bbrmon_pscheduler_result['hits']['hits']]\n",
    "\n",
    "bbrmon_pscheduler_documents[0].keys()\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['_index', '_type', '_id', '_score', '_source'])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "timestamp = bbrmon_pscheduler_documents[0]['_source']['@timestamp']#.keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2021-02-11T19:05:08.972Z'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "bbrmon_pscheduler_documents[0]['_source'].keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['@timestamp', 'test', 'tags', 'id', 'reference', '@version', 'result', 'meta', 'schedule', 'pscheduler'])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "uuid = bbrmon_pscheduler_documents[0]['_source']['id']#.keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'8b0cf698-6144-4dc1-8090-c246ba5c3bdc'"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "hostname = bbrmon_pscheduler_documents[0]['_source']['meta']['hostname']#.keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'observer': {'ip': '127.0.0.1', 'hostname': 'localhost', 'geo': {'as': {}}}}"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "bbrmon_pscheduler_documents[0]['_source']['pscheduler'].keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['added', 'end_time', 'task_href', 'run_href', 'test_checksum', 'start_time', 'task_id', 'participants', 'tool', 'run_id', 'duration'])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "bbrmon_pscheduler_documents[0]['_source']['pscheduler']['duration']#.keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "bbrmon_ss_result = es.search(index=bbrmon_ss[0],\n",
    "                            body={\"query\":{\"match_all\":{}}},\n",
    "                            size=10000,\n",
    "                            )\n",
    "bbrmon_ss_documents = [doc for doc in bbrmon_ss_result['hits']['hits']]\n",
    "\n",
    "bbrmon_ss_documents[0].keys()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TransportError",
     "evalue": "TransportError(503, 'search_phase_execution_exception')",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTransportError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-24c083030efd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m bbrmon_ss_result = es.search(index=bbrmon_ss[0],\n\u001b[0m\u001b[1;32m      2\u001b[0m                             \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"match_all\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                             \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                             )\n\u001b[1;32m      5\u001b[0m \u001b[0mbbrmon_ss_documents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbbrmon_ss_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bbrv2/lib/python3.8/site-packages/elasticsearch/client/utils.py\u001b[0m in \u001b[0;36m_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bbrv2/lib/python3.8/site-packages/elasticsearch/client/__init__.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, body, index, doc_type, params, headers)\u001b[0m\n\u001b[1;32m   1668\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"from\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"from_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m         return self.transport.perform_request(\n\u001b[0m\u001b[1;32m   1671\u001b[0m             \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m             \u001b[0m_make_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_search\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bbrv2/lib/python3.8/site-packages/elasticsearch/transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    411\u001b[0m                     \u001b[0;31m# raise exception on last retry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bbrv2/lib/python3.8/site-packages/elasticsearch/transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                 status, headers_response, data = connection.perform_request(\n\u001b[0m\u001b[1;32m    382\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bbrv2/lib/python3.8/site-packages/elasticsearch/connection/http_urllib3.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             )\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         self.log_request_success(\n",
      "\u001b[0;32m~/miniconda3/envs/bbrv2/lib/python3.8/site-packages/elasticsearch/connection/base.py\u001b[0m in \u001b[0;36m_raise_error\u001b[0;34m(self, status_code, raw_data)\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Undecodable raw error response from server: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         raise HTTP_EXCEPTIONS.get(status_code, TransportError)(\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         )\n",
      "\u001b[0;31mTransportError\u001b[0m: TransportError(503, 'search_phase_execution_exception')"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6e2000e74d83ab38a7a73c8353776f8595191be383670aed524d2a15b88663d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('bbrv2': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "f6e2000e74d83ab38a7a73c8353776f8595191be383670aed524d2a15b88663d"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}