{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "# --- System ---\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# --- Utility ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# --- Plot ---\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Pytorch ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# random weight initialization\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "root_dir = os.getcwd()\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# data loading and preprocessing\n",
    "dataPath = \"data/statistics-5.csv\"\n",
    "df = pd.read_csv(dataPath)\n",
    "\n",
    "# Dropping columns that are not required at the moment\n",
    "df = df.drop(columns=[ 'Unnamed: 0', 'UUID', 'HOSTNAME', 'TIMESTAMP',\n",
    "                       'THROUGHPUT (Receiver)', 'LATENCY (mean)', \n",
    "                       'CONGESTION (Receiver)', 'BYTES (Receiver)'\n",
    "                     ])\n",
    "\n",
    "# Pre-processing\n",
    "pacing = df['PACING'].values\n",
    "for i, p in enumerate(pacing):\n",
    "    v, _ = p.split(\"gbit\")\n",
    "    pacing[i] = float(v) # int(v)\n",
    "\n",
    "df['PACING'] = pacing\n",
    "df['CONGESTION (Sender)'] = (df['CONGESTION (Sender)'] == 'cubic').astype(int)\n",
    "df['ALIAS'] = pd.factorize(df['ALIAS'])[0]\n",
    "\n",
    "num_of_classes = len(df['PACING'].unique())\n",
    "\n",
    "X = df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']].values\n",
    "y = df['PACING'].values\n",
    "y = y.astype('int')\n",
    "\n",
    "# Normalization\n",
    "minmax_scale = preprocessing.MinMaxScaler().fit(df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']])\n",
    "df_minmax = minmax_scale.transform(df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']])\n",
    "\n",
    "final_df = pd.DataFrame(df_minmax, columns=['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS'])\n",
    "X = final_df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=1)\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "X_test  = torch.tensor(X_test)\n",
    "y_test  = torch.tensor(y_test)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# Custom data loader for ELK stack dataset\n",
    "class PacingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    TensorDataset with support of transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        y = self.tensors[1][index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# accuracy computation\n",
    "def accuracy(model, ds, pct):\n",
    "    # assumes model.eval()\n",
    "    # percent correct within pct of true pacing rate\n",
    "    n_correct = 0; n_wrong = 0\n",
    "\n",
    "    for i in range(len(ds)):\n",
    "        (X, Y) = ds[i]                # (predictors, target)\n",
    "        X, Y = X.float(), Y.float()\n",
    "        with torch.no_grad():\n",
    "            output, _, _, _ = model(X)         # computed price\n",
    "\n",
    "        abs_delta = np.abs(output.item() - Y.item())\n",
    "        max_allow = np.abs(pct * Y.item())\n",
    "        if abs_delta < max_allow:\n",
    "            n_correct +=1\n",
    "        else:\n",
    "            n_wrong += 1\n",
    "\n",
    "    acc = (n_correct * 1.0) / (n_correct + n_wrong)\n",
    "    return acc*100\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "input_feature = 7\n",
    "latent_feature = 16\n",
    "\n",
    "class VAERegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAERegressor, self).__init__()\n",
    " \n",
    "        # encoder\n",
    "        self.enc1 = nn.Linear(in_features=input_feature, out_features=128)\n",
    "        self.enc2 = nn.Linear(in_features=128, out_features=128)\n",
    "        self.enc3 = nn.Linear(in_features=128, out_features=latent_feature*2)\n",
    "        # decoder\n",
    "        self.dec1 = nn.Linear(in_features=latent_feature, out_features=128)\n",
    "        self.dec2 = nn.Linear(in_features=128, out_features=128)\n",
    "        self.dec3 = nn.Linear(in_features=128, out_features=7)\n",
    "        # Regressor\n",
    "        self.fc1 = torch.nn.Linear (7, 32)\n",
    "        self.fc2 = torch.nn.Linear (32, num_of_classes)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.enc1.weight)\n",
    "        torch.nn.init.zeros_(self.enc1.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.enc2.weight)\n",
    "        torch.nn.init.zeros_(self.enc2.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.dec1.weight)\n",
    "        torch.nn.init.zeros_(self.dec1.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.dec2.weight)\n",
    "        torch.nn.init.zeros_(self.dec2.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.zeros_(self.fc1.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        torch.nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample\n",
    " \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # encoding\n",
    "        x = self.enc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.enc2(x).view(-1, 2, latent_feature)\n",
    "\n",
    "        # get `mu` and `log_var`\n",
    "        mu      = x[:, 0, :]    # the first feature values as mean\n",
    "        log_var = x[:, 1, :]    # the other feature values as variance\n",
    "\n",
    "        # get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    " \n",
    "        # decoding\n",
    "        x = self.dec1(z)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec2(x)\n",
    "        recon = torch.sigmoid(x)\n",
    "\n",
    "        # regressor\n",
    "        x = self.fc1(recon)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, recon, mu, log_var\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "model = VAERegressor()\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCH = 500\n",
    "BATCH = 256\n",
    "LEARNING_RATE = 0.05\n",
    "\n",
    "INTERVAL = 50\n",
    "SAVE = False\n",
    "BESTLOSS = 10\n",
    "\n",
    "CE  = nn.CrossEntropyLoss()\n",
    "BCE = nn.BCELoss(reduction='mean')\n",
    "MSE = nn.MSELoss(reduction='mean') # 'mean', 'sum'. 'none'\n",
    "\n",
    "def criterion(bce_loss, mu, logvar):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (BCELoss) and the \n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    BCE = bce_loss \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "print(\"\\nBatch Size = %3d \" % BATCH)\n",
    "print(\"Loss = \" + str(criterion))\n",
    "print(\"Pptimizer = Adam\")\n",
    "print(\"Max Epochs = %3d \" % EPOCH)\n",
    "print(\"Learning Rate = %0.3f \" % LEARNING_RATE)\n",
    "\n",
    "# Dataset w/o any tranformations\n",
    "traindata   = PacingDataset(tensors=(X_train, y_train), transform=None)\n",
    "trainloader = torch.utils.data.DataLoader(traindata, batch_size=BATCH)\n",
    "\n",
    "testdata    = PacingDataset(tensors=(X_test, y_test), transform=None)\n",
    "testloader = torch.utils.data.DataLoader(testdata, batch_size=1) # BATCH)\n",
    "\n",
    "print(\"\\nStarting training with saved checkpoints\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(0, EPOCH):\n",
    "    torch.manual_seed(epoch+1) # recovery reproducibility\n",
    "    epoch_loss = 0             # for one full epoch\n",
    "\n",
    "    for (batch_idx, batch) in enumerate(trainloader):\n",
    "        (xs, ys) = batch                # (predictors, targets)\n",
    "        xs, ys = xs.float(), ys.float()\n",
    "        optimizer.zero_grad()           # prepare gradients\n",
    "\n",
    "        # output = model(xs)            # predicted pacing rate\n",
    "        # loss = criterion(ys, output)  # avg per item in batch\n",
    "        output, recon, mu, log_var = model(xs)\n",
    "        # mse_loss = MSE(ys, output)\n",
    "        bce_loss = BCE(recon, xs)\n",
    "        loss = criterion(bce_loss, mu, log_var) # + mse_loss\n",
    "\n",
    "        epoch_loss += loss.item()       # accumulate averages\n",
    "        loss.backward()                 # compute gradients\n",
    "        optimizer.step()                # update weights\n",
    "\n",
    "    if epoch % INTERVAL == 0:\n",
    "        print(\"Epoch = %4d    Loss = %0.4f\" % (epoch, epoch_loss))\n",
    "\n",
    "        # save checkpoint\n",
    "        dt = time.strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "        fn = str(dt) + str(\"-\") + str(epoch) + \"_ckpt.pt\"\n",
    "\n",
    "        info_dict = {\n",
    "            'epoch' : epoch,\n",
    "            'model_state' : model.state_dict(),\n",
    "            'optimizer_state' : optimizer.state_dict()\n",
    "        }\n",
    "        if SAVE:\n",
    "            torch.save(info_dict, fn)\n",
    "\n",
    "print(\"\\nDone\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Batch Size = 256 \n",
      "Loss = <function criterion at 0x7faf4de29d08>\n",
      "Pptimizer = Adam\n",
      "Max Epochs = 500 \n",
      "Learning Rate = 0.050 \n",
      "\n",
      "Starting training with saved checkpoints\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c90a43f6b325>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# mse_loss = MSE(ys, output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0mbce_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBCE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbce_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# + mse_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2525\u001b[0m     return torch._C._nn.binary_cross_entropy(\n\u001b[0;32m-> 2526\u001b[0;31m         input, target, weight, reduction_enum)\n\u001b[0m\u001b[1;32m   2527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# evaluate model accuracy\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.float(), labels.float()\n",
    "\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs, recon, mu, log_var = model(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the 10000 test images: 1 %\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "# print(f\"Accuracy (within {gap:.2f}) on train data = {acc_train:.2f}%\")\n",
    "\n",
    "\n",
    "# make prediction on a random sample from test data\n",
    "pivot = random.randint(0, len(testdata))\n",
    "print(\"Pivot: \", pivot)\n",
    "x, y = testdata[pivot]\n",
    "tput, lat, loss, streams, cong = x.detach().numpy()\n",
    "# tput, lat, loss, streams, cong = 0.149677, 0.577766, 1.00000, 0.0, 1.0\n",
    "print(f\"\\nPredicting pacing rate for:\\n\\\n",
    "    (norm. values)\\n\\\n",
    "    throughput = {tput}\\n\\\n",
    "    latency = {lat}\\n\\\n",
    "    loss = {loss}\\n\\\n",
    "    congestion = {cong}\\n\\\n",
    "    streams = {streams}\\n\\\n",
    "    pacing = {y}\")\n",
    "\n",
    "# converting the sample to tensor array\n",
    "ukn = np.array([[tput, lat, loss, streams, cong]], dtype=np.float32)\n",
    "sample = torch.tensor(ukn, dtype=torch.float32).to(device)\n",
    "\n",
    "# testing the sample\n",
    "with torch.no_grad():\n",
    "    pred, _, _, _ = model(sample)\n",
    "pred = pred.item()\n",
    "print(f\"\\nPredicted Pacing rate: {pred:.4f}\\nGround-truth Pacing rate: {y:.4f}\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pivot:  228\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 5)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-076440e046af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pivot: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpivot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcong\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# tput, lat, loss, streams, cong = 0.149677, 0.577766, 1.00000, 0.0, 1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m print(f\"\\nPredicting pacing rate for:\\n\\\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 5)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# evaluate model accuracy\n",
    "model.eval()\n",
    "\n",
    "gap = 0.50\n",
    "acc_train = accuracy(model, traindata, gap)\n",
    "print(f\"Accuracy (within {gap:.2f}) on train data = {acc_train:.2f}%\")\n",
    "\n",
    "\n",
    "# make prediction on a random sample from test data\n",
    "pivot = random.randint(0, len(testdata))\n",
    "print(\"Pivot: \", pivot)\n",
    "x, y = testdata[pivot]\n",
    "tput, lat, loss, streams, cong = x.detach().numpy()\n",
    "# tput, lat, loss, streams, cong = 0.149677, 0.577766, 1.00000, 0.0, 1.0\n",
    "print(f\"\\nPredicting pacing rate for:\\n\\\n",
    "    (norm. values)\\n\\\n",
    "    throughput = {tput}\\n\\\n",
    "    latency = {lat}\\n\\\n",
    "    loss = {loss}\\n\\\n",
    "    congestion = {cong}\\n\\\n",
    "    streams = {streams}\\n\\\n",
    "    pacing = {y}\")\n",
    "\n",
    "# converting the sample to tensor array\n",
    "ukn = np.array([[tput, lat, loss, streams, cong]], dtype=np.float32)\n",
    "sample = torch.tensor(ukn, dtype=torch.float32).to(device)\n",
    "\n",
    "# testing the sample\n",
    "with torch.no_grad():\n",
    "    pred, _, _, _ = model(sample)\n",
    "pred = pred.item()\n",
    "print(f\"\\nPredicted Pacing rate: {pred:.4f}\\nGround-truth Pacing rate: {y:.4f}\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy (within 0.50) on train data = 62.83%\n",
      "Pivot:  457\n",
      "\n",
      "Predicting pacing rate for:\n",
      "    (norm. values)\n",
      "    throughput = 0.9791648790420612\n",
      "    latency = 0.02891032754538912\n",
      "    loss = 0.017566555457207055\n",
      "    congestion = 1.0\n",
      "    streams = 0.2\n",
      "    pacing = 5\n",
      "\n",
      "Predicted Pacing rate: 4.8019\n",
      "Ground-truth Pacing rate: 5.0000\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# evaluate model accuracy\n",
    "model.eval()\n",
    "\n",
    "gap = 0.50\n",
    "acc_train = accuracy(model, traindata, gap)\n",
    "print(f\"Accuracy (within {gap:.2f}) on train data = {acc_train:.2f}%\")\n",
    "\n",
    "\n",
    "# make prediction on a random sample from test data\n",
    "pivot = random.randint(0, len(testdata))\n",
    "print(\"Pivot: \", pivot)\n",
    "x, y = testdata[pivot]\n",
    "tput, lat, loss, streams, cong = x.detach().numpy()\n",
    "# tput, lat, loss, streams, cong = 0.149677, 0.577766, 1.00000, 0.0, 1.0\n",
    "print(f\"\\nPredicting pacing rate for:\\n\\\n",
    "    (norm. values)\\n\\\n",
    "    throughput = {tput}\\n\\\n",
    "    latency = {lat}\\n\\\n",
    "    loss = {loss}\\n\\\n",
    "    congestion = {cong}\\n\\\n",
    "    streams = {streams}\\n\\\n",
    "    pacing = {y}\")\n",
    "\n",
    "# converting the sample to tensor array\n",
    "ukn = np.array([[tput, lat, loss, streams, cong]], dtype=np.float32)\n",
    "sample = torch.tensor(ukn, dtype=torch.float32).to(device)\n",
    "\n",
    "# testing the sample\n",
    "with torch.no_grad():\n",
    "    pred, _, _, _ = model(sample)\n",
    "pred = pred.item()\n",
    "print(f\"\\nPredicted Pacing rate: {pred:.4f}\\nGround-truth Pacing rate: {y:.4f}\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy (within 0.50) on train data = 62.83%\n",
      "Pivot:  285\n",
      "\n",
      "Predicting pacing rate for:\n",
      "    (norm. values)\n",
      "    throughput = 0.5268682123236014\n",
      "    latency = 0.1797940649802828\n",
      "    loss = 0.1363888375531325\n",
      "    congestion = 0.0\n",
      "    streams = 0.0\n",
      "    pacing = 6\n",
      "\n",
      "Predicted Pacing rate: 4.8022\n",
      "Ground-truth Pacing rate: 6.0000\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}