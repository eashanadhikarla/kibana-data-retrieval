{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "==============================================\n",
    "Name   : Eashan Adhikarla\n",
    "Course : CSE498 - Adversarial Machine Learning\n",
    "==============================================\n",
    "\n",
    "Homework 1\n",
    "\"\"\"\n",
    "# --- Sklearn ---\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn import decomposition, discriminant_analysis, linear_model, svm, tree, neural_network\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# --- Models ---\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn import neural_network\n",
    "\n",
    "# --- Utility ---\n",
    "import os\n",
    "import pickle, torch\n",
    "import numpy as np, pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rootdir = os.getcwd()\n",
    "\n",
    "# data loading and preprocessing\n",
    "dataPath = \"data/statistics-5.csv\"\n",
    "df = pd.read_csv(dataPath)\n",
    "# ----------------------------------\n",
    "# Dropping columns that are not required at the moment\n",
    "df = df.drop(columns=['Unnamed: 0', 'UUID', 'HOSTNAME', 'TIMESTAMP', 'THROUGHPUT (Receiver)', 'LATENCY (mean)', 'CONGESTION (Receiver)', 'BYTES (Receiver)'])\n",
    "\n",
    "# Pre-processing\n",
    "pacing = df['PACING'].values\n",
    "for i, p in enumerate(pacing):\n",
    "    v, _ = p.split(\"gbit\")\n",
    "    pacing[i] = float(v) # int(v)\n",
    "\n",
    "df['PACING'] = pacing\n",
    "df['CONGESTION (Sender)'] = (df['CONGESTION (Sender)'] == 'cubic').astype(int)\n",
    "df['ALIAS'] = pd.factorize(df['ALIAS'])[0]\n",
    "\n",
    "num_of_classes = len(df['PACING'].unique())\n",
    "\n",
    "X = df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']].values\n",
    "y = df['PACING'].values\n",
    "y = y.astype('int')\n",
    "\n",
    "# Normalization\n",
    "minmax_scale = preprocessing.MinMaxScaler().fit(df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']])\n",
    "df_minmax = minmax_scale.transform(df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']])\n",
    "\n",
    "final_df = pd.DataFrame(df_minmax, columns=['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS'])\n",
    "X = final_df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']].values\n",
    "# ----------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=1)\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "X_test  = torch.tensor(X_test)\n",
    "y_test  = torch.tensor(y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_and_tune(X, y, model, parameters, scoring='f1_macro', kfold=5, verbose=0):\n",
    "    \"\"\"\n",
    "    X:          array-like of shape (n_samples, n_features)\n",
    "    y:          array-like of shape (n_samples,)\n",
    "    model:      (object) a sklearn model class\n",
    "    parameters: (dict) contains the parameters you want to tune in the model\n",
    "    metric:     (str) the metric used to evaluate the quality of the model\n",
    "    return:     a trained model with the best parameters\n",
    "    \"\"\"\n",
    "    cvSearchObj = GridSearchCV(model,\n",
    "                               parameters,\n",
    "                               scoring=scoring,\n",
    "                               n_jobs=-1,\n",
    "                               cv=kfold,\n",
    "                               verbose=verbose)\n",
    "    cvSearchObj.fit(X,y)\n",
    "    return cvSearchObj.best_estimator_\n",
    "\n",
    "def save_model(filename, model):\n",
    "    \"\"\"\n",
    "    filename: Filename to save the model\n",
    "    model:    Model weights to be saved\n",
    "    \"\"\"\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    print(\"Model Saved\")\n",
    "\n",
    "def load_model(filename):\n",
    "    \"\"\"\n",
    "    filename: Filename to load the model\n",
    "    return:   Model weights that are reloaded\n",
    "    \"\"\"\n",
    "    model_reloaded = pickle.load(open(filename, 'rb'))\n",
    "    return model_reloaded\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "==================================\n",
    "Method 1: Decision Tree Classifier\n",
    "==================================\n",
    "'''\n",
    "def DecisionTree(train, save, test): \n",
    "    filename = str(rootdir)+\"checkpoint/traditional/dtreeBest_\"+str(dataset)+\".pkl\"\n",
    "    decisiontreeclassifier = DecisionTreeClassifier(random_state=999)\n",
    "    if train:\n",
    "        '''\n",
    "        Train\n",
    "        '''\n",
    "        params = {'min_samples_leaf':[1,2,3]}\n",
    "\n",
    "        dtreeBest = train_and_tune(X_train,\n",
    "                                   y_train,\n",
    "                                   decisiontreeclassifier,\n",
    "                                   params,\n",
    "                                   scoring='f1_macro',\n",
    "                                   kfold=5)\n",
    "\n",
    "        if save:\n",
    "            save_model(filename, dtreeBest)\n",
    "\n",
    "    if test:\n",
    "        '''\n",
    "        Test\n",
    "        '''\n",
    "        dtreeBest_reloaded = load_model(filename)\n",
    "        pred = dtreeBest_reloaded.predict(X_test)\n",
    "        acc  = dtreeBest_reloaded.score(X_test, y_test)\n",
    "        \n",
    "        # cf_matrix = confusion_matrix(Y_test, pred)\n",
    "        # df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in classes],\n",
    "        #                      columns = [i for i in classes])\n",
    "        # plt.figure(figsize = (12,10))\n",
    "        # sn.heatmap(df_cm, annot=True)\n",
    " \n",
    "        print(\"Accuracy: \", acc)\n",
    "    print(\"Method-1 completed!\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "DecisionTree(train=True, save=False, test=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"Load the Boston dataset and examine its target (label) distribution.\"\"\"\n",
    "\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "################################\n",
    "### ADD EXTRA LIBRARIES HERE ###\n",
    "################################\n",
    "from sklearn.metrics import mean_squared_error,median_absolute_error,r2_score,mean_absolute_error\n",
    "from sklearn import grid_search\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load the Boston dataset.\"\"\"\n",
    "\n",
    "    boston = datasets.load_boston()\n",
    "    return boston\n",
    "\n",
    "\n",
    "def explore_city_data(city_data):\n",
    "    \"\"\"Calculate the Boston housing statistics.\"\"\"\n",
    "\n",
    "    # Get the labels and features from the housing data\n",
    "    housing_prices = city_data.target\n",
    "    housing_features = city_data.data\n",
    "\n",
    "    ###################################\n",
    "    ### Step 1. YOUR CODE GOES HERE ###\n",
    "    ###################################\n",
    "\n",
    "    # Please calculate the following values using the Numpy library\n",
    "    # Size of data (number of houses)?\n",
    "    # Number of features?\n",
    "    # Minimum price?\n",
    "    # Maximum price?\n",
    "    # Calculate mean price?\n",
    "    # Calculate median price?\n",
    "    # Calculate standard deviation?\n",
    "    number_of_houses = housing_features.shape[0]\n",
    "    number_of_features = housing_features.shape[1]\n",
    "    max_price = np.max(housing_prices)\n",
    "    min_price = np.min(housing_prices)\n",
    "    mean_price = np.mean(housing_prices)\n",
    "    median_price = np.median(housing_prices)\n",
    "    standard_deviation = np.std(housing_prices)\n",
    "\n",
    "    print \"number of houses:\",number_of_houses\n",
    "    print \"number of features:\",number_of_features\n",
    "    print \"max price of house:\",max_price\n",
    "    print \"min price of house:\",min_price\n",
    "    print \"mean price of house:\",mean_price\n",
    "    print \"median price of house:\",median_price\n",
    "    print \"standard deviation for prices of house:\",standard_deviation\n",
    "\n",
    "def performance_metric(label, prediction):\n",
    "    \"\"\"Calculate and return the appropriate error performance metric.\"\"\"\n",
    "\n",
    "    ###################################\n",
    "    ### Step 2. YOUR CODE GOES HERE ###\n",
    "    ###################################\n",
    "\n",
    "    # http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics\n",
    "    #return median_absolute_error(label, prediction)\n",
    "    #return r2_score(label, prediction)\n",
    "    #return mean_absolute_error(label, prediction)\n",
    "    return mean_squared_error(label,prediction)\n",
    "    pass\n",
    "\n",
    "\n",
    "def split_data(city_data):\n",
    "    \"\"\"Randomly shuffle the sample set. Divide it into 70 percent training and 30 percent testing data.\"\"\"\n",
    "\n",
    "    # Get the features and labels from the Boston housing data\n",
    "    X, y = city_data.data, city_data.target\n",
    "\n",
    "    ###################################\n",
    "    ### Step 3. YOUR CODE GOES HERE ###\n",
    "    ###################################\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "         X, y, test_size=0.30, train_size=0.70, random_state=42)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def learning_curve(depth, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Calculate the performance of the model after a set of training data.\"\"\"\n",
    "\n",
    "    # We will vary the training set size so that we have 50 different sizes\n",
    "    sizes = np.linspace(1, len(X_train), 50)\n",
    "    train_err = np.zeros(len(sizes))\n",
    "    test_err = np.zeros(len(sizes))\n",
    "\n",
    "    print \"Decision Tree with Max Depth: \"\n",
    "    print depth\n",
    "    \n",
    "\n",
    "\n",
    "    for i, s in enumerate(sizes):\n",
    "\n",
    "        # Create and fit the decision tree regressor model\n",
    "        regressor = DecisionTreeRegressor(max_depth=depth)\n",
    "        regressor.fit(X_train[:s], y_train[:s])\n",
    "\n",
    "        # Find the performance on the training and testing set\n",
    "        train_err[i] = performance_metric(y_train[:s], regressor.predict(X_train[:s]))\n",
    "        \n",
    "        test_err[i] = performance_metric(y_test, regressor.predict(X_test))\n",
    "\n",
    "    \n",
    "\n",
    "    pl.figure()\n",
    "    pl.plot(y_train - regressor.predict(X_train))\n",
    "    pl.savefig(\"residual_plot.png\")\n",
    "    # Plot learning curve graph\n",
    "    learning_curve_graph(sizes, train_err, test_err, depth)\n",
    "\n",
    "\n",
    "def learning_curve_graph(sizes, train_err, test_err, depth):\n",
    "    \"\"\"Plot training and test error as a function of the training size.\"\"\"\n",
    "\n",
    "    pl.figure()\n",
    "    pl.title('Decision Trees: Performance vs Training Size')\n",
    "    pl.plot(sizes, test_err, lw=2, label = 'test error')\n",
    "    pl.plot(sizes, train_err, lw=2, label = 'training error')\n",
    "    pl.legend()\n",
    "    pl.xlabel('Training Size')\n",
    "    pl.ylabel('Error')\n",
    "    #pl.show()\n",
    "    pl.savefig(\"learning_curve\"+\"_\"+str(depth)+\".png\")\n",
    "\n",
    "\n",
    "def model_complexity(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Calculate the performance of the model as model complexity increases.\"\"\"\n",
    "\n",
    "    print \"Model Complexity: \"\n",
    "\n",
    "    # We will vary the depth of decision trees from 2 to 25\n",
    "    max_depth = np.arange(1, 25)\n",
    "    train_err = np.zeros(len(max_depth))\n",
    "    test_err = np.zeros(len(max_depth))\n",
    "\n",
    "    for i, d in enumerate(max_depth):\n",
    "        # Setup a Decision Tree Regressor so that it learns a tree with depth d\n",
    "        regressor = DecisionTreeRegressor(max_depth=d)\n",
    "\n",
    "        # Fit the learner to the training data\n",
    "        regressor.fit(X_train, y_train)\n",
    "\n",
    "        # Find the performance on the training set\n",
    "        train_err[i] = performance_metric(y_train, regressor.predict(X_train))\n",
    "\n",
    "        # Find the performance on the testing set\n",
    "        test_err[i] = performance_metric(y_test, regressor.predict(X_test))\n",
    "\n",
    "    # Plot the model complexity graph\n",
    "    model_complexity_graph(max_depth, train_err, test_err)\n",
    "\n",
    "\n",
    "def model_complexity_graph(max_depth, train_err, test_err):\n",
    "    \"\"\"Plot training and test error as a function of the depth of the decision tree learn.\"\"\"\n",
    "\n",
    "    pl.figure()\n",
    "    pl.title('Decision Trees: Performance vs Max Depth')\n",
    "    pl.plot(max_depth, test_err, lw=2, label = 'test error')\n",
    "    pl.plot(max_depth, train_err, lw=2, label = 'training error')\n",
    "    pl.legend()\n",
    "    pl.xlabel('Max Depth')\n",
    "    pl.ylabel('Error')\n",
    "    #pl.show()\n",
    "    pl.savefig(\"model_complexity.png\")\n",
    "\n",
    "\n",
    "def fit_predict_model(city_data):\n",
    "    \"\"\"Find and tune the optimal model. Make a prediction on housing data.\"\"\"\n",
    "\n",
    "    # Get the features and labels from the Boston housing data\n",
    "    X, y = city_data.data, city_data.target\n",
    "\n",
    "    # Setup a Decision Tree Regressor\n",
    "    regressor = DecisionTreeRegressor()\n",
    "\n",
    "    parameters = {'max_depth':(1,2,3,4,5,6,7,8,9,10),\n",
    "        'min_samples_split': (1, 2, 3),\n",
    "        'min_samples_leaf': (1, 2, 3)\n",
    "    }\n",
    "\n",
    "    ###################################\n",
    "    ### Step 4. YOUR CODE GOES HERE ###\n",
    "    ###################################\n",
    "\n",
    "    # 1. Find the best performance metric\n",
    "    # should be the same as your performance_metric procedure\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\n",
    "\n",
    "    # 2. Use gridearch to fine tune the Decision Tree Regressor and find the best model\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV\n",
    "\n",
    "    regressors = grid_search.GridSearchCV(regressor, parameters, scoring='mean_squared_error')\n",
    "\n",
    "    regressors.fit(X,y)\n",
    "\n",
    "    # pick the best\n",
    "    reg = regressors.best_estimator_\n",
    "\n",
    "    # Fit the learner to the training data\n",
    "    print \"Final Model: \"\n",
    "    print reg.fit(X, y)\n",
    "    \n",
    "    # Use the model to predict the output of a particular sample\n",
    "    x = [11.95, 0.00, 18.100, 0, 0.6590, 5.6090, 90.00, 1.385, 24, 680.0, 20.20, 332.09, 12.13]\n",
    "    y = reg.predict(x)\n",
    "    print \"House: \" + str(x)\n",
    "    print \"Prediction: \" + str(y)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Analyze the Boston housing data. Evaluate and validate the\n",
    "    performanance of a Decision Tree regressor on the housing data.\n",
    "    Fine tune the model to make prediction on unseen data.\"\"\"\n",
    "\n",
    "    # Load data\n",
    "    city_data = load_data()\n",
    "\n",
    "    # Explore the data\n",
    "    explore_city_data(city_data)\n",
    "\n",
    "    # Training/Test dataset split\n",
    "    X_train, y_train, X_test, y_test = split_data(city_data)\n",
    "\n",
    "    # Learning Curve Graphs\n",
    "    max_depths = [1,2,3,4,5,6,7,8,9,10]\n",
    "    for max_depth in max_depths:\n",
    "        learning_curve(max_depth, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Model Complexity Graph\n",
    "    model_complexity(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Tune and predict Model\n",
    "    fit_predict_model(city_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "{\"mode\":\"full\",\"isActive\":false}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# from __future__ import absolute_import, print_function\n",
    "\n",
    "# # --- System ---\n",
    "# import os\n",
    "# import sys\n",
    "# import time\n",
    "# import warnings\n",
    "\n",
    "# # --- Utility ---\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import math\n",
    "# import random\n",
    "# import logging\n",
    "# import pickle\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn import preprocessing\n",
    "\n",
    "# # --- Plot ---\n",
    "# import matplotlib.pyplot as plt\n",
    "# # %matplotlib inline\n",
    "# import seaborn as sns\n",
    "\n",
    "# # --- Pytorch ---\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# import torch.backends.cudnn as cudnn\n",
    "\n",
    "# from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "# from tqdm import tqdm\n",
    "# from datetime import datetime\n",
    "# from torch.utils.data import random_split\n",
    "\n",
    "# from lib.dataloader import PacingDataset\n",
    "# from lib.classifier import PacingClassifier\n",
    "# import lib.utils\n",
    "\n",
    "# # random weight initialization\n",
    "# def seed_everything(seed=42):\n",
    "#     random.seed(seed)\n",
    "#     os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# seed_everything()\n",
    "# # ----------------------------------\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# root_dir = os.getcwd()\n",
    "\n",
    "# # data loading and preprocessing\n",
    "# dataPath = \"data/statistics-5.csv\"\n",
    "# df = pd.read_csv(dataPath)\n",
    "# # ----------------------------------\n",
    "# # Dropping columns that are not required at the moment\n",
    "# df = df.drop(columns=['Unnamed: 0', 'UUID', 'HOSTNAME', 'TIMESTAMP', 'THROUGHPUT (Receiver)', 'LATENCY (mean)', 'CONGESTION (Receiver)', 'BYTES (Receiver)'])\n",
    "\n",
    "# # Pre-processing\n",
    "# pacing = df['PACING'].values\n",
    "# for i, p in enumerate(pacing):\n",
    "#     v, _ = p.split(\"gbit\")\n",
    "#     pacing[i] = float(v) # int(v)\n",
    "\n",
    "# df['PACING'] = pacing\n",
    "# df['CONGESTION (Sender)'] = (df['CONGESTION (Sender)'] == 'cubic').astype(int)\n",
    "# df['ALIAS'] = pd.factorize(df['ALIAS'])[0]\n",
    "\n",
    "# num_of_classes = len(df['PACING'].unique())\n",
    "\n",
    "# X = df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']].values\n",
    "# y = df['PACING'].values\n",
    "# y = y.astype('int')\n",
    "\n",
    "# # Normalization\n",
    "# minmax_scale = preprocessing.MinMaxScaler().fit(df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']])\n",
    "# df_minmax = minmax_scale.transform(df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']])\n",
    "\n",
    "# final_df = pd.DataFrame(df_minmax, columns=['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS'])\n",
    "# X = final_df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']].values\n",
    "# # ----------------------------------\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "#                                                     test_size=0.25,\n",
    "#                                                     random_state=1)\n",
    "\n",
    "# X_train = torch.tensor(X_train)\n",
    "# y_train = torch.tensor(y_train)\n",
    "# X_test  = torch.tensor(X_test)\n",
    "# y_test  = torch.tensor(y_test)\n",
    "\n",
    "# # Hyperparameters\n",
    "# EPOCH = 1000\n",
    "# BATCH = 512\n",
    "# LEARNING_RATE = 0.05\n",
    "\n",
    "# INTERVAL = 50\n",
    "# SAVE = False\n",
    "# BESTLOSS = 10\n",
    "\n",
    "# CE  = nn.CrossEntropyLoss()\n",
    "# BCE = nn.BCELoss(reduction='mean')\n",
    "# MSE = nn.MSELoss(reduction='mean') # 'mean', 'sum'. 'none'\n",
    "\n",
    "# # Dataset w/o any tranformations\n",
    "# traindata   = PacingDataset(tensors=(X_train, y_train), transform=None)\n",
    "# trainloader = torch.utils.data.DataLoader(traindata, batch_size=BATCH)\n",
    "\n",
    "# testdata    = PacingDataset(tensors=(X_test, y_test), transform=None)\n",
    "# testloader = torch.utils.data.DataLoader(testdata, batch_size=1) # BATCH)\n",
    "\n",
    "# inputFea = len(traindata[0][0])\n",
    "# model = PacingClassifier (nc=num_of_classes, inputFeatures=inputFea)\n",
    "# print(model)\n",
    "\n",
    "# # optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# print(\"\\nBatch Size = %3d \" % BATCH)\n",
    "# print(\"Loss = \" + str(CE))\n",
    "# print(\"Optimizer = SGD\")\n",
    "# print(\"Max Epochs = %3d \" % EPOCH)\n",
    "# print(\"Learning Rate = %0.3f \" % LEARNING_RATE)\n",
    "# print(\"Number of Classes = %d \" % num_of_classes)\n",
    "\n",
    "# print(\"\\nStarting training with saved checkpoints\")\n",
    "\n",
    "# model.train()\n",
    "# for epoch in range(0, EPOCH):\n",
    "#     torch.manual_seed(epoch+1) # recovery reproducibility\n",
    "#     epoch_loss = 0             # for one full epoch\n",
    "\n",
    "#     for (batch_idx, batch) in enumerate(trainloader):\n",
    "#         (xs, ys) = batch                # (predictors, targets)\n",
    "#         xs, ys = xs.float(), ys.float()\n",
    "#         optimizer.zero_grad()           # prepare gradients\n",
    "\n",
    "#         output = model(xs)              # predicted pacing rate\n",
    "#         loss = CE(output, ys.long())    # avg per item in batch\n",
    "\n",
    "#         epoch_loss += loss.item()       # accumulate averages\n",
    "#         loss.backward()                 # compute gradients\n",
    "#         optimizer.step()                # update weights\n",
    "\n",
    "#     if epoch % INTERVAL == 0:\n",
    "#         print(\"Epoch = %4d    Loss = %0.4f\" % (epoch, epoch_loss))\n",
    "\n",
    "#         # save checkpoint\n",
    "#         dt = time.strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "#         fn = str(dt) + str(\"-\") + str(epoch) + \"_ckpt.pt\"\n",
    "\n",
    "#         info_dict = {\n",
    "#             'epoch' : epoch,\n",
    "#             'model_state' : model.state_dict(),\n",
    "#             'optimizer_state' : optimizer.state_dict()\n",
    "#         }\n",
    "#         if SAVE:\n",
    "#             torch.save(info_dict, fn)\n",
    "\n",
    "# print(\"\\nDone\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PacingClassifier(\n",
      "  (fc1): Linear(in_features=7, out_features=32, bias=True)\n",
      "  (drop1): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (drop2): Dropout(p=0.7, inplace=False)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (drop3): Dropout(p=0.7, inplace=False)\n",
      "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (drop4): Dropout(p=0.7, inplace=False)\n",
      "  (fc5): Linear(in_features=32, out_features=21, bias=True)\n",
      ")\n",
      "\n",
      "Batch Size = 512 \n",
      "Loss = CrossEntropyLoss()\n",
      "Optimizer = SGD\n",
      "Max Epochs = 1000 \n",
      "Learning Rate = 0.050 \n",
      "Number of Classes = 21 \n",
      "\n",
      "Starting training with saved checkpoints\n",
      "Epoch =    0    Loss = 24.5920\n",
      "Epoch =   50    Loss = 17.9660\n",
      "Epoch =  100    Loss = 16.5130\n",
      "Epoch =  150    Loss = 16.1677\n",
      "Epoch =  200    Loss = 15.8473\n",
      "Epoch =  250    Loss = 15.7375\n",
      "Epoch =  300    Loss = 15.4794\n",
      "Epoch =  350    Loss = 15.6110\n",
      "Epoch =  400    Loss = 15.4521\n",
      "Epoch =  450    Loss = 15.5056\n",
      "Epoch =  500    Loss = 15.3059\n",
      "Epoch =  550    Loss = 15.4462\n",
      "Epoch =  600    Loss = 15.3075\n",
      "Epoch =  650    Loss = 15.2869\n",
      "Epoch =  700    Loss = 15.2204\n",
      "Epoch =  750    Loss = 15.2463\n",
      "Epoch =  800    Loss = 15.2868\n",
      "Epoch =  850    Loss = 15.1561\n",
      "Epoch =  900    Loss = 15.1532\n",
      "Epoch =  950    Loss = 15.1698\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "correct, acc, total = 0, 0, 0\n",
    "with torch.no_grad():\n",
    "    for xs, ys in testloader:\n",
    "        xs, ys = xs.float(), ys.long()\n",
    "\n",
    "        output = model(xs)\n",
    "        \n",
    "        mse_loss = MSE(ys, output)\n",
    "        bce_loss = BCE(recon, xs)\n",
    "        loss = criterion(bce_loss, mu, log_var) + mse_loss\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total += ys.size(0)\n",
    "        pred = torch.max(output, 1)[1]\n",
    "        correct += (pred == ys).sum().item()\n",
    "    acc = (100 * correct / total)\n",
    "print(acc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('bbrv2': conda)"
  },
  "interpreter": {
   "hash": "f6e2000e74d83ab38a7a73c8353776f8595191be383670aed524d2a15b88663d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}