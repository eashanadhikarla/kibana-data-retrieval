{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "# --- System ---\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# --- Utility ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# --- Plot ---\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Pytorch ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# random weight initialization\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "root_dir = os.getcwd()\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# data loading and preprocessing\n",
    "dataPath = \"data/statistics-4.csv\" # \"data/statistics (pacing).csv\"\n",
    "df = pd.read_csv(dataPath)\n",
    "\n",
    "# Dropping columns that are not required at the moment\n",
    "df = df.drop(columns=[ 'Unnamed: 0', 'UUID', 'HOSTNAME', 'ALIAS', 'TIMESTAMP',\n",
    "                       'THROUGHPUT (Receiver)', 'LATENCY (min.)', 'LATENCY (max.)', \n",
    "                       'CONGESTION (Receiver)', 'BYTES (Receiver)'\n",
    "                     ])\n",
    "\n",
    "# Pre-processing\n",
    "pacing = df['PACING'].values\n",
    "for i, p in enumerate(pacing):\n",
    "    v, _ = p.split(\"gbit\")\n",
    "    pacing[i] = float(v)\n",
    "\n",
    "df['PACING'] = pacing\n",
    "df['CONGESTION (Sender)'] = (df['CONGESTION (Sender)'] == 'cubic').astype(int)\n",
    "\n",
    "X = df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)']].values\n",
    "y = df['PACING'].values\n",
    "y = y.astype('int')\n",
    "\n",
    "# Normalization\n",
    "minmax_scale = preprocessing.MinMaxScaler().fit(df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)']])\n",
    "df_minmax = minmax_scale.transform(df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)']])\n",
    "\n",
    "final_df = pd.DataFrame(df_minmax, columns=['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)'])\n",
    "X = final_df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)']].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=1)\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "X_test  = torch.tensor(X_test)\n",
    "y_test  = torch.tensor(y_test) \n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# Custom data loader for ELK stack dataset\n",
    "class PacingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    TensorDataset with support of transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        y = self.tensors[1][index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# accuracy computation\n",
    "def accuracy(model, ds, pct):\n",
    "    # assumes model.eval()\n",
    "    # percent correct within pct of true pacing rate\n",
    "    n_correct = 0; n_wrong = 0\n",
    "\n",
    "    for i in range(len(ds)):\n",
    "        (X, Y) = ds[i]                # (predictors, target)\n",
    "        X, Y = X.float(), Y.float()\n",
    "        with torch.no_grad():\n",
    "            output = model(X)         # computed price\n",
    "\n",
    "        abs_delta = np.abs(output.item() - Y.item())\n",
    "        max_allow = np.abs(pct * Y.item())\n",
    "        if abs_delta < max_allow:\n",
    "            n_correct +=1\n",
    "        else:\n",
    "            n_wrong += 1\n",
    "\n",
    "    acc = (n_correct * 1.0) / (n_correct + n_wrong)\n",
    "    return acc*100\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# model definition\n",
    "class PacingOptimizer(nn.Module):\n",
    "    # https://visualstudiomagazine.com/Articles/2021/02/11/pytorch-define.aspx?Page=2\n",
    "    def __init__(self):\n",
    "        super(PacingOptimizer, self).__init__()\n",
    "        self.hid1 = torch.nn.Linear(5, 256)\n",
    "        self.drop1 = torch.nn.Dropout(0.50)\n",
    "        \n",
    "        self.hid2 = torch.nn.Linear(256, 480)\n",
    "        self.drop2 = torch.nn.Dropout(0.50)\n",
    "        \n",
    "        self.hid3 = torch.nn.Linear(480, 960)\n",
    "        self.drop3 = torch.nn.Dropout(0.50)\n",
    "\n",
    "        self.hid4 = torch.nn.Linear(960, 256)\n",
    "        self.oupt = torch.nn.Linear(256, 1)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.hid1.weight)\n",
    "        torch.nn.init.zeros_(self.hid1.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.hid2.weight)\n",
    "        torch.nn.init.zeros_(self.hid2.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.hid3.weight)\n",
    "        torch.nn.init.zeros_(self.hid3.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.oupt.weight)\n",
    "        torch.nn.init.zeros_(self.oupt.bias)\n",
    "        self.lrelu = torch.nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # z = self.drop1(torch.relu(self.hid1(x)))\n",
    "        # z = self.drop2(torch.relu(self.hid2(z)))\n",
    "        # z = self.drop3(torch.relu(self.hid3(z)))\n",
    "        # z = torch.relu(self.hid4(z))\n",
    "        # z = self.oupt(z)  # no activation\n",
    "        z = self.drop1(self.lrelu(self.hid1(x)))\n",
    "        z = self.drop2(self.lrelu(self.hid2(z)))\n",
    "        z = self.drop3(self.lrelu(self.hid3(z)))\n",
    "        z = self.lrelu(self.hid4(z))\n",
    "        z = self.oupt(z)  # no activation\n",
    "        return z\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "model = PacingOptimizer()\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCH = 500\n",
    "BATCH = 64\n",
    "LEARNING_RATE = 0.05\n",
    "\n",
    "INTERVAL = 50\n",
    "SAVE = False\n",
    "BESTLOSS = 10\n",
    "\n",
    "criterion = nn.MSELoss(reduction='mean') # 'mean', 'sum'. 'none'\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "print(\"\\nBatch Size = %3d \" % BATCH)\n",
    "print(\"Loss = \" + str(criterion))\n",
    "print(\"Pptimizer = Adam\")\n",
    "print(\"Max Epochs = %3d \" % EPOCH)\n",
    "print(\"Learning Rate = %0.3f \" % LEARNING_RATE)\n",
    "\n",
    "# Dataset w/o any tranformations\n",
    "traindata   = PacingDataset(tensors=(X_train, y_train), transform=None)\n",
    "trainloader = torch.utils.data.DataLoader(traindata, batch_size=BATCH)\n",
    "\n",
    "testdata    = PacingDataset(tensors=(X_test, y_test), transform=None)\n",
    "testloader = torch.utils.data.DataLoader(testdata, batch_size=BATCH)\n",
    "\n",
    "print(\"\\nStarting training with saved checkpoints\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(0, EPOCH):\n",
    "    torch.manual_seed(epoch+1) # recovery reproducibility\n",
    "    epoch_loss = 0             # for one full epoch\n",
    "\n",
    "    for (batch_idx, batch) in enumerate(trainloader):\n",
    "        (xs, ys) = batch                # (predictors, targets)\n",
    "        xs, ys = xs.float(), ys.float()\n",
    "        optimizer.zero_grad()           # prepare gradients\n",
    "\n",
    "        output = model(xs)              # predicted pacing rate\n",
    "        loss = criterion(ys, output)    # avg per item in batch\n",
    "\n",
    "        epoch_loss += loss.item()       # accumulate averages\n",
    "        loss.backward()                 # compute gradients\n",
    "        optimizer.step()                # update weights\n",
    "\n",
    "    if epoch % INTERVAL == 0:\n",
    "        print(\"Epoch = %4d    Loss = %0.4f\" % (epoch, epoch_loss))\n",
    "\n",
    "        # save checkpoint\n",
    "        dt = time.strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "        fn = str(dt) + str(\"-\") + str(epoch) + \"_ckpt.pt\"\n",
    "\n",
    "        info_dict = {\n",
    "            'epoch' : epoch,\n",
    "            'model_state' : model.state_dict(),\n",
    "            'optimizer_state' : optimizer.state_dict()\n",
    "        }\n",
    "        if SAVE:\n",
    "            torch.save(info_dict, fn)\n",
    "\n",
    "print(\"\\nDone\")\n",
    "\n",
    "# evaluate model accuracy\n",
    "model.eval()\n",
    "gap = 0.50\n",
    "acc_train = accuracy(model, traindata, gap)\n",
    "print(f\"Accuracy (within {gap:.2f}) on train data = {acc_train:.2f}%\")\n",
    "\n",
    "\n",
    "# make prediction\n",
    "tput, lat, loss, streams, cong = 0.149677, 0.577766, 1.00000, 0.0, 1.0\n",
    "print(f\"\\nPredicting pacing rate for:\\n\\\n",
    "    (norm. values)\\n\\\n",
    "    throughput = {tput}\\n\\\n",
    "    latency = {lat}\\n\\\n",
    "    loss = {loss}\\n\\\n",
    "    congestion = {cong}\\n\\\n",
    "    streams = {streams}\")\n",
    "\n",
    "# converting the sample to tensor array\n",
    "ukn = np.array([[tput, lat, loss, streams, cong]], dtype=np.float32)\n",
    "sample = torch.tensor(ukn, dtype=torch.float32).to(device)\n",
    "\n",
    "# testing the sample\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    pred = model(sample)\n",
    "pred = pred.item()\n",
    "print(f\"\\nPacing rate: {pred:.4f}\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Batch Size =  64 \n",
      "Loss = MSELoss()\n",
      "Pptimizer = Adam\n",
      "Max Epochs = 500 \n",
      "Learning Rate = 0.050 \n",
      "\n",
      "Starting training with saved checkpoints\n",
      "Epoch =    0    Loss = 2232.5319\n",
      "Epoch =   50    Loss = 406.3627\n",
      "Epoch =  100    Loss = 406.4533\n",
      "Epoch =  150    Loss = 406.4577\n",
      "Epoch =  200    Loss = 406.4579\n",
      "Epoch =  250    Loss = 406.4579\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cf8032831dc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# accumulate averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;31m# compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;31m# update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mINTERVAL\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bbrv2/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bbrv2/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bbrv2/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    108\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bbrv2/lib/python3.8/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "# --- System ---\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# --- Utility ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# --- Plot --\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Pytorch ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "root_dir = os.getcwd()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "dataPath = \"data/statistics-3.csv\"\n",
    "df = pd.read_csv(dataPath)\n",
    "# columnList = df.columns\n",
    "\n",
    "# Dropping columns that are not required at the moment\n",
    "df = df.drop(columns=[ 'Unnamed: 0', 'UUID', 'HOSTNAME', 'ALIAS', 'TIMESTAMP',\n",
    "                       'THROUGHPUT (Receiver)', 'LATENCY (min.)', 'LATENCY (max.)', \n",
    "                       'CONGESTION (Receiver)', 'BYTES (Receiver)'\n",
    "                     ])\n",
    "\n",
    "# Pre-processing\n",
    "pacing = df['PACING'].values\n",
    "for i, p in enumerate(pacing):\n",
    "    v, _ = p.split(\"gbit\")\n",
    "    pacing[i] = int(v)\n",
    "\n",
    "df['PACING'] = pacing\n",
    "df['CONGESTION (Sender)'] = (df['CONGESTION (Sender)'] == 'cubic').astype(int) # Cubic = 1 & BBRV2 = 0\n",
    "\n",
    "X = df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)']].values\n",
    "y = df['PACING'].values\n",
    "y = y.astype('int')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df.head(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   STREAMS PACING  THROUGHPUT (Sender)  LATENCY (mean)  RETRANSMITS  \\\n",
       "0        1      1         1.630381e+09         30185.5         1535   \n",
       "1        1      1         1.659032e+09         30193.0         1344   \n",
       "2        1      1         9.887439e+08         31422.5            0   \n",
       "3        1      2         1.976052e+09         31420.5            0   \n",
       "4        1      3         2.962443e+09         31489.0            0   \n",
       "\n",
       "   CONGESTION (Sender)  \n",
       "0                    1  \n",
       "1                    1  \n",
       "2                    1  \n",
       "3                    1  \n",
       "4                    1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STREAMS</th>\n",
       "      <th>PACING</th>\n",
       "      <th>THROUGHPUT (Sender)</th>\n",
       "      <th>LATENCY (mean)</th>\n",
       "      <th>RETRANSMITS</th>\n",
       "      <th>CONGESTION (Sender)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.630381e+09</td>\n",
       "      <td>30185.5</td>\n",
       "      <td>1535</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.659032e+09</td>\n",
       "      <td>30193.0</td>\n",
       "      <td>1344</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.887439e+08</td>\n",
       "      <td>31422.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.976052e+09</td>\n",
       "      <td>31420.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.962443e+09</td>\n",
       "      <td>31489.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# # Standerdization\n",
    "# std_scale = preprocessing.StandardScaler().fit(df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)']])\n",
    "# df_std = std_scale.transform(df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)']])\n",
    "\n",
    "# # Normalization\n",
    "minmax_scale = preprocessing.MinMaxScaler().fit(df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)']])\n",
    "df_minmax = minmax_scale.transform(df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)']])\n",
    "\n",
    "final_df = pd.DataFrame(df_minmax, columns=['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)'])\n",
    "\n",
    "X = final_df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)']].values\n",
    "\n",
    "final_df.head(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   THROUGHPUT (Sender)  LATENCY (mean)  RETRANSMITS  STREAMS  \\\n",
       "0             0.149677        0.577766      1.00000      0.0   \n",
       "1             0.152625        0.577916      0.87557      0.0   \n",
       "2             0.083649        0.602453      0.00000      0.0   \n",
       "3             0.185249        0.602413      0.00000      0.0   \n",
       "4             0.286755        0.603780      0.00000      0.0   \n",
       "\n",
       "   CONGESTION (Sender)  \n",
       "0                  1.0  \n",
       "1                  1.0  \n",
       "2                  1.0  \n",
       "3                  1.0  \n",
       "4                  1.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>THROUGHPUT (Sender)</th>\n",
       "      <th>LATENCY (mean)</th>\n",
       "      <th>RETRANSMITS</th>\n",
       "      <th>STREAMS</th>\n",
       "      <th>CONGESTION (Sender)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.149677</td>\n",
       "      <td>0.577766</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.152625</td>\n",
       "      <td>0.577916</td>\n",
       "      <td>0.87557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.083649</td>\n",
       "      <td>0.602453</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.185249</td>\n",
       "      <td>0.602413</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.286755</td>\n",
       "      <td>0.603780</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "EPOCH = 400\n",
    "BATCH = 32\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "X_test  = torch.tensor(X_test)\n",
    "y_test  = torch.tensor(y_test) \n",
    "\n",
    "class CustomTensorDataset(Dataset):\n",
    "    \"\"\"\n",
    "    TensorDataset with support of transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        y = self.tensors[1][index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "\n",
    "# Dataset w/o any tranformations\n",
    "traindata   = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
    "trainloader = torch.utils.data.DataLoader(traindata, batch_size=BATCH)\n",
    "\n",
    "testdata    = CustomTensorDataset(tensors=(X_test, y_test), transform=None)\n",
    "testloader = torch.utils.data.DataLoader(testdata, batch_size=BATCH)\n",
    "\n",
    "print(len(traindata), len(testdata))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "128 56\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "for xs, ys in trainloader:\n",
    "    print(xs, ys)\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.1857, 0.4042, 0.0000, 1.0000, 1.0000],\n",
      "        [0.1815, 0.3843, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4904, 0.4109, 0.0000, 0.0000, 1.0000],\n",
      "        [0.1820, 0.5853, 0.0000, 1.0000, 0.0000],\n",
      "        [0.1851, 0.6036, 0.0000, 1.0000, 1.0000],\n",
      "        [0.0107, 0.9786, 0.0143, 0.0000, 1.0000],\n",
      "        [0.0820, 0.3894, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3890, 0.4057, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0019, 0.9790, 0.0137, 0.0000, 1.0000],\n",
      "        [0.2507, 0.6061, 0.0000, 1.0000, 1.0000],\n",
      "        [0.0837, 0.6025, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0288, 0.9800, 0.0202, 0.0000, 1.0000],\n",
      "        [0.5922, 0.4100, 0.0000, 0.0000, 1.0000],\n",
      "        [0.1856, 0.4042, 0.0000, 1.0000, 1.0000],\n",
      "        [0.0214, 0.9787, 0.0169, 1.0000, 1.0000],\n",
      "        [0.0104, 0.9787, 0.0143, 0.0000, 1.0000],\n",
      "        [0.1818, 0.3883, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6961, 0.0042, 0.0000, 0.0000, 1.0000],\n",
      "        [0.1805, 0.9882, 0.1036, 0.0000, 0.0000],\n",
      "        [0.0234, 0.9802, 0.0169, 0.0000, 1.0000],\n",
      "        [0.2513, 0.4079, 0.0000, 1.0000, 1.0000],\n",
      "        [0.0110, 0.9796, 0.0117, 0.0000, 1.0000],\n",
      "        [0.2203, 0.6092, 0.0000, 1.0000, 1.0000],\n",
      "        [0.1857, 0.4034, 0.0000, 0.0000, 1.0000],\n",
      "        [0.4795, 0.5941, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2508, 0.6073, 0.0000, 1.0000, 1.0000],\n",
      "        [0.2201, 0.6070, 0.0000, 1.0000, 1.0000],\n",
      "        [0.0153, 0.9789, 0.0189, 1.0000, 1.0000],\n",
      "        [0.8945, 0.4050, 0.0000, 0.0000, 1.0000],\n",
      "        [0.4906, 0.4110, 0.0000, 0.0000, 1.0000],\n",
      "        [0.1852, 0.6036, 0.0000, 1.0000, 1.0000],\n",
      "        [0.2352, 0.6068, 0.0000, 1.0000, 1.0000]], dtype=torch.float64) tensor([2, 2, 5, 2, 2, 9, 1, 4, 5, 7, 1, 7, 6, 2, 7, 6, 2, 7, 2, 4, 5, 4, 8, 2,\n",
      "        5, 5, 3, 2, 9, 5, 2, 5])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "class PacingOptimizer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PacingOptimizer, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear (5, 32)\n",
    "        self.fc2 = torch.nn.Linear (32, 32)\n",
    "        self.fc3 = torch.nn.Linear (32, 32)\n",
    "        self.fc4 = torch.nn.Linear (32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = PacingOptimizer()\n",
    "print( f\"====================\\nTotal params: {len(list(model.parameters()))}\\n====================\" )\n",
    "# print(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================\n",
      "Total params: 8\n",
      "====================\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "CE = nn.CrossEntropyLoss()\n",
    "MSE = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "bestloss = 10\n",
    "\n",
    "for epoch in range(EPOCH):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        xs, ys = data\n",
    "        xs, ys = xs.float(), ys.float()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        output = model(xs)\n",
    "        # print(output, ys)\n",
    "        \n",
    "        # loss = CE(output, ys)\n",
    "        loss = MSE(ys, output)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"[{epoch+1}/{EPOCH}] loss: {running_loss/len(trainloader.dataset):.3f}\")\n",
    "    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "correct, acc, total = 0, 0, 0\n",
    "with torch.no_grad():\n",
    "    for xs, ys in testloader:\n",
    "        xs, ys = xs.float(), ys.long()\n",
    "\n",
    "        output = model(xs)\n",
    "        loss = MSE(ys, output)\n",
    "        running_loss += loss.item()\n",
    "        # total += ys.size(0)\n",
    "        # pred = torch.max(output, 1)[1]\n",
    "        # correct += (pred == ys).sum().item()\n",
    "    # acc = (100 * correct / total)\n",
    "# print(acc)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1/400] loss: 0.692\n",
      "[2/400] loss: 0.649\n",
      "[3/400] loss: 0.577\n",
      "[4/400] loss: 0.488\n",
      "[5/400] loss: 0.385\n",
      "[6/400] loss: 0.278\n",
      "[7/400] loss: 0.193\n",
      "[8/400] loss: 0.172\n",
      "[9/400] loss: 0.193\n",
      "[10/400] loss: 0.186\n",
      "[11/400] loss: 0.168\n",
      "[12/400] loss: 0.166\n",
      "[13/400] loss: 0.169\n",
      "[14/400] loss: 0.169\n",
      "[15/400] loss: 0.167\n",
      "[16/400] loss: 0.165\n",
      "[17/400] loss: 0.165\n",
      "[18/400] loss: 0.165\n",
      "[19/400] loss: 0.165\n",
      "[20/400] loss: 0.164\n",
      "[21/400] loss: 0.164\n",
      "[22/400] loss: 0.164\n",
      "[23/400] loss: 0.164\n",
      "[24/400] loss: 0.164\n",
      "[25/400] loss: 0.164\n",
      "[26/400] loss: 0.164\n",
      "[27/400] loss: 0.164\n",
      "[28/400] loss: 0.164\n",
      "[29/400] loss: 0.164\n",
      "[30/400] loss: 0.164\n",
      "[31/400] loss: 0.164\n",
      "[32/400] loss: 0.164\n",
      "[33/400] loss: 0.164\n",
      "[34/400] loss: 0.164\n",
      "[35/400] loss: 0.164\n",
      "[36/400] loss: 0.164\n",
      "[37/400] loss: 0.164\n",
      "[38/400] loss: 0.164\n",
      "[39/400] loss: 0.164\n",
      "[40/400] loss: 0.164\n",
      "[41/400] loss: 0.164\n",
      "[42/400] loss: 0.164\n",
      "[43/400] loss: 0.164\n",
      "[44/400] loss: 0.164\n",
      "[45/400] loss: 0.164\n",
      "[46/400] loss: 0.164\n",
      "[47/400] loss: 0.164\n",
      "[48/400] loss: 0.164\n",
      "[49/400] loss: 0.164\n",
      "[50/400] loss: 0.164\n",
      "[51/400] loss: 0.164\n",
      "[52/400] loss: 0.164\n",
      "[53/400] loss: 0.164\n",
      "[54/400] loss: 0.164\n",
      "[55/400] loss: 0.164\n",
      "[56/400] loss: 0.164\n",
      "[57/400] loss: 0.164\n",
      "[58/400] loss: 0.164\n",
      "[59/400] loss: 0.164\n",
      "[60/400] loss: 0.164\n",
      "[61/400] loss: 0.164\n",
      "[62/400] loss: 0.164\n",
      "[63/400] loss: 0.164\n",
      "[64/400] loss: 0.164\n",
      "[65/400] loss: 0.164\n",
      "[66/400] loss: 0.164\n",
      "[67/400] loss: 0.164\n",
      "[68/400] loss: 0.164\n",
      "[69/400] loss: 0.164\n",
      "[70/400] loss: 0.164\n",
      "[71/400] loss: 0.164\n",
      "[72/400] loss: 0.164\n",
      "[73/400] loss: 0.164\n",
      "[74/400] loss: 0.164\n",
      "[75/400] loss: 0.164\n",
      "[76/400] loss: 0.164\n",
      "[77/400] loss: 0.164\n",
      "[78/400] loss: 0.164\n",
      "[79/400] loss: 0.164\n",
      "[80/400] loss: 0.164\n",
      "[81/400] loss: 0.164\n",
      "[82/400] loss: 0.164\n",
      "[83/400] loss: 0.164\n",
      "[84/400] loss: 0.163\n",
      "[85/400] loss: 0.163\n",
      "[86/400] loss: 0.163\n",
      "[87/400] loss: 0.163\n",
      "[88/400] loss: 0.163\n",
      "[89/400] loss: 0.163\n",
      "[90/400] loss: 0.163\n",
      "[91/400] loss: 0.163\n",
      "[92/400] loss: 0.163\n",
      "[93/400] loss: 0.163\n",
      "[94/400] loss: 0.163\n",
      "[95/400] loss: 0.163\n",
      "[96/400] loss: 0.163\n",
      "[97/400] loss: 0.163\n",
      "[98/400] loss: 0.163\n",
      "[99/400] loss: 0.163\n",
      "[100/400] loss: 0.163\n",
      "[101/400] loss: 0.163\n",
      "[102/400] loss: 0.163\n",
      "[103/400] loss: 0.163\n",
      "[104/400] loss: 0.163\n",
      "[105/400] loss: 0.163\n",
      "[106/400] loss: 0.163\n",
      "[107/400] loss: 0.163\n",
      "[108/400] loss: 0.163\n",
      "[109/400] loss: 0.163\n",
      "[110/400] loss: 0.163\n",
      "[111/400] loss: 0.163\n",
      "[112/400] loss: 0.163\n",
      "[113/400] loss: 0.163\n",
      "[114/400] loss: 0.163\n",
      "[115/400] loss: 0.163\n",
      "[116/400] loss: 0.163\n",
      "[117/400] loss: 0.163\n",
      "[118/400] loss: 0.163\n",
      "[119/400] loss: 0.163\n",
      "[120/400] loss: 0.163\n",
      "[121/400] loss: 0.163\n",
      "[122/400] loss: 0.163\n",
      "[123/400] loss: 0.163\n",
      "[124/400] loss: 0.163\n",
      "[125/400] loss: 0.163\n",
      "[126/400] loss: 0.163\n",
      "[127/400] loss: 0.163\n",
      "[128/400] loss: 0.163\n",
      "[129/400] loss: 0.163\n",
      "[130/400] loss: 0.163\n",
      "[131/400] loss: 0.163\n",
      "[132/400] loss: 0.163\n",
      "[133/400] loss: 0.163\n",
      "[134/400] loss: 0.163\n",
      "[135/400] loss: 0.163\n",
      "[136/400] loss: 0.163\n",
      "[137/400] loss: 0.163\n",
      "[138/400] loss: 0.163\n",
      "[139/400] loss: 0.163\n",
      "[140/400] loss: 0.163\n",
      "[141/400] loss: 0.163\n",
      "[142/400] loss: 0.163\n",
      "[143/400] loss: 0.163\n",
      "[144/400] loss: 0.163\n",
      "[145/400] loss: 0.163\n",
      "[146/400] loss: 0.163\n",
      "[147/400] loss: 0.163\n",
      "[148/400] loss: 0.163\n",
      "[149/400] loss: 0.163\n",
      "[150/400] loss: 0.163\n",
      "[151/400] loss: 0.163\n",
      "[152/400] loss: 0.163\n",
      "[153/400] loss: 0.163\n",
      "[154/400] loss: 0.163\n",
      "[155/400] loss: 0.163\n",
      "[156/400] loss: 0.163\n",
      "[157/400] loss: 0.163\n",
      "[158/400] loss: 0.163\n",
      "[159/400] loss: 0.163\n",
      "[160/400] loss: 0.163\n",
      "[161/400] loss: 0.163\n",
      "[162/400] loss: 0.163\n",
      "[163/400] loss: 0.163\n",
      "[164/400] loss: 0.163\n",
      "[165/400] loss: 0.163\n",
      "[166/400] loss: 0.163\n",
      "[167/400] loss: 0.163\n",
      "[168/400] loss: 0.163\n",
      "[169/400] loss: 0.163\n",
      "[170/400] loss: 0.163\n",
      "[171/400] loss: 0.163\n",
      "[172/400] loss: 0.163\n",
      "[173/400] loss: 0.163\n",
      "[174/400] loss: 0.163\n",
      "[175/400] loss: 0.163\n",
      "[176/400] loss: 0.163\n",
      "[177/400] loss: 0.163\n",
      "[178/400] loss: 0.163\n",
      "[179/400] loss: 0.163\n",
      "[180/400] loss: 0.163\n",
      "[181/400] loss: 0.163\n",
      "[182/400] loss: 0.163\n",
      "[183/400] loss: 0.163\n",
      "[184/400] loss: 0.163\n",
      "[185/400] loss: 0.163\n",
      "[186/400] loss: 0.163\n",
      "[187/400] loss: 0.163\n",
      "[188/400] loss: 0.163\n",
      "[189/400] loss: 0.163\n",
      "[190/400] loss: 0.163\n",
      "[191/400] loss: 0.163\n",
      "[192/400] loss: 0.163\n",
      "[193/400] loss: 0.163\n",
      "[194/400] loss: 0.163\n",
      "[195/400] loss: 0.163\n",
      "[196/400] loss: 0.163\n",
      "[197/400] loss: 0.163\n",
      "[198/400] loss: 0.163\n",
      "[199/400] loss: 0.163\n",
      "[200/400] loss: 0.163\n",
      "[201/400] loss: 0.163\n",
      "[202/400] loss: 0.163\n",
      "[203/400] loss: 0.163\n",
      "[204/400] loss: 0.163\n",
      "[205/400] loss: 0.163\n",
      "[206/400] loss: 0.163\n",
      "[207/400] loss: 0.163\n",
      "[208/400] loss: 0.163\n",
      "[209/400] loss: 0.163\n",
      "[210/400] loss: 0.163\n",
      "[211/400] loss: 0.163\n",
      "[212/400] loss: 0.163\n",
      "[213/400] loss: 0.163\n",
      "[214/400] loss: 0.163\n",
      "[215/400] loss: 0.163\n",
      "[216/400] loss: 0.163\n",
      "[217/400] loss: 0.163\n",
      "[218/400] loss: 0.163\n",
      "[219/400] loss: 0.163\n",
      "[220/400] loss: 0.163\n",
      "[221/400] loss: 0.163\n",
      "[222/400] loss: 0.163\n",
      "[223/400] loss: 0.163\n",
      "[224/400] loss: 0.163\n",
      "[225/400] loss: 0.163\n",
      "[226/400] loss: 0.163\n",
      "[227/400] loss: 0.163\n",
      "[228/400] loss: 0.163\n",
      "[229/400] loss: 0.163\n",
      "[230/400] loss: 0.163\n",
      "[231/400] loss: 0.163\n",
      "[232/400] loss: 0.163\n",
      "[233/400] loss: 0.163\n",
      "[234/400] loss: 0.163\n",
      "[235/400] loss: 0.163\n",
      "[236/400] loss: 0.163\n",
      "[237/400] loss: 0.163\n",
      "[238/400] loss: 0.163\n",
      "[239/400] loss: 0.163\n",
      "[240/400] loss: 0.163\n",
      "[241/400] loss: 0.163\n",
      "[242/400] loss: 0.163\n",
      "[243/400] loss: 0.163\n",
      "[244/400] loss: 0.163\n",
      "[245/400] loss: 0.163\n",
      "[246/400] loss: 0.163\n",
      "[247/400] loss: 0.163\n",
      "[248/400] loss: 0.163\n",
      "[249/400] loss: 0.163\n",
      "[250/400] loss: 0.163\n",
      "[251/400] loss: 0.163\n",
      "[252/400] loss: 0.163\n",
      "[253/400] loss: 0.163\n",
      "[254/400] loss: 0.163\n",
      "[255/400] loss: 0.163\n",
      "[256/400] loss: 0.163\n",
      "[257/400] loss: 0.163\n",
      "[258/400] loss: 0.163\n",
      "[259/400] loss: 0.163\n",
      "[260/400] loss: 0.163\n",
      "[261/400] loss: 0.163\n",
      "[262/400] loss: 0.163\n",
      "[263/400] loss: 0.163\n",
      "[264/400] loss: 0.163\n",
      "[265/400] loss: 0.163\n",
      "[266/400] loss: 0.163\n",
      "[267/400] loss: 0.163\n",
      "[268/400] loss: 0.163\n",
      "[269/400] loss: 0.163\n",
      "[270/400] loss: 0.163\n",
      "[271/400] loss: 0.163\n",
      "[272/400] loss: 0.163\n",
      "[273/400] loss: 0.163\n",
      "[274/400] loss: 0.163\n",
      "[275/400] loss: 0.163\n",
      "[276/400] loss: 0.163\n",
      "[277/400] loss: 0.163\n",
      "[278/400] loss: 0.163\n",
      "[279/400] loss: 0.163\n",
      "[280/400] loss: 0.163\n",
      "[281/400] loss: 0.163\n",
      "[282/400] loss: 0.163\n",
      "[283/400] loss: 0.163\n",
      "[284/400] loss: 0.163\n",
      "[285/400] loss: 0.163\n",
      "[286/400] loss: 0.163\n",
      "[287/400] loss: 0.163\n",
      "[288/400] loss: 0.163\n",
      "[289/400] loss: 0.163\n",
      "[290/400] loss: 0.163\n",
      "[291/400] loss: 0.163\n",
      "[292/400] loss: 0.163\n",
      "[293/400] loss: 0.163\n",
      "[294/400] loss: 0.163\n",
      "[295/400] loss: 0.163\n",
      "[296/400] loss: 0.163\n",
      "[297/400] loss: 0.163\n",
      "[298/400] loss: 0.163\n",
      "[299/400] loss: 0.163\n",
      "[300/400] loss: 0.163\n",
      "[301/400] loss: 0.163\n",
      "[302/400] loss: 0.163\n",
      "[303/400] loss: 0.163\n",
      "[304/400] loss: 0.163\n",
      "[305/400] loss: 0.163\n",
      "[306/400] loss: 0.163\n",
      "[307/400] loss: 0.163\n",
      "[308/400] loss: 0.163\n",
      "[309/400] loss: 0.163\n",
      "[310/400] loss: 0.163\n",
      "[311/400] loss: 0.163\n",
      "[312/400] loss: 0.163\n",
      "[313/400] loss: 0.163\n",
      "[314/400] loss: 0.163\n",
      "[315/400] loss: 0.163\n",
      "[316/400] loss: 0.163\n",
      "[317/400] loss: 0.163\n",
      "[318/400] loss: 0.163\n",
      "[319/400] loss: 0.163\n",
      "[320/400] loss: 0.163\n",
      "[321/400] loss: 0.163\n",
      "[322/400] loss: 0.163\n",
      "[323/400] loss: 0.163\n",
      "[324/400] loss: 0.163\n",
      "[325/400] loss: 0.163\n",
      "[326/400] loss: 0.163\n",
      "[327/400] loss: 0.163\n",
      "[328/400] loss: 0.163\n",
      "[329/400] loss: 0.163\n",
      "[330/400] loss: 0.163\n",
      "[331/400] loss: 0.163\n",
      "[332/400] loss: 0.163\n",
      "[333/400] loss: 0.163\n",
      "[334/400] loss: 0.163\n",
      "[335/400] loss: 0.163\n",
      "[336/400] loss: 0.163\n",
      "[337/400] loss: 0.163\n",
      "[338/400] loss: 0.163\n",
      "[339/400] loss: 0.163\n",
      "[340/400] loss: 0.163\n",
      "[341/400] loss: 0.163\n",
      "[342/400] loss: 0.163\n",
      "[343/400] loss: 0.163\n",
      "[344/400] loss: 0.163\n",
      "[345/400] loss: 0.163\n",
      "[346/400] loss: 0.163\n",
      "[347/400] loss: 0.163\n",
      "[348/400] loss: 0.163\n",
      "[349/400] loss: 0.163\n",
      "[350/400] loss: 0.163\n",
      "[351/400] loss: 0.163\n",
      "[352/400] loss: 0.163\n",
      "[353/400] loss: 0.163\n",
      "[354/400] loss: 0.163\n",
      "[355/400] loss: 0.163\n",
      "[356/400] loss: 0.163\n",
      "[357/400] loss: 0.163\n",
      "[358/400] loss: 0.163\n",
      "[359/400] loss: 0.163\n",
      "[360/400] loss: 0.163\n",
      "[361/400] loss: 0.163\n",
      "[362/400] loss: 0.163\n",
      "[363/400] loss: 0.163\n",
      "[364/400] loss: 0.163\n",
      "[365/400] loss: 0.163\n",
      "[366/400] loss: 0.163\n",
      "[367/400] loss: 0.163\n",
      "[368/400] loss: 0.163\n",
      "[369/400] loss: 0.163\n",
      "[370/400] loss: 0.163\n",
      "[371/400] loss: 0.163\n",
      "[372/400] loss: 0.163\n",
      "[373/400] loss: 0.163\n",
      "[374/400] loss: 0.163\n",
      "[375/400] loss: 0.163\n",
      "[376/400] loss: 0.163\n",
      "[377/400] loss: 0.163\n",
      "[378/400] loss: 0.163\n",
      "[379/400] loss: 0.163\n",
      "[380/400] loss: 0.163\n",
      "[381/400] loss: 0.163\n",
      "[382/400] loss: 0.163\n",
      "[383/400] loss: 0.163\n",
      "[384/400] loss: 0.163\n",
      "[385/400] loss: 0.163\n",
      "[386/400] loss: 0.163\n",
      "[387/400] loss: 0.163\n",
      "[388/400] loss: 0.163\n",
      "[389/400] loss: 0.163\n",
      "[390/400] loss: 0.163\n",
      "[391/400] loss: 0.163\n",
      "[392/400] loss: 0.163\n",
      "[393/400] loss: 0.163\n",
      "[394/400] loss: 0.163\n",
      "[395/400] loss: 0.163\n",
      "[396/400] loss: 0.163\n",
      "[397/400] loss: 0.163\n",
      "[398/400] loss: 0.163\n",
      "[399/400] loss: 0.163\n",
      "[400/400] loss: 0.163\n",
      "Finished Training\n",
      "0.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import os\n",
    "# import sys\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import warnings\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# # --- Sklearn ---\n",
    "# from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "# from sklearn import decomposition, discriminant_analysis\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # --- Models ---\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn import svm\n",
    "# from sklearn import neural_network\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# # --- Utility ---\n",
    "# import os\n",
    "# import pickle, torch\n",
    "# import numpy as np, pandas as pd\n",
    "# import seaborn as sn\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# dataPath = \"data/statistics (pacing).csv\"\n",
    "# df_full = pd.read_csv(dataPath)\n",
    "# print(df_full.describe())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "columnList = df_full.columns\n",
    "print(f\"Total columns list: {columnList}\")\n",
    "\n",
    "# Dropping columns that are not required at the moment\n",
    "df = df_full.drop(columns=[ 'Unnamed: 0', 'UUID', 'HOSTNAME', 'ALIAS', 'TIMESTAMP', 'STREAMS',\n",
    "                            'THROUGHPUT (Receiver)', 'LATENCY (min.)', 'LATENCY (max.)', \n",
    "                            'CONGESTION (Receiver)', 'BYTES (Receiver)'\n",
    "                          ])\n",
    "\n",
    "print(f\"New columns list: {df.columns}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# df.head(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pacing = df['PACING'].values\n",
    "for i, p in enumerate(pacing):\n",
    "    v, _ = p.split(\"gbit\")\n",
    "    pacing[i] = int(v)\n",
    "\n",
    "df['PACING'] = pacing\n",
    "# df['CONGESTION (Sender)'] = df['CONGESTION (Sender)']==\"cubic\"=1\n",
    "# df['CONGESTION (Sender)'] = df['CONGESTION (Sender)']==\"bbr2\"=0\n",
    "df['CONGESTION (Sender)'] = (df['CONGESTION (Sender)'] == 'cubic').astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "df.head(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sns.set(style='whitegrid', context='notebook')\n",
    "# cols = ['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'CONGESTION (Sender)', 'PACING']\n",
    "\n",
    "# sns.pairplot(df[cols], height=3)\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig('./figures/scatter.png', dpi=300)\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS']].values\n",
    "y = df['PACING'].values\n",
    "y = y.astype('int')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_and_tune(X, y, model, parameters, scoring='f1_macro', kfold=5, verbose=0):\n",
    "    \"\"\"\n",
    "    X:          array-like of shape (n_samples, n_features)\n",
    "    y:          array-like of shape (n_samples,)\n",
    "    model:      (object) a sklearn model class\n",
    "    parameters: (dict) contains the parameters you want to tune in the model\n",
    "    metric:     (str) the metric used to evaluate the quality of the model\n",
    "    return:     a trained model with the best parameters\n",
    "    \"\"\"\n",
    "    cvSearchObj = GridSearchCV(model,\n",
    "                               parameters,\n",
    "                               scoring=scoring,\n",
    "                               n_jobs=-1,\n",
    "                               cv=kfold,\n",
    "                               verbose=verbose)\n",
    "    cvSearchObj.fit(X,y)\n",
    "    return cvSearchObj.best_estimator_\n",
    "\n",
    "def save_model(filename, model):\n",
    "    \"\"\"\n",
    "    filename: Filename to save the model\n",
    "    model:    Model weights to be saved\n",
    "    \"\"\"\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    print(\"Model Saved\")\n",
    "\n",
    "def load_model(filename):\n",
    "    \"\"\"\n",
    "    filename: Filename to load the model\n",
    "    return:   Model weights that are reloaded\n",
    "    \"\"\"\n",
    "    model_reloaded = pickle.load(open(filename, 'rb'))\n",
    "    return model_reloaded\n",
    "\n",
    "\n",
    "def MLP(train, save, test):\n",
    "    filename = \"./mlpBest.pkl\"\n",
    "    mlp = neural_network.MLPClassifier(random_state=999)\n",
    "    if train:\n",
    "        '''\n",
    "        Train\n",
    "        '''\n",
    "        params = {\"alpha\" : [0.0001],\n",
    "                \"learning_rate_init\" : [0.001],\n",
    "                \"batch_size\" : [32, 64, 128],\n",
    "                \"activation\" : [\"relu\"],\n",
    "                \"early_stopping\" : [True],\n",
    "                \"hidden_layer_sizes\" : [10, 50, 100],\n",
    "                }\n",
    "\n",
    "        mlpBest = train_and_tune(X, y,\n",
    "                                 mlp,\n",
    "                                 params,\n",
    "                                 scoring='f1_macro',\n",
    "                                 kfold=5)\n",
    "\n",
    "        if save:\n",
    "            save_model(filename, mlpBest)\n",
    "\n",
    "    if test:\n",
    "        '''\n",
    "        Test\n",
    "        '''\n",
    "        mlpBest_reloaded = load_model(filename)\n",
    "        pred = mlpBest_reloaded.predict(X)\n",
    "        acc  = mlpBest_reloaded.score(X, y)\n",
    "        \n",
    "        # cf_matrix = confusion_matrix(y, pred)\n",
    "        # df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in classes],\n",
    "        #                      columns = [i for i in classes])\n",
    "        # plt.figure(figsize = (12,10))\n",
    "        # sn.heatmap(df_cm, annot=True)\n",
    "        \n",
    "        print(\"Accuracy: \", acc)\n",
    "\n",
    "MLP(train=True, save=True, test=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "# le.transform(['M', 'B'])\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print('Test Accuracy: %.3f' % model.score(X_test, y_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bf92e7c56550015a5ef2c115972b5497eb0f5cccb81c21dbff7b4769ac887066"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}