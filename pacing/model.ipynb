{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "# --- System ---\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# --- Utility ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# --- Plot --\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Pytorch ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "root_dir = os.getcwd()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "dataPath = \"data/statistics-3.csv\"\n",
    "df = pd.read_csv(dataPath)\n",
    "# columnList = df.columns\n",
    "\n",
    "# Dropping columns that are not required at the moment\n",
    "df = df.drop(columns=[ 'Unnamed: 0', 'UUID', 'HOSTNAME', 'ALIAS', 'TIMESTAMP',\n",
    "                       'THROUGHPUT (Receiver)', 'LATENCY (min.)', 'LATENCY (max.)', \n",
    "                       'CONGESTION (Receiver)', 'BYTES (Receiver)'\n",
    "                     ])\n",
    "\n",
    "# Pre-processing\n",
    "pacing = df['PACING'].values\n",
    "for i, p in enumerate(pacing):\n",
    "    v, _ = p.split(\"gbit\")\n",
    "    pacing[i] = int(v)\n",
    "\n",
    "df['PACING'] = pacing\n",
    "df['CONGESTION (Sender)'] = (df['CONGESTION (Sender)'] == 'cubic').astype(int) # Cubic = 1 & BBRV2 = 0\n",
    "\n",
    "X = df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)']].values\n",
    "y = df['PACING'].values\n",
    "y = y.astype('int')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "df.head(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   STREAMS PACING  THROUGHPUT (Sender)  LATENCY (mean)  RETRANSMITS  \\\n",
       "0        1      1         1.630381e+09         30185.5         1535   \n",
       "1        1      1         1.659032e+09         30193.0         1344   \n",
       "2        1      1         9.887439e+08         31422.5            0   \n",
       "3        1      2         1.976052e+09         31420.5            0   \n",
       "4        1      3         2.962443e+09         31489.0            0   \n",
       "\n",
       "   CONGESTION (Sender)  \n",
       "0                    1  \n",
       "1                    1  \n",
       "2                    1  \n",
       "3                    1  \n",
       "4                    1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STREAMS</th>\n",
       "      <th>PACING</th>\n",
       "      <th>THROUGHPUT (Sender)</th>\n",
       "      <th>LATENCY (mean)</th>\n",
       "      <th>RETRANSMITS</th>\n",
       "      <th>CONGESTION (Sender)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.630381e+09</td>\n",
       "      <td>30185.5</td>\n",
       "      <td>1535</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.659032e+09</td>\n",
       "      <td>30193.0</td>\n",
       "      <td>1344</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.887439e+08</td>\n",
       "      <td>31422.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.976052e+09</td>\n",
       "      <td>31420.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.962443e+09</td>\n",
       "      <td>31489.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# # Standerdization\n",
    "# std_scale = preprocessing.StandardScaler().fit(df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)']])\n",
    "# df_std = std_scale.transform(df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)']])\n",
    "\n",
    "# # Normalization\n",
    "minmax_scale = preprocessing.MinMaxScaler().fit(df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)']])\n",
    "df_minmax = minmax_scale.transform(df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)']])\n",
    "\n",
    "final_df = pd.DataFrame(df_minmax, columns=['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)'])\n",
    "final_df.head(5)\n",
    "\n",
    "X = final_df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)']].values\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "EPOCH = 400\n",
    "BATCH = 32\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "X_test  = torch.tensor(X_test)\n",
    "y_test  = torch.tensor(y_test) \n",
    "\n",
    "class CustomTensorDataset(Dataset):\n",
    "    \"\"\"\n",
    "    TensorDataset with support of transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        y = self.tensors[1][index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "\n",
    "# Dataset w/o any tranformations\n",
    "traindata   = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
    "trainloader = torch.utils.data.DataLoader(traindata, batch_size=BATCH)\n",
    "\n",
    "testdata    = CustomTensorDataset(tensors=(X_test, y_test), transform=None)\n",
    "testloader = torch.utils.data.DataLoader(testdata, batch_size=BATCH)\n",
    "\n",
    "print(len(traindata), len(testdata))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "128 56\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "class PacingOptimizer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PacingOptimizer, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear (5, 32)\n",
    "        self.fc2 = torch.nn.Linear (32, 32)\n",
    "        self.fc3 = torch.nn.Linear (32, 32)\n",
    "        self.fc4 = torch.nn.Linear (32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = PacingOptimizer()\n",
    "print( f\"====================\\nTotal params: {len(list(model.parameters()))}\\n====================\" )\n",
    "# print(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================\n",
      "Total params: 8\n",
      "====================\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "CE = nn.CrossEntropyLoss()\n",
    "MSE = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "bestloss = 10\n",
    "\n",
    "for epoch in range(EPOCH):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        xs, ys = data\n",
    "        xs, ys = xs.float(), ys.float()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        output = model(xs)\n",
    "        # print(output, ys)\n",
    "        \n",
    "        # loss = CE(output, ys)\n",
    "        loss = MSE(ys, output)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"[{epoch+1}/{EPOCH}] loss: {running_loss/len(trainloader.dataset):.3f}\")\n",
    "    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "correct, acc, total = 0, 0, 0\n",
    "with torch.no_grad():\n",
    "    for xs, ys in testloader:\n",
    "        xs, ys = xs.float(), ys.long()\n",
    "\n",
    "        output = model(xs)\n",
    "        loss = MSE(ys, output)\n",
    "        running_loss += loss.item()\n",
    "        # total += ys.size(0)\n",
    "        # pred = torch.max(output, 1)[1]\n",
    "        # correct += (pred == ys).sum().item()\n",
    "    # acc = (100 * correct / total)\n",
    "# print(acc)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1/400] loss: 0.692\n",
      "[2/400] loss: 0.649\n",
      "[3/400] loss: 0.577\n",
      "[4/400] loss: 0.488\n",
      "[5/400] loss: 0.385\n",
      "[6/400] loss: 0.278\n",
      "[7/400] loss: 0.193\n",
      "[8/400] loss: 0.172\n",
      "[9/400] loss: 0.193\n",
      "[10/400] loss: 0.186\n",
      "[11/400] loss: 0.168\n",
      "[12/400] loss: 0.166\n",
      "[13/400] loss: 0.169\n",
      "[14/400] loss: 0.169\n",
      "[15/400] loss: 0.167\n",
      "[16/400] loss: 0.165\n",
      "[17/400] loss: 0.165\n",
      "[18/400] loss: 0.165\n",
      "[19/400] loss: 0.165\n",
      "[20/400] loss: 0.164\n",
      "[21/400] loss: 0.164\n",
      "[22/400] loss: 0.164\n",
      "[23/400] loss: 0.164\n",
      "[24/400] loss: 0.164\n",
      "[25/400] loss: 0.164\n",
      "[26/400] loss: 0.164\n",
      "[27/400] loss: 0.164\n",
      "[28/400] loss: 0.164\n",
      "[29/400] loss: 0.164\n",
      "[30/400] loss: 0.164\n",
      "[31/400] loss: 0.164\n",
      "[32/400] loss: 0.164\n",
      "[33/400] loss: 0.164\n",
      "[34/400] loss: 0.164\n",
      "[35/400] loss: 0.164\n",
      "[36/400] loss: 0.164\n",
      "[37/400] loss: 0.164\n",
      "[38/400] loss: 0.164\n",
      "[39/400] loss: 0.164\n",
      "[40/400] loss: 0.164\n",
      "[41/400] loss: 0.164\n",
      "[42/400] loss: 0.164\n",
      "[43/400] loss: 0.164\n",
      "[44/400] loss: 0.164\n",
      "[45/400] loss: 0.164\n",
      "[46/400] loss: 0.164\n",
      "[47/400] loss: 0.164\n",
      "[48/400] loss: 0.164\n",
      "[49/400] loss: 0.164\n",
      "[50/400] loss: 0.164\n",
      "[51/400] loss: 0.164\n",
      "[52/400] loss: 0.164\n",
      "[53/400] loss: 0.164\n",
      "[54/400] loss: 0.164\n",
      "[55/400] loss: 0.164\n",
      "[56/400] loss: 0.164\n",
      "[57/400] loss: 0.164\n",
      "[58/400] loss: 0.164\n",
      "[59/400] loss: 0.164\n",
      "[60/400] loss: 0.164\n",
      "[61/400] loss: 0.164\n",
      "[62/400] loss: 0.164\n",
      "[63/400] loss: 0.164\n",
      "[64/400] loss: 0.164\n",
      "[65/400] loss: 0.164\n",
      "[66/400] loss: 0.164\n",
      "[67/400] loss: 0.164\n",
      "[68/400] loss: 0.164\n",
      "[69/400] loss: 0.164\n",
      "[70/400] loss: 0.164\n",
      "[71/400] loss: 0.164\n",
      "[72/400] loss: 0.164\n",
      "[73/400] loss: 0.164\n",
      "[74/400] loss: 0.164\n",
      "[75/400] loss: 0.164\n",
      "[76/400] loss: 0.164\n",
      "[77/400] loss: 0.164\n",
      "[78/400] loss: 0.164\n",
      "[79/400] loss: 0.164\n",
      "[80/400] loss: 0.164\n",
      "[81/400] loss: 0.164\n",
      "[82/400] loss: 0.164\n",
      "[83/400] loss: 0.164\n",
      "[84/400] loss: 0.163\n",
      "[85/400] loss: 0.163\n",
      "[86/400] loss: 0.163\n",
      "[87/400] loss: 0.163\n",
      "[88/400] loss: 0.163\n",
      "[89/400] loss: 0.163\n",
      "[90/400] loss: 0.163\n",
      "[91/400] loss: 0.163\n",
      "[92/400] loss: 0.163\n",
      "[93/400] loss: 0.163\n",
      "[94/400] loss: 0.163\n",
      "[95/400] loss: 0.163\n",
      "[96/400] loss: 0.163\n",
      "[97/400] loss: 0.163\n",
      "[98/400] loss: 0.163\n",
      "[99/400] loss: 0.163\n",
      "[100/400] loss: 0.163\n",
      "[101/400] loss: 0.163\n",
      "[102/400] loss: 0.163\n",
      "[103/400] loss: 0.163\n",
      "[104/400] loss: 0.163\n",
      "[105/400] loss: 0.163\n",
      "[106/400] loss: 0.163\n",
      "[107/400] loss: 0.163\n",
      "[108/400] loss: 0.163\n",
      "[109/400] loss: 0.163\n",
      "[110/400] loss: 0.163\n",
      "[111/400] loss: 0.163\n",
      "[112/400] loss: 0.163\n",
      "[113/400] loss: 0.163\n",
      "[114/400] loss: 0.163\n",
      "[115/400] loss: 0.163\n",
      "[116/400] loss: 0.163\n",
      "[117/400] loss: 0.163\n",
      "[118/400] loss: 0.163\n",
      "[119/400] loss: 0.163\n",
      "[120/400] loss: 0.163\n",
      "[121/400] loss: 0.163\n",
      "[122/400] loss: 0.163\n",
      "[123/400] loss: 0.163\n",
      "[124/400] loss: 0.163\n",
      "[125/400] loss: 0.163\n",
      "[126/400] loss: 0.163\n",
      "[127/400] loss: 0.163\n",
      "[128/400] loss: 0.163\n",
      "[129/400] loss: 0.163\n",
      "[130/400] loss: 0.163\n",
      "[131/400] loss: 0.163\n",
      "[132/400] loss: 0.163\n",
      "[133/400] loss: 0.163\n",
      "[134/400] loss: 0.163\n",
      "[135/400] loss: 0.163\n",
      "[136/400] loss: 0.163\n",
      "[137/400] loss: 0.163\n",
      "[138/400] loss: 0.163\n",
      "[139/400] loss: 0.163\n",
      "[140/400] loss: 0.163\n",
      "[141/400] loss: 0.163\n",
      "[142/400] loss: 0.163\n",
      "[143/400] loss: 0.163\n",
      "[144/400] loss: 0.163\n",
      "[145/400] loss: 0.163\n",
      "[146/400] loss: 0.163\n",
      "[147/400] loss: 0.163\n",
      "[148/400] loss: 0.163\n",
      "[149/400] loss: 0.163\n",
      "[150/400] loss: 0.163\n",
      "[151/400] loss: 0.163\n",
      "[152/400] loss: 0.163\n",
      "[153/400] loss: 0.163\n",
      "[154/400] loss: 0.163\n",
      "[155/400] loss: 0.163\n",
      "[156/400] loss: 0.163\n",
      "[157/400] loss: 0.163\n",
      "[158/400] loss: 0.163\n",
      "[159/400] loss: 0.163\n",
      "[160/400] loss: 0.163\n",
      "[161/400] loss: 0.163\n",
      "[162/400] loss: 0.163\n",
      "[163/400] loss: 0.163\n",
      "[164/400] loss: 0.163\n",
      "[165/400] loss: 0.163\n",
      "[166/400] loss: 0.163\n",
      "[167/400] loss: 0.163\n",
      "[168/400] loss: 0.163\n",
      "[169/400] loss: 0.163\n",
      "[170/400] loss: 0.163\n",
      "[171/400] loss: 0.163\n",
      "[172/400] loss: 0.163\n",
      "[173/400] loss: 0.163\n",
      "[174/400] loss: 0.163\n",
      "[175/400] loss: 0.163\n",
      "[176/400] loss: 0.163\n",
      "[177/400] loss: 0.163\n",
      "[178/400] loss: 0.163\n",
      "[179/400] loss: 0.163\n",
      "[180/400] loss: 0.163\n",
      "[181/400] loss: 0.163\n",
      "[182/400] loss: 0.163\n",
      "[183/400] loss: 0.163\n",
      "[184/400] loss: 0.163\n",
      "[185/400] loss: 0.163\n",
      "[186/400] loss: 0.163\n",
      "[187/400] loss: 0.163\n",
      "[188/400] loss: 0.163\n",
      "[189/400] loss: 0.163\n",
      "[190/400] loss: 0.163\n",
      "[191/400] loss: 0.163\n",
      "[192/400] loss: 0.163\n",
      "[193/400] loss: 0.163\n",
      "[194/400] loss: 0.163\n",
      "[195/400] loss: 0.163\n",
      "[196/400] loss: 0.163\n",
      "[197/400] loss: 0.163\n",
      "[198/400] loss: 0.163\n",
      "[199/400] loss: 0.163\n",
      "[200/400] loss: 0.163\n",
      "[201/400] loss: 0.163\n",
      "[202/400] loss: 0.163\n",
      "[203/400] loss: 0.163\n",
      "[204/400] loss: 0.163\n",
      "[205/400] loss: 0.163\n",
      "[206/400] loss: 0.163\n",
      "[207/400] loss: 0.163\n",
      "[208/400] loss: 0.163\n",
      "[209/400] loss: 0.163\n",
      "[210/400] loss: 0.163\n",
      "[211/400] loss: 0.163\n",
      "[212/400] loss: 0.163\n",
      "[213/400] loss: 0.163\n",
      "[214/400] loss: 0.163\n",
      "[215/400] loss: 0.163\n",
      "[216/400] loss: 0.163\n",
      "[217/400] loss: 0.163\n",
      "[218/400] loss: 0.163\n",
      "[219/400] loss: 0.163\n",
      "[220/400] loss: 0.163\n",
      "[221/400] loss: 0.163\n",
      "[222/400] loss: 0.163\n",
      "[223/400] loss: 0.163\n",
      "[224/400] loss: 0.163\n",
      "[225/400] loss: 0.163\n",
      "[226/400] loss: 0.163\n",
      "[227/400] loss: 0.163\n",
      "[228/400] loss: 0.163\n",
      "[229/400] loss: 0.163\n",
      "[230/400] loss: 0.163\n",
      "[231/400] loss: 0.163\n",
      "[232/400] loss: 0.163\n",
      "[233/400] loss: 0.163\n",
      "[234/400] loss: 0.163\n",
      "[235/400] loss: 0.163\n",
      "[236/400] loss: 0.163\n",
      "[237/400] loss: 0.163\n",
      "[238/400] loss: 0.163\n",
      "[239/400] loss: 0.163\n",
      "[240/400] loss: 0.163\n",
      "[241/400] loss: 0.163\n",
      "[242/400] loss: 0.163\n",
      "[243/400] loss: 0.163\n",
      "[244/400] loss: 0.163\n",
      "[245/400] loss: 0.163\n",
      "[246/400] loss: 0.163\n",
      "[247/400] loss: 0.163\n",
      "[248/400] loss: 0.163\n",
      "[249/400] loss: 0.163\n",
      "[250/400] loss: 0.163\n",
      "[251/400] loss: 0.163\n",
      "[252/400] loss: 0.163\n",
      "[253/400] loss: 0.163\n",
      "[254/400] loss: 0.163\n",
      "[255/400] loss: 0.163\n",
      "[256/400] loss: 0.163\n",
      "[257/400] loss: 0.163\n",
      "[258/400] loss: 0.163\n",
      "[259/400] loss: 0.163\n",
      "[260/400] loss: 0.163\n",
      "[261/400] loss: 0.163\n",
      "[262/400] loss: 0.163\n",
      "[263/400] loss: 0.163\n",
      "[264/400] loss: 0.163\n",
      "[265/400] loss: 0.163\n",
      "[266/400] loss: 0.163\n",
      "[267/400] loss: 0.163\n",
      "[268/400] loss: 0.163\n",
      "[269/400] loss: 0.163\n",
      "[270/400] loss: 0.163\n",
      "[271/400] loss: 0.163\n",
      "[272/400] loss: 0.163\n",
      "[273/400] loss: 0.163\n",
      "[274/400] loss: 0.163\n",
      "[275/400] loss: 0.163\n",
      "[276/400] loss: 0.163\n",
      "[277/400] loss: 0.163\n",
      "[278/400] loss: 0.163\n",
      "[279/400] loss: 0.163\n",
      "[280/400] loss: 0.163\n",
      "[281/400] loss: 0.163\n",
      "[282/400] loss: 0.163\n",
      "[283/400] loss: 0.163\n",
      "[284/400] loss: 0.163\n",
      "[285/400] loss: 0.163\n",
      "[286/400] loss: 0.163\n",
      "[287/400] loss: 0.163\n",
      "[288/400] loss: 0.163\n",
      "[289/400] loss: 0.163\n",
      "[290/400] loss: 0.163\n",
      "[291/400] loss: 0.163\n",
      "[292/400] loss: 0.163\n",
      "[293/400] loss: 0.163\n",
      "[294/400] loss: 0.163\n",
      "[295/400] loss: 0.163\n",
      "[296/400] loss: 0.163\n",
      "[297/400] loss: 0.163\n",
      "[298/400] loss: 0.163\n",
      "[299/400] loss: 0.163\n",
      "[300/400] loss: 0.163\n",
      "[301/400] loss: 0.163\n",
      "[302/400] loss: 0.163\n",
      "[303/400] loss: 0.163\n",
      "[304/400] loss: 0.163\n",
      "[305/400] loss: 0.163\n",
      "[306/400] loss: 0.163\n",
      "[307/400] loss: 0.163\n",
      "[308/400] loss: 0.163\n",
      "[309/400] loss: 0.163\n",
      "[310/400] loss: 0.163\n",
      "[311/400] loss: 0.163\n",
      "[312/400] loss: 0.163\n",
      "[313/400] loss: 0.163\n",
      "[314/400] loss: 0.163\n",
      "[315/400] loss: 0.163\n",
      "[316/400] loss: 0.163\n",
      "[317/400] loss: 0.163\n",
      "[318/400] loss: 0.163\n",
      "[319/400] loss: 0.163\n",
      "[320/400] loss: 0.163\n",
      "[321/400] loss: 0.163\n",
      "[322/400] loss: 0.163\n",
      "[323/400] loss: 0.163\n",
      "[324/400] loss: 0.163\n",
      "[325/400] loss: 0.163\n",
      "[326/400] loss: 0.163\n",
      "[327/400] loss: 0.163\n",
      "[328/400] loss: 0.163\n",
      "[329/400] loss: 0.163\n",
      "[330/400] loss: 0.163\n",
      "[331/400] loss: 0.163\n",
      "[332/400] loss: 0.163\n",
      "[333/400] loss: 0.163\n",
      "[334/400] loss: 0.163\n",
      "[335/400] loss: 0.163\n",
      "[336/400] loss: 0.163\n",
      "[337/400] loss: 0.163\n",
      "[338/400] loss: 0.163\n",
      "[339/400] loss: 0.163\n",
      "[340/400] loss: 0.163\n",
      "[341/400] loss: 0.163\n",
      "[342/400] loss: 0.163\n",
      "[343/400] loss: 0.163\n",
      "[344/400] loss: 0.163\n",
      "[345/400] loss: 0.163\n",
      "[346/400] loss: 0.163\n",
      "[347/400] loss: 0.163\n",
      "[348/400] loss: 0.163\n",
      "[349/400] loss: 0.163\n",
      "[350/400] loss: 0.163\n",
      "[351/400] loss: 0.163\n",
      "[352/400] loss: 0.163\n",
      "[353/400] loss: 0.163\n",
      "[354/400] loss: 0.163\n",
      "[355/400] loss: 0.163\n",
      "[356/400] loss: 0.163\n",
      "[357/400] loss: 0.163\n",
      "[358/400] loss: 0.163\n",
      "[359/400] loss: 0.163\n",
      "[360/400] loss: 0.163\n",
      "[361/400] loss: 0.163\n",
      "[362/400] loss: 0.163\n",
      "[363/400] loss: 0.163\n",
      "[364/400] loss: 0.163\n",
      "[365/400] loss: 0.163\n",
      "[366/400] loss: 0.163\n",
      "[367/400] loss: 0.163\n",
      "[368/400] loss: 0.163\n",
      "[369/400] loss: 0.163\n",
      "[370/400] loss: 0.163\n",
      "[371/400] loss: 0.163\n",
      "[372/400] loss: 0.163\n",
      "[373/400] loss: 0.163\n",
      "[374/400] loss: 0.163\n",
      "[375/400] loss: 0.163\n",
      "[376/400] loss: 0.163\n",
      "[377/400] loss: 0.163\n",
      "[378/400] loss: 0.163\n",
      "[379/400] loss: 0.163\n",
      "[380/400] loss: 0.163\n",
      "[381/400] loss: 0.163\n",
      "[382/400] loss: 0.163\n",
      "[383/400] loss: 0.163\n",
      "[384/400] loss: 0.163\n",
      "[385/400] loss: 0.163\n",
      "[386/400] loss: 0.163\n",
      "[387/400] loss: 0.163\n",
      "[388/400] loss: 0.163\n",
      "[389/400] loss: 0.163\n",
      "[390/400] loss: 0.163\n",
      "[391/400] loss: 0.163\n",
      "[392/400] loss: 0.163\n",
      "[393/400] loss: 0.163\n",
      "[394/400] loss: 0.163\n",
      "[395/400] loss: 0.163\n",
      "[396/400] loss: 0.163\n",
      "[397/400] loss: 0.163\n",
      "[398/400] loss: 0.163\n",
      "[399/400] loss: 0.163\n",
      "[400/400] loss: 0.163\n",
      "Finished Training\n",
      "0.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import os\n",
    "# import sys\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import warnings\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# # --- Sklearn ---\n",
    "# from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "# from sklearn import decomposition, discriminant_analysis\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # --- Models ---\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn import svm\n",
    "# from sklearn import neural_network\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# # --- Utility ---\n",
    "# import os\n",
    "# import pickle, torch\n",
    "# import numpy as np, pandas as pd\n",
    "# import seaborn as sn\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# dataPath = \"data/statistics (pacing).csv\"\n",
    "# df_full = pd.read_csv(dataPath)\n",
    "# print(df_full.describe())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "columnList = df_full.columns\n",
    "print(f\"Total columns list: {columnList}\")\n",
    "\n",
    "# Dropping columns that are not required at the moment\n",
    "df = df_full.drop(columns=[ 'Unnamed: 0', 'UUID', 'HOSTNAME', 'ALIAS', 'TIMESTAMP', 'STREAMS',\n",
    "                            'THROUGHPUT (Receiver)', 'LATENCY (min.)', 'LATENCY (max.)', \n",
    "                            'CONGESTION (Receiver)', 'BYTES (Receiver)'\n",
    "                          ])\n",
    "\n",
    "print(f\"New columns list: {df.columns}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# df.head(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pacing = df['PACING'].values\n",
    "for i, p in enumerate(pacing):\n",
    "    v, _ = p.split(\"gbit\")\n",
    "    pacing[i] = int(v)\n",
    "\n",
    "df['PACING'] = pacing\n",
    "# df['CONGESTION (Sender)'] = df['CONGESTION (Sender)']==\"cubic\"=1\n",
    "# df['CONGESTION (Sender)'] = df['CONGESTION (Sender)']==\"bbr2\"=0\n",
    "df['CONGESTION (Sender)'] = (df['CONGESTION (Sender)'] == 'cubic').astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "df.head(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sns.set(style='whitegrid', context='notebook')\n",
    "# cols = ['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'CONGESTION (Sender)', 'PACING']\n",
    "\n",
    "# sns.pairplot(df[cols], height=3)\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig('./figures/scatter.png', dpi=300)\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS']].values\n",
    "y = df['PACING'].values\n",
    "y = y.astype('int')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_and_tune(X, y, model, parameters, scoring='f1_macro', kfold=5, verbose=0):\n",
    "    \"\"\"\n",
    "    X:          array-like of shape (n_samples, n_features)\n",
    "    y:          array-like of shape (n_samples,)\n",
    "    model:      (object) a sklearn model class\n",
    "    parameters: (dict) contains the parameters you want to tune in the model\n",
    "    metric:     (str) the metric used to evaluate the quality of the model\n",
    "    return:     a trained model with the best parameters\n",
    "    \"\"\"\n",
    "    cvSearchObj = GridSearchCV(model,\n",
    "                               parameters,\n",
    "                               scoring=scoring,\n",
    "                               n_jobs=-1,\n",
    "                               cv=kfold,\n",
    "                               verbose=verbose)\n",
    "    cvSearchObj.fit(X,y)\n",
    "    return cvSearchObj.best_estimator_\n",
    "\n",
    "def save_model(filename, model):\n",
    "    \"\"\"\n",
    "    filename: Filename to save the model\n",
    "    model:    Model weights to be saved\n",
    "    \"\"\"\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    print(\"Model Saved\")\n",
    "\n",
    "def load_model(filename):\n",
    "    \"\"\"\n",
    "    filename: Filename to load the model\n",
    "    return:   Model weights that are reloaded\n",
    "    \"\"\"\n",
    "    model_reloaded = pickle.load(open(filename, 'rb'))\n",
    "    return model_reloaded\n",
    "\n",
    "\n",
    "def MLP(train, save, test):\n",
    "    filename = \"./mlpBest.pkl\"\n",
    "    mlp = neural_network.MLPClassifier(random_state=999)\n",
    "    if train:\n",
    "        '''\n",
    "        Train\n",
    "        '''\n",
    "        params = {\"alpha\" : [0.0001],\n",
    "                \"learning_rate_init\" : [0.001],\n",
    "                \"batch_size\" : [32, 64, 128],\n",
    "                \"activation\" : [\"relu\"],\n",
    "                \"early_stopping\" : [True],\n",
    "                \"hidden_layer_sizes\" : [10, 50, 100],\n",
    "                }\n",
    "\n",
    "        mlpBest = train_and_tune(X, y,\n",
    "                                 mlp,\n",
    "                                 params,\n",
    "                                 scoring='f1_macro',\n",
    "                                 kfold=5)\n",
    "\n",
    "        if save:\n",
    "            save_model(filename, mlpBest)\n",
    "\n",
    "    if test:\n",
    "        '''\n",
    "        Test\n",
    "        '''\n",
    "        mlpBest_reloaded = load_model(filename)\n",
    "        pred = mlpBest_reloaded.predict(X)\n",
    "        acc  = mlpBest_reloaded.score(X, y)\n",
    "        \n",
    "        # cf_matrix = confusion_matrix(y, pred)\n",
    "        # df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in classes],\n",
    "        #                      columns = [i for i in classes])\n",
    "        # plt.figure(figsize = (12,10))\n",
    "        # sn.heatmap(df_cm, annot=True)\n",
    "        \n",
    "        print(\"Accuracy: \", acc)\n",
    "\n",
    "MLP(train=True, save=True, test=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "# le.transform(['M', 'B'])\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print('Test Accuracy: %.3f' % model.score(X_test, y_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bf92e7c56550015a5ef2c115972b5497eb0f5cccb81c21dbff7b4769ac887066"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}