{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "# --- System ---\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# --- Utility ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# --- Plot --\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Pytorch ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "root_dir = os.getcwd()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataPath = \"data/statistics-3.csv\"\n",
    "df = pd.read_csv(dataPath)\n",
    "# columnList = df.columns\n",
    "\n",
    "# Dropping columns that are not required at the moment\n",
    "df = df.drop(columns=[ 'Unnamed: 0', 'UUID', 'HOSTNAME', 'ALIAS', 'TIMESTAMP',\n",
    "                       'THROUGHPUT (Receiver)', 'LATENCY (min.)', 'LATENCY (max.)', \n",
    "                       'CONGESTION (Receiver)', 'BYTES (Receiver)'\n",
    "                     ])\n",
    "\n",
    "# Pre-processing\n",
    "pacing = df['PACING'].values\n",
    "for i, p in enumerate(pacing):\n",
    "    v, _ = p.split(\"gbit\")\n",
    "    pacing[i] = int(v)\n",
    "\n",
    "df['PACING'] = pacing\n",
    "df['CONGESTION (Sender)'] = (df['CONGESTION (Sender)'] == 'cubic').astype(int) # Cubic = 1 & BBRV2 = 0\n",
    "\n",
    "X = df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)']].values\n",
    "y = df['PACING'].values\n",
    "y = y.astype('float')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['ALIAS'].unique()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.head(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # Standerdization\n",
    "# std_scale = preprocessing.StandardScaler().fit(df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)']])\n",
    "# df_std = std_scale.transform(df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)']])\n",
    "\n",
    "# # Normalization\n",
    "minmax_scale = preprocessing.MinMaxScaler().fit(df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)']])\n",
    "df_minmax = minmax_scale.transform(df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)']])\n",
    "\n",
    "final_df = pd.DataFrame(df_minmax, columns=['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)'])\n",
    "\n",
    "X = final_df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)']].values\n",
    "\n",
    "final_df.head(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "EPOCH = 400\n",
    "BATCH = 32\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "X_test  = torch.tensor(X_test)\n",
    "y_test  = torch.tensor(y_test) \n",
    "\n",
    "class CustomTensorDataset(Dataset):\n",
    "    \"\"\"\n",
    "    TensorDataset with support of transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        y = self.tensors[1][index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "\n",
    "# Dataset w/o any tranformations\n",
    "traindata   = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
    "trainloader = torch.utils.data.DataLoader(traindata, batch_size=BATCH)\n",
    "\n",
    "testdata    = CustomTensorDataset(tensors=(X_test, y_test), transform=None)\n",
    "testloader = torch.utils.data.DataLoader(testdata, batch_size=BATCH)\n",
    "\n",
    "print(len(traindata), len(testdata))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_feature = 5\n",
    "latent_feature = 16\n",
    "\n",
    "class VAERegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAERegressor, self).__init__()\n",
    " \n",
    "        # encoder\n",
    "        self.enc1 = nn.Linear(in_features=input_feature, out_features=128)\n",
    "        self.enc2 = nn.Linear(in_features=128, out_features=latent_feature*2)\n",
    " \n",
    "        # decoder\n",
    "        self.dec1 = nn.Linear(in_features=latent_feature, out_features=128)\n",
    "        self.dec2 = nn.Linear(in_features=128, out_features=5)\n",
    "\n",
    "        # Regressor\n",
    "        self.fc1 = torch.nn.Linear (5, 32)\n",
    "        self.fc2 = torch.nn.Linear (32, 1)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample\n",
    " \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # encoding\n",
    "        x = self.enc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.enc2(x).view(-1, 2, latent_feature)\n",
    "\n",
    "        # get `mu` and `log_var`\n",
    "        mu      = x[:, 0, :]    # the first feature values as mean\n",
    "        log_var = x[:, 1, :]    # the other feature values as variance\n",
    "\n",
    "        # get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    " \n",
    "        # decoding\n",
    "        x = self.dec1(z)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec2(x)\n",
    "        recon = torch.sigmoid(x)\n",
    "\n",
    "        # regressor\n",
    "        x = self.fc1(recon)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, recon, mu, log_var\n",
    "\n",
    "\n",
    "model = VAERegressor()\n",
    "print( f\"====================\\nTotal params: {len(list(model.parameters()))}\\n====================\" )\n",
    "# print(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "CE  = nn.CrossEntropyLoss()\n",
    "BCE = nn.BCELoss(reduction='mean')\n",
    "MSE = nn.MSELoss(reduction='mean')\n",
    "\n",
    "def criterion(bce_loss, mu, logvar):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (BCELoss) and the \n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    BCE = bce_loss \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "bestloss = 10\n",
    "\n",
    "for epoch in range(EPOCH):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        xs, ys = data\n",
    "        xs, ys = xs.float(), ys.float()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        output, recon, mu, log_var = model(xs)\n",
    "        # print(output, ys)\n",
    "        \n",
    "        # loss = CE(output, ys)\n",
    "        mse_loss = MSE(ys, output)\n",
    "        bce_loss = BCE(recon, xs)\n",
    "        loss = criterion(bce_loss, mu, log_var) + mse_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"[{epoch+1}/{EPOCH}] loss: {running_loss/len(trainloader.dataset):.3f}\")\n",
    "    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "correct, acc, total = 0, 0, 0\n",
    "with torch.no_grad():\n",
    "    for xs, ys in testloader:\n",
    "        xs, ys = xs.float(), ys.long()\n",
    "\n",
    "        output = model(xs)\n",
    "        \n",
    "        mse_loss = MSE(ys, output)\n",
    "        bce_loss = BCE(recon, xs)\n",
    "        loss = criterion(bce_loss, mu, log_var) + mse_loss\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # total += ys.size(0)\n",
    "        # pred = torch.max(output, 1)[1]\n",
    "        # correct += (pred == ys).sum().item()\n",
    "    # acc = (100 * correct / total)\n",
    "# print(acc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import os\n",
    "# import sys\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import warnings\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# # --- Sklearn ---\n",
    "# from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "# from sklearn import decomposition, discriminant_analysis\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # --- Models ---\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn import svm\n",
    "# from sklearn import neural_network\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# # --- Utility ---\n",
    "# import os\n",
    "# import pickle, torch\n",
    "# import numpy as np, pandas as pd\n",
    "# import seaborn as sn\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# dataPath = \"data/statistics (pacing).csv\"\n",
    "# df_full = pd.read_csv(dataPath)\n",
    "# print(df_full.describe())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "columnList = df_full.columns\n",
    "print(f\"Total columns list: {columnList}\")\n",
    "\n",
    "# Dropping columns that are not required at the moment\n",
    "df = df_full.drop(columns=[ 'Unnamed: 0', 'UUID', 'HOSTNAME', 'ALIAS', 'TIMESTAMP', 'STREAMS',\n",
    "                            'THROUGHPUT (Receiver)', 'LATENCY (min.)', 'LATENCY (max.)', \n",
    "                            'CONGESTION (Receiver)', 'BYTES (Receiver)'\n",
    "                          ])\n",
    "\n",
    "print(f\"New columns list: {df.columns}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# df.head(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pacing = df['PACING'].values\n",
    "for i, p in enumerate(pacing):\n",
    "    v, _ = p.split(\"gbit\")\n",
    "    pacing[i] = int(v)\n",
    "\n",
    "df['PACING'] = pacing\n",
    "# df['CONGESTION (Sender)'] = df['CONGESTION (Sender)']==\"cubic\"=1\n",
    "# df['CONGESTION (Sender)'] = df['CONGESTION (Sender)']==\"bbr2\"=0\n",
    "df['CONGESTION (Sender)'] = (df['CONGESTION (Sender)'] == 'cubic').astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "df.head(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sns.set(style='whitegrid', context='notebook')\n",
    "# cols = ['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'CONGESTION (Sender)', 'PACING']\n",
    "\n",
    "# sns.pairplot(df[cols], height=3)\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig('./figures/scatter.png', dpi=300)\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS']].values\n",
    "y = df['PACING'].values\n",
    "y = y.astype('int')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_and_tune(X, y, model, parameters, scoring='f1_macro', kfold=5, verbose=0):\n",
    "    \"\"\"\n",
    "    X:          array-like of shape (n_samples, n_features)\n",
    "    y:          array-like of shape (n_samples,)\n",
    "    model:      (object) a sklearn model class\n",
    "    parameters: (dict) contains the parameters you want to tune in the model\n",
    "    metric:     (str) the metric used to evaluate the quality of the model\n",
    "    return:     a trained model with the best parameters\n",
    "    \"\"\"\n",
    "    cvSearchObj = GridSearchCV(model,\n",
    "                               parameters,\n",
    "                               scoring=scoring,\n",
    "                               n_jobs=-1,\n",
    "                               cv=kfold,\n",
    "                               verbose=verbose)\n",
    "    cvSearchObj.fit(X,y)\n",
    "    return cvSearchObj.best_estimator_\n",
    "\n",
    "def save_model(filename, model):\n",
    "    \"\"\"\n",
    "    filename: Filename to save the model\n",
    "    model:    Model weights to be saved\n",
    "    \"\"\"\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    print(\"Model Saved\")\n",
    "\n",
    "def load_model(filename):\n",
    "    \"\"\"\n",
    "    filename: Filename to load the model\n",
    "    return:   Model weights that are reloaded\n",
    "    \"\"\"\n",
    "    model_reloaded = pickle.load(open(filename, 'rb'))\n",
    "    return model_reloaded\n",
    "\n",
    "\n",
    "def MLP(train, save, test):\n",
    "    filename = \"./mlpBest.pkl\"\n",
    "    mlp = neural_network.MLPClassifier(random_state=999)\n",
    "    if train:\n",
    "        '''\n",
    "        Train\n",
    "        '''\n",
    "        params = {\"alpha\" : [0.0001],\n",
    "                \"learning_rate_init\" : [0.001],\n",
    "                \"batch_size\" : [32, 64, 128],\n",
    "                \"activation\" : [\"relu\"],\n",
    "                \"early_stopping\" : [True],\n",
    "                \"hidden_layer_sizes\" : [10, 50, 100],\n",
    "                }\n",
    "\n",
    "        mlpBest = train_and_tune(X, y,\n",
    "                                 mlp,\n",
    "                                 params,\n",
    "                                 scoring='f1_macro',\n",
    "                                 kfold=5)\n",
    "\n",
    "        if save:\n",
    "            save_model(filename, mlpBest)\n",
    "\n",
    "    if test:\n",
    "        '''\n",
    "        Test\n",
    "        '''\n",
    "        mlpBest_reloaded = load_model(filename)\n",
    "        pred = mlpBest_reloaded.predict(X)\n",
    "        acc  = mlpBest_reloaded.score(X, y)\n",
    "        \n",
    "        # cf_matrix = confusion_matrix(y, pred)\n",
    "        # df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in classes],\n",
    "        #                      columns = [i for i in classes])\n",
    "        # plt.figure(figsize = (12,10))\n",
    "        # sn.heatmap(df_cm, annot=True)\n",
    "        \n",
    "        print(\"Accuracy: \", acc)\n",
    "\n",
    "MLP(train=True, save=True, test=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "# le.transform(['M', 'B'])\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print('Test Accuracy: %.3f' % model.score(X_test, y_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "# --- System ---\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# --- Utility ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# --- Plot ---\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Pytorch ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from lib.dataloader import PacingDataset\n",
    "from lib.classifier import PacingClassifier, resnet50\n",
    "import lib.utils\n",
    "\n",
    "# random weight initialization\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()\n",
    "# ----------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "root_dir = os.getcwd()\n",
    "\n",
    "# data loading and preprocessing\n",
    "dataPath = \"data/statistics-5.csv\"\n",
    "df = pd.read_csv(dataPath)\n",
    "# ----------------------------------\n",
    "# Dropping columns that are not required at the moment\n",
    "df = df.drop(columns=['Unnamed: 0', 'UUID', 'HOSTNAME', 'TIMESTAMP', 'THROUGHPUT (Receiver)', 'LATENCY (mean)', 'CONGESTION (Receiver)', 'BYTES (Receiver)'])\n",
    "\n",
    "# Pre-processing\n",
    "pacing = df['PACING'].values\n",
    "for i, p in enumerate(pacing):\n",
    "    v, _ = p.split(\"gbit\")\n",
    "    pacing[i] = float(v) # int(v)\n",
    "\n",
    "df['PACING'] = pacing\n",
    "df['CONGESTION (Sender)'] = (df['CONGESTION (Sender)'] == 'cubic').astype(int)\n",
    "df['ALIAS'] = pd.factorize(df['ALIAS'])[0]\n",
    "\n",
    "num_of_classes = len(df['PACING'].unique())\n",
    "\n",
    "X = df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']].values\n",
    "y = df['PACING'].values\n",
    "y = y.astype('int')\n",
    "\n",
    "# Normalization\n",
    "minmax_scale = preprocessing.MinMaxScaler().fit(df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']])\n",
    "df_minmax = minmax_scale.transform(df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']])\n",
    "\n",
    "final_df = pd.DataFrame(df_minmax, columns=['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS'])\n",
    "X = final_df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']].values\n",
    "# ----------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=1)\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "X_test  = torch.tensor(X_test)\n",
    "y_test  = torch.tensor(y_test)\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCH = 1500\n",
    "BATCH = 512\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "INTERVAL = 50\n",
    "SAVE = False\n",
    "BESTLOSS = 10\n",
    "\n",
    "CE  = nn.CrossEntropyLoss()\n",
    "BCE = nn.BCELoss(reduction='mean')\n",
    "MSE = nn.MSELoss(reduction='mean') # 'mean', 'sum'. 'none'\n",
    "\n",
    "# Dataset w/o any tranformations\n",
    "traindata   = PacingDataset(tensors=(X_train, y_train), transform=None)\n",
    "trainloader = torch.utils.data.DataLoader(traindata, batch_size=BATCH)\n",
    "testdata    = PacingDataset(tensors=(X_test, y_test), transform=None)\n",
    "testloader = torch.utils.data.DataLoader(testdata, batch_size=1)\n",
    "\n",
    "inputFea = len(traindata[0][0])\n",
    "model = PacingClassifier (nc=num_of_classes, inputFeatures=inputFea)\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[350,500], gamma=0.1)\n",
    "\n",
    "print(\"\\nBatch Size = %3d \" % BATCH)\n",
    "print(\"Loss = \" + str(CE))\n",
    "print(\"Optimizer = SGD\")\n",
    "print(\"Max Epochs = %3d \" % EPOCH)\n",
    "print(\"Learning Rate = %0.3f \" % LEARNING_RATE)\n",
    "print(\"Number of Classes = %d \" % num_of_classes)\n",
    "print(\"\\nStarting training ...\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(0, EPOCH):\n",
    "    torch.manual_seed(epoch+1) # recovery reproducibility\n",
    "    epoch_loss = 0             # for one full epoch\n",
    "\n",
    "    for (batch_idx, batch) in enumerate(trainloader):\n",
    "        (xs, ys) = batch                # (predictors, targets)\n",
    "        xs, ys = xs.float(), ys.float()\n",
    "        optimizer.zero_grad()           # prepare gradients\n",
    "\n",
    "        output = model(xs)              # predicted pacing rate\n",
    "        loss = CE(output, ys.long())    # avg per item in batch\n",
    "\n",
    "        epoch_loss += loss.item()       # accumulate averages\n",
    "        loss.backward()                 # compute gradients\n",
    "        optimizer.step()                # update weights\n",
    "    \n",
    "    scheduler.step()\n",
    "    if epoch % INTERVAL == 0:\n",
    "        print(\"Epoch = %4d    Loss = %0.4f\" % (epoch, epoch_loss))\n",
    "\n",
    "        # save checkpoint\n",
    "        dt = time.strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "        fn = str(dt) + str(\"-\") + str(epoch) + \"_ckpt.pt\"\n",
    "\n",
    "        info_dict = {\n",
    "            'epoch' : epoch,\n",
    "            'model_state' : model.state_dict(),\n",
    "            'optimizer_state' : optimizer.state_dict()\n",
    "        }\n",
    "        if SAVE:\n",
    "            torch.save(info_dict, fn)\n",
    "\n",
    "print(\"\\nDone\")\n",
    "\n",
    "model.eval()\n",
    "correct, acc = 0, 0\n",
    "with torch.no_grad():\n",
    "    for xs, ys in testloader:\n",
    "        xs, ys = xs.float(), ys.long()\n",
    "        pred = torch.max(model(xs), 1)[1]\n",
    "        correct += (pred == ys).sum().item()\n",
    "    acc = (100 * float(correct / len(testdata)) )\n",
    "\n",
    "print(f\"Accuracy: {acc:.3f}%\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.PACING.hist(figsize=(14,4))\n",
    "plt.title('Pacing Rates')\n",
    "df[['THROUGHPUT (Sender)']].hist(figsize=(14,4))\n",
    "plt.title('Throughputs')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.pairplot(final_df, diag_kind=\"kde\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train_ = pd.get_dummies(X_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_test.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_test_ = pd.get_dummies(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "# --- System ---\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# --- Utility ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# --- Plot ---\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Pytorch ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from lib.dataloader import PacingDataset\n",
    "from lib.classifier import PacingClassifier, resnet50\n",
    "import lib.utils\n",
    "\n",
    "# random weight initialization\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()\n",
    "# ----------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "root_dir = os.getcwd()\n",
    "\n",
    "# data loading and preprocessing\n",
    "dataPath = \"data/statistics-5.csv\"\n",
    "df = pd.read_csv(dataPath)\n",
    "# ----------------------------------\n",
    "# Dropping columns that are not required at the moment\n",
    "df = df.drop(columns=['Unnamed: 0', 'UUID', 'HOSTNAME', 'TIMESTAMP', 'THROUGHPUT (Receiver)', 'LATENCY (mean)', 'CONGESTION (Receiver)', 'BYTES (Receiver)'])\n",
    "\n",
    "# Pre-processing\n",
    "pacing = df['PACING'].values\n",
    "for i, p in enumerate(pacing):\n",
    "    v, _ = p.split(\"gbit\")\n",
    "    pacing[i] = float(v) # int(v)\n",
    "\n",
    "df['PACING'] = pacing\n",
    "df['CONGESTION (Sender)'] = (df['CONGESTION (Sender)'] == 'cubic').astype(int)\n",
    "df['ALIAS'] = pd.factorize(df['ALIAS'])[0]\n",
    "\n",
    "num_of_classes = len(df['PACING'].unique())\n",
    "\n",
    "X = df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']].values\n",
    "y = df['PACING'].values\n",
    "y = y.astype('float')\n",
    "\n",
    "# Normalization\n",
    "minmax_scale = preprocessing.MinMaxScaler().fit(df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']])\n",
    "df_minmax = minmax_scale.transform(df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']])\n",
    "\n",
    "final_df = pd.DataFrame(df_minmax, columns=['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS'])\n",
    "X = final_df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']].values\n",
    "# ----------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "X_test  = torch.tensor(X_test)\n",
    "y_test  = torch.tensor(y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from xgboost import XGBRegressor\n",
    "from ml_metrics import rmse\n",
    "\n",
    "model_up = XGBRegressor(n_estimators=350,max_depth=15,random_state=5,learning_rate=.1,\n",
    "                        n_jobs=10, subsample=1,min_child_weight=0.6) # Your code here\n",
    "\n",
    "# Fit the model\n",
    "model_up.fit(X_train, y_train)\n",
    "\n",
    "# Get validation predictions and MAE\n",
    "preds = model_up.predict(X_test)\n",
    "\n",
    "print(\"RMSE (Your appraoch):\")\n",
    "print(rmse(y_test, preds))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "# --- System ---\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# --- Utility ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# --- Plot ---\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Pytorch ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "# random weight initialization\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()\n",
    "# ----------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "root_dir = os.getcwd()\n",
    "\n",
    "# data loading and preprocessing\n",
    "dataPath = \"data/statistics-5.csv\"\n",
    "df = pd.read_csv(dataPath)\n",
    "# ----------------------------------\n",
    "# Dropping columns that are not required at the moment\n",
    "df = df.drop(columns=['Unnamed: 0', 'UUID', 'HOSTNAME', 'TIMESTAMP', 'THROUGHPUT (Receiver)', 'LATENCY (mean)', 'CONGESTION (Receiver)', 'BYTES (Receiver)'])\n",
    "original_df = df\n",
    "\n",
    "# Pre-processing\n",
    "pacing = original_df['PACING'].values\n",
    "for i, p in enumerate(pacing):\n",
    "    v, _ = p.split(\"gbit\")\n",
    "    pacing[i] = float(v) # int(v)\n",
    "\n",
    "original_df['PACING'] = pacing\n",
    "original_df.drop( original_df[ original_df['PACING'] == 10.5 ].index, inplace=True)\n",
    "num_of_classes = len(original_df['PACING'].unique())\n",
    "\n",
    "# w/o preprocessing\n",
    "original_df.head(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   ALIAS  STREAMS PACING  THROUGHPUT (Sender)  LATENCY (min.)  LATENCY (max.)  \\\n",
       "0  hostA        1    1.0         1.623277e+09         30062.0         30264.5   \n",
       "1  hostA        1    1.0         1.652145e+09         60206.5         60572.0   \n",
       "2  hostA        1    1.0         9.833584e+08         91576.5         92073.0   \n",
       "3  hostA        1    2.0         1.965511e+09        122954.0        123533.5   \n",
       "4  hostA        1    3.0         2.946649e+09        154383.5        155109.0   \n",
       "\n",
       "   RETRANSMITS CONGESTION (Sender)  \n",
       "0       1535.0               cubic  \n",
       "1       2879.0               cubic  \n",
       "2       2879.0               cubic  \n",
       "3       2879.0               cubic  \n",
       "4       2879.0               cubic  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALIAS</th>\n",
       "      <th>STREAMS</th>\n",
       "      <th>PACING</th>\n",
       "      <th>THROUGHPUT (Sender)</th>\n",
       "      <th>LATENCY (min.)</th>\n",
       "      <th>LATENCY (max.)</th>\n",
       "      <th>RETRANSMITS</th>\n",
       "      <th>CONGESTION (Sender)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hostA</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.623277e+09</td>\n",
       "      <td>30062.0</td>\n",
       "      <td>30264.5</td>\n",
       "      <td>1535.0</td>\n",
       "      <td>cubic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hostA</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.652145e+09</td>\n",
       "      <td>60206.5</td>\n",
       "      <td>60572.0</td>\n",
       "      <td>2879.0</td>\n",
       "      <td>cubic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hostA</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.833584e+08</td>\n",
       "      <td>91576.5</td>\n",
       "      <td>92073.0</td>\n",
       "      <td>2879.0</td>\n",
       "      <td>cubic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hostA</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.965511e+09</td>\n",
       "      <td>122954.0</td>\n",
       "      <td>123533.5</td>\n",
       "      <td>2879.0</td>\n",
       "      <td>cubic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hostA</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.946649e+09</td>\n",
       "      <td>154383.5</td>\n",
       "      <td>155109.0</td>\n",
       "      <td>2879.0</td>\n",
       "      <td>cubic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "\n",
    "alias_df = original_df.join(pd.DataFrame.sparse.from_spmatrix(mlb.fit_transform(original_df.pop('ALIAS')),\n",
    "                                                            index=original_df.index,\n",
    "                                                            columns=mlb.classes_))\n",
    "\n",
    "df_ = alias_df.join(pd.DataFrame.sparse.from_spmatrix(mlb.fit_transform(alias_df.pop('CONGESTION (Sender)')),\n",
    "                                                            index=alias_df.index,\n",
    "                                                            columns=mlb.classes_),\n",
    "                                                            how = 'left', lsuffix='left', rsuffix='right')\n",
    "\n",
    "# df_['CONGESTION (Sender)'] = (df_['CONGESTION (Sender)'] == 'cubic').astype(int)\n",
    "df_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      STREAMS PACING  THROUGHPUT (Sender)  LATENCY (min.)  LATENCY (max.)  \\\n",
       "0           1    1.0         1.623277e+09         30062.0         30264.5   \n",
       "1           1    1.0         1.652145e+09         60206.5         60572.0   \n",
       "2           1    1.0         9.833584e+08         91576.5         92073.0   \n",
       "3           1    2.0         1.965511e+09        122954.0        123533.5   \n",
       "4           1    3.0         2.946649e+09        154383.5        155109.0   \n",
       "...       ...    ...                  ...             ...             ...   \n",
       "5121        1    2.0         1.612733e+09       3131908.0       4538737.5   \n",
       "5122        1    2.0         1.866827e+09       3163305.0       4570179.0   \n",
       "5123        1    1.0         8.338280e+08       3243547.5       4651874.0   \n",
       "5124        1    1.0         4.970197e+08       3298707.5       4709556.5   \n",
       "5125        1    1.0         8.381282e+08       3378968.5       4791379.5   \n",
       "\n",
       "      RETRANSMITS  A  B  C  D  ...  h  o  s  t  2  bright  cright  i  r  u  \n",
       "0          1535.0  1  0  0  0  ...  1  1  1  1  0       1       1  1  0  1  \n",
       "1          2879.0  1  0  0  0  ...  1  1  1  1  0       1       1  1  0  1  \n",
       "2          2879.0  1  0  0  0  ...  1  1  1  1  0       1       1  1  0  1  \n",
       "3          2879.0  1  0  0  0  ...  1  1  1  1  0       1       1  1  0  1  \n",
       "4          2879.0  1  0  0  0  ...  1  1  1  1  0       1       1  1  0  1  \n",
       "...           ... .. .. .. ..  ... .. .. .. .. ..     ...     ... .. .. ..  \n",
       "5121      10626.0  0  0  0  1  ...  1  1  1  1  0       1       1  1  0  1  \n",
       "5122      10626.0  1  0  0  0  ...  1  1  1  1  0       1       1  1  0  1  \n",
       "5123      10626.0  0  0  1  0  ...  1  1  1  1  0       1       1  1  0  1  \n",
       "5124      10635.0  0  1  0  0  ...  1  1  1  1  0       1       1  1  0  1  \n",
       "5125      10636.0  1  0  0  0  ...  1  1  1  1  0       1       1  1  0  1  \n",
       "\n",
       "[5054 rows x 27 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STREAMS</th>\n",
       "      <th>PACING</th>\n",
       "      <th>THROUGHPUT (Sender)</th>\n",
       "      <th>LATENCY (min.)</th>\n",
       "      <th>LATENCY (max.)</th>\n",
       "      <th>RETRANSMITS</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>...</th>\n",
       "      <th>h</th>\n",
       "      <th>o</th>\n",
       "      <th>s</th>\n",
       "      <th>t</th>\n",
       "      <th>2</th>\n",
       "      <th>bright</th>\n",
       "      <th>cright</th>\n",
       "      <th>i</th>\n",
       "      <th>r</th>\n",
       "      <th>u</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.623277e+09</td>\n",
       "      <td>30062.0</td>\n",
       "      <td>30264.5</td>\n",
       "      <td>1535.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.652145e+09</td>\n",
       "      <td>60206.5</td>\n",
       "      <td>60572.0</td>\n",
       "      <td>2879.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.833584e+08</td>\n",
       "      <td>91576.5</td>\n",
       "      <td>92073.0</td>\n",
       "      <td>2879.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.965511e+09</td>\n",
       "      <td>122954.0</td>\n",
       "      <td>123533.5</td>\n",
       "      <td>2879.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.946649e+09</td>\n",
       "      <td>154383.5</td>\n",
       "      <td>155109.0</td>\n",
       "      <td>2879.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5121</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.612733e+09</td>\n",
       "      <td>3131908.0</td>\n",
       "      <td>4538737.5</td>\n",
       "      <td>10626.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5122</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.866827e+09</td>\n",
       "      <td>3163305.0</td>\n",
       "      <td>4570179.0</td>\n",
       "      <td>10626.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.338280e+08</td>\n",
       "      <td>3243547.5</td>\n",
       "      <td>4651874.0</td>\n",
       "      <td>10626.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5124</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.970197e+08</td>\n",
       "      <td>3298707.5</td>\n",
       "      <td>4709556.5</td>\n",
       "      <td>10635.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5125</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.381282e+08</td>\n",
       "      <td>3378968.5</td>\n",
       "      <td>4791379.5</td>\n",
       "      <td>10636.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5054 rows  27 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "X = df_[df_.columns.values].values\n",
    "y = df_['PACING'].values\n",
    "y = y.astype('float')\n",
    "\n",
    "# Normalization\n",
    "minmax_scale = preprocessing.MinMaxScaler().fit(df_[df_.columns.values])\n",
    "df_minmax = minmax_scale.transform(df_[df_.columns.values])\n",
    "\n",
    "final_df = pd.DataFrame(df_minmax, columns=df_.columns.values)\n",
    "X = final_df[df_.columns.values].values\n",
    "# ----------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=1)\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "X_test  = torch.tensor(X_test)\n",
    "y_test  = torch.tensor(y_test)\n",
    "\n",
    "# normalized data\n",
    "final_df.head(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   STREAMS    PACING  THROUGHPUT (Sender)  LATENCY (min.)  LATENCY (max.)  \\\n",
       "0      0.0  0.052632             0.162861        0.000000        0.000000   \n",
       "1      0.0  0.052632             0.165783        0.000132        0.000129   \n",
       "2      0.0  0.052632             0.098080        0.000269        0.000263   \n",
       "3      0.0  0.157895             0.197506        0.000407        0.000397   \n",
       "4      0.0  0.263158             0.296830        0.000544        0.000531   \n",
       "\n",
       "   RETRANSMITS    A    B    C    D  ...    h    o    s    t    2  bright  \\\n",
       "0     0.014931  1.0  0.0  0.0  0.0  ...  1.0  1.0  1.0  1.0  0.0     0.0   \n",
       "1     0.028003  1.0  0.0  0.0  0.0  ...  1.0  1.0  1.0  1.0  0.0     0.0   \n",
       "2     0.028003  1.0  0.0  0.0  0.0  ...  1.0  1.0  1.0  1.0  0.0     0.0   \n",
       "3     0.028003  1.0  0.0  0.0  0.0  ...  1.0  1.0  1.0  1.0  0.0     0.0   \n",
       "4     0.028003  1.0  0.0  0.0  0.0  ...  1.0  1.0  1.0  1.0  0.0     0.0   \n",
       "\n",
       "   cright    i    r    u  \n",
       "0     1.0  1.0  0.0  1.0  \n",
       "1     1.0  1.0  0.0  1.0  \n",
       "2     1.0  1.0  0.0  1.0  \n",
       "3     1.0  1.0  0.0  1.0  \n",
       "4     1.0  1.0  0.0  1.0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STREAMS</th>\n",
       "      <th>PACING</th>\n",
       "      <th>THROUGHPUT (Sender)</th>\n",
       "      <th>LATENCY (min.)</th>\n",
       "      <th>LATENCY (max.)</th>\n",
       "      <th>RETRANSMITS</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>...</th>\n",
       "      <th>h</th>\n",
       "      <th>o</th>\n",
       "      <th>s</th>\n",
       "      <th>t</th>\n",
       "      <th>2</th>\n",
       "      <th>bright</th>\n",
       "      <th>cright</th>\n",
       "      <th>i</th>\n",
       "      <th>r</th>\n",
       "      <th>u</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.162861</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014931</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.165783</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.028003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.098080</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.028003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.197506</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.028003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.296830</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.028003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  27 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Hyperparameters\n",
    "EPOCH = 300\n",
    "BATCH = 256\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "INTERVAL = 50\n",
    "SAVE = False\n",
    "BESTLOSS = 10\n",
    "\n",
    "CE  = nn.CrossEntropyLoss()\n",
    "BCE = nn.BCELoss(reduction='mean')\n",
    "MSE = nn.MSELoss(reduction='mean') # 'mean', 'sum'. 'none'\n",
    "\n",
    "# Custom data loader for ELK stack dataset\n",
    "class PacingDataset(Dataset):\n",
    "    \"\"\" TensorDataset with support of transforms. \"\"\"\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        y = self.tensors[1][index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "\n",
    "# Dataset w/o any tranformations\n",
    "traindata   = PacingDataset(tensors=(X_train, y_train), transform=None)\n",
    "trainloader = torch.utils.data.DataLoader(traindata, batch_size=BATCH)\n",
    "testdata    = PacingDataset(tensors=(X_test, y_test), transform=None)\n",
    "testloader = torch.utils.data.DataLoader(testdata, batch_size=1)\n",
    "\n",
    "inputFea = len(traindata[0][0])\n",
    "\n",
    "# model definition\n",
    "class PacingClassifier (nn.Module):\n",
    "    # https://visualstudiomagazine.com/Articles/2021/02/11/pytorch-define.aspx?Page=2\n",
    "    def __init__(self, nc=20, inputFeatures=7):\n",
    "        super(PacingClassifier, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(inputFeatures, 32)\n",
    "        self.fc2 = torch.nn.Linear(32, 64)\n",
    "        self.fc3 = torch.nn.Linear(64, 128)\n",
    "        self.fc4 = torch.nn.Linear(128, 128)\n",
    "        self.fc5 = torch.nn.Linear(128, 64)\n",
    "        self.fc6 = torch.nn.Linear(64, nc)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.zeros_(self.fc1.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        torch.nn.init.zeros_(self.fc2.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        torch.nn.init.zeros_(self.fc3.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.fc4.weight)\n",
    "        torch.nn.init.zeros_(self.fc4.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.fc5.weight)\n",
    "        torch.nn.init.zeros_(self.fc5.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.fc6.weight)\n",
    "        torch.nn.init.zeros_(self.fc6.bias)\n",
    "\n",
    "        self.lrelu = torch.nn.LeakyReLU(negative_slope=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.lrelu(self.fc1(x))\n",
    "        z = self.lrelu(self.fc2(z))\n",
    "        z = self.lrelu(self.fc3(z))\n",
    "        z = self.lrelu(self.fc4(z))\n",
    "        z = self.lrelu(self.fc5(z))\n",
    "        z = self.fc6(z)  # no activation\n",
    "        return z\n",
    "\n",
    "model = PacingClassifier (nc=num_of_classes, inputFeatures=inputFea)\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[250], gamma=0.1)\n",
    "\n",
    "print(\"\\nBatch Size = %3d \" % BATCH)\n",
    "print(\"Loss = \" + str(CE))\n",
    "print(\"Optimizer = SGD\")\n",
    "print(\"Max Epochs = %3d \" % EPOCH)\n",
    "print(\"Learning Rate = %0.3f \" % LEARNING_RATE)\n",
    "print(\"Number of Classes = %d \" % num_of_classes)\n",
    "print(\"\\nStarting training ...\")\n",
    "\n",
    "model.train()\n",
    "trainloss = []\n",
    "for epoch in range(0, EPOCH):\n",
    "    torch.manual_seed(epoch+1) # recovery reproducibility\n",
    "    epoch_loss = 0             # for one full epoch\n",
    "\n",
    "    for (batch_idx, batch) in enumerate(trainloader):\n",
    "        (xs, ys) = batch                # (predictors, targets)\n",
    "        xs, ys = xs.float(), ys.float()\n",
    "        optimizer.zero_grad()           # prepare gradients\n",
    "\n",
    "        output = model(xs)              # predicted pacing rate\n",
    "        loss = CE(output, ys.long())    # avg per item in batch\n",
    "\n",
    "        epoch_loss += loss.item()       # accumulate averages\n",
    "        loss.backward()                 # compute gradients\n",
    "        optimizer.step()                # update weights\n",
    "    \n",
    "    scheduler.step()\n",
    "    if epoch % INTERVAL == 0:\n",
    "        # print(\"Epoch = %4d    Loss = %0.4f\" % (epoch, epoch_loss))\n",
    "\n",
    "        model.eval()\n",
    "        correct, acc = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for xs, ys in testloader:\n",
    "                xs, ys = xs.float(), ys.long()\n",
    "                pred = torch.max(model(xs), 1)[1]\n",
    "                correct += (pred == ys).sum().item()\n",
    "            acc = (100 * float(correct / len(testdata)) )\n",
    "\n",
    "        print(\"Epoch = %4d    Loss = %0.4f    Accuracy = %0.4f\" % (epoch, epoch_loss, acc))\n",
    "\n",
    "        # save checkpoint\n",
    "        dt = time.strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "        fn = str(dt) + str(\"-\") + str(epoch) + \"_ckpt.pt\"\n",
    "\n",
    "        info_dict = {\n",
    "            'epoch' : epoch,\n",
    "            'model_state' : model.state_dict(),\n",
    "            'optimizer_state' : optimizer.state_dict()\n",
    "        }\n",
    "        if SAVE:\n",
    "            torch.save(info_dict, fn)\n",
    "\n",
    "print(\"\\nDone\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PacingClassifier(\n",
      "  (fc1): Linear(in_features=27, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (fc4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc5): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc6): Linear(in_features=64, out_features=20, bias=True)\n",
      "  (lrelu): LeakyReLU(negative_slope=0.02)\n",
      ")\n",
      "\n",
      "Batch Size = 256 \n",
      "Loss = CrossEntropyLoss()\n",
      "Optimizer = SGD\n",
      "Max Epochs = 300 \n",
      "Learning Rate = 0.001 \n",
      "Number of Classes = 20 \n",
      "\n",
      "Starting training ...\n",
      "Epoch =    0    Loss = 44.5977    Accuracy = 6.0918\n",
      "Epoch =   50    Loss = 33.3877    Accuracy = 16.5348\n",
      "Epoch =  100    Loss = 15.5130    Accuracy = 73.0222\n",
      "Epoch =  150    Loss = 3.0257    Accuracy = 97.1519\n",
      "Epoch =  200    Loss = 0.8853    Accuracy = 99.2089\n",
      "Epoch =  250    Loss = 0.3739    Accuracy = 99.8418\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6e2000e74d83ab38a7a73c8353776f8595191be383670aed524d2a15b88663d"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('bbrv2': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}