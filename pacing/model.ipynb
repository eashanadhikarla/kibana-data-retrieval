{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "# --- System ---\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# --- Utility ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Plot --\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Pytorch ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "# import torchvision.datasets as datasets\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.data import random_split \n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "root_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \"data/statistics (pacing).csv\"\n",
    "df = pd.read_csv(dataPath)\n",
    "# columnList = df.columns\n",
    "\n",
    "# Dropping columns that are not required at the moment\n",
    "df = df.drop(columns=[ 'Unnamed: 0', 'UUID', 'HOSTNAME', 'ALIAS', 'TIMESTAMP',\n",
    "                       'THROUGHPUT (Receiver)', 'LATENCY (min.)', 'LATENCY (max.)', \n",
    "                       'CONGESTION (Receiver)', 'BYTES (Receiver)'\n",
    "                     ])\n",
    "\n",
    "# Pre-processing\n",
    "pacing = df['PACING'].values\n",
    "for i, p in enumerate(pacing):\n",
    "    v, _ = p.split(\"gbit\")\n",
    "    pacing[i] = int(v)\n",
    "\n",
    "df['PACING'] = pacing\n",
    "df['CONGESTION (Sender)'] = (df['CONGESTION (Sender)'] == 'cubic').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)']].values\n",
    "y = df['PACING'].values\n",
    "y = y.astype('int')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 50\n",
    "BATCH = 4\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "mean, std = 602736768.0000, 1598374016.0000\n",
    "\n",
    "transform_=transforms.Compose([\n",
    "          transforms.ToTensor()\n",
    "        , transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "traindata   = TensorDataset(torch.Tensor(X_train),\n",
    "                            torch.Tensor(y_train),\n",
    "                            transform=transform_)\n",
    "trainloader = DataLoader(traindata,\n",
    "                        batch_size = BATCH,\n",
    "                        shuffle = True\n",
    "                        )\n",
    "\n",
    "testdata   = TensorDataset(torch.Tensor(X_test),\n",
    "                           torch.Tensor(y_test),\n",
    "                           transform=transform_\n",
    "                          )\n",
    "testloader = DataLoader(testdata,\n",
    "                        batch_size = BATCH,\n",
    "                        shuffle = True\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602736768.0000, 1598374016.0000\n"
     ]
    }
   ],
   "source": [
    "# data = next(iter(trainloader))\n",
    "# mean, std = data[0].mean(), data[0].std()\n",
    "# print(f\"{mean:.4f}, {std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear (5, 64)\n",
    "        self.fc2 = torch.nn.Linear (64, 64)\n",
    "        self.fc3 = torch.nn.Linear (64, 1)\n",
    "        self.sig = torch.nn.Sigmoid(),\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Network()\n",
    "print( f\"====================\\nTotal params: {len(list(model.parameters()))}\\n====================\" )\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "bestloss = 10\n",
    "\n",
    "# for epoch in range(EPOCH):  # loop over the dataset multiple times\n",
    "\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(trainloader, 0):\n",
    "#         # get the inputs; data is a list of [inputs, labels]\n",
    "#         xs, ys = data\n",
    "#         xs, ys = xs.float(), ys.float()\n",
    "\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward + backward + optimize\n",
    "#         outputs = net(xs)\n",
    "\n",
    "#         loss = criterion(ys, outputs)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # print statistics\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     print(f\"[{epoch+1}/{i+1}] loss: {running_loss/len(trainloader.dataset)}\")\n",
    "#     running_loss = 0.0\n",
    "\n",
    "# print('Finished Training')\n",
    "\n",
    "def train(epoch):\n",
    "\n",
    "    acc, correct, loss = 0.0, 0.0, 0.0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for xs, ys in trainloader:\n",
    "        xs, ys = xs.to(device), ys.to(device)\n",
    "        # --- Model ---\n",
    "        optimizer.zero_grad()\n",
    "        output =  model(xs)\n",
    "        # --- Loss ---\n",
    "        loss = criterion(ys, output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # --- Statistics ---\n",
    "        running_loss += loss.item() * xs.size(0)\n",
    "    epoch_loss  = running_loss/len(traindata)\n",
    "    return epoch_loss\n",
    "\n",
    "def test(epoch):\n",
    "    \n",
    "    acc, correct, loss = 0.0, 0.0, 0.0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xs, ys in testloader:\n",
    "            xs, ys = xs.to(device), ys.to(device)\n",
    "            \n",
    "            # --- Model ---\n",
    "            output = model(xs)\n",
    "            # --- Loss ---\n",
    "            loss = criterion(ys, output)\n",
    "            # --- Statistics ---\n",
    "            running_loss += loss.item() * xs.size(0)\n",
    "        epoch_loss  = running_loss/len(testdata)\n",
    "    return epoch_loss\n",
    "    \n",
    "if not os.path.isdir(str(root_dir)+'/checkpoint'):\n",
    "    os.mkdir(str(root_dir)+'/checkpoint')\n",
    "\n",
    "print(\"Epoch\", \"TR-loss\", \"TS-loss\", sep=' '*8, end=\"\\n\")\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    \n",
    "    trainloss = train(epoch)\n",
    "    testloss = test(epoch)\n",
    "\n",
    "    print(f\"{epoch+0:03}/{EPOCH}\", f\"{trainloss:.4f}\", f\"{testloss:.4f}\", sep=' '*8, end=\"\\n\")\n",
    "    \n",
    "    # Saving the model.\n",
    "    is_best = testloss < bestloss\n",
    "    bestloss = min(testloss, bestloss)\n",
    "    if is_best:\n",
    "        torch.save(model.state_dict(), str(root_dir)+\"/checkpoint/pacing_\"+str(epoch)+\".pt\")\n",
    "        print(\"Model Saved.\")\n",
    "print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import warnings\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# # --- Sklearn ---\n",
    "# from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "# from sklearn import decomposition, discriminant_analysis\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # --- Models ---\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn import svm\n",
    "# from sklearn import neural_network\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# # --- Utility ---\n",
    "# import os\n",
    "# import pickle, torch\n",
    "# import numpy as np, pandas as pd\n",
    "# import seaborn as sn\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# dataPath = \"data/statistics (pacing).csv\"\n",
    "# df_full = pd.read_csv(dataPath)\n",
    "# print(df_full.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnList = df_full.columns\n",
    "print(f\"Total columns list: {columnList}\")\n",
    "\n",
    "# Dropping columns that are not required at the moment\n",
    "df = df_full.drop(columns=[ 'Unnamed: 0', 'UUID', 'HOSTNAME', 'ALIAS', 'TIMESTAMP', 'STREAMS',\n",
    "                            'THROUGHPUT (Receiver)', 'LATENCY (min.)', 'LATENCY (max.)', \n",
    "                            'CONGESTION (Receiver)', 'BYTES (Receiver)'\n",
    "                          ])\n",
    "\n",
    "print(f\"New columns list: {df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacing = df['PACING'].values\n",
    "for i, p in enumerate(pacing):\n",
    "    v, _ = p.split(\"gbit\")\n",
    "    pacing[i] = int(v)\n",
    "\n",
    "df['PACING'] = pacing\n",
    "# df['CONGESTION (Sender)'] = df['CONGESTION (Sender)']==\"cubic\"=1\n",
    "# df['CONGESTION (Sender)'] = df['CONGESTION (Sender)']==\"bbr2\"=0\n",
    "df['CONGESTION (Sender)'] = (df['CONGESTION (Sender)'] == 'cubic').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(style='whitegrid', context='notebook')\n",
    "# cols = ['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS', 'CONGESTION (Sender)', 'PACING']\n",
    "\n",
    "# sns.pairplot(df[cols], height=3)\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig('./figures/scatter.png', dpi=300)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['THROUGHPUT (Sender)', 'LATENCY (mean)', 'RETRANSMITS']].values\n",
    "y = df['PACING'].values\n",
    "y = y.astype('int')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_tune(X, y, model, parameters, scoring='f1_macro', kfold=5, verbose=0):\n",
    "    \"\"\"\n",
    "    X:          array-like of shape (n_samples, n_features)\n",
    "    y:          array-like of shape (n_samples,)\n",
    "    model:      (object) a sklearn model class\n",
    "    parameters: (dict) contains the parameters you want to tune in the model\n",
    "    metric:     (str) the metric used to evaluate the quality of the model\n",
    "    return:     a trained model with the best parameters\n",
    "    \"\"\"\n",
    "    cvSearchObj = GridSearchCV(model,\n",
    "                               parameters,\n",
    "                               scoring=scoring,\n",
    "                               n_jobs=-1,\n",
    "                               cv=kfold,\n",
    "                               verbose=verbose)\n",
    "    cvSearchObj.fit(X,y)\n",
    "    return cvSearchObj.best_estimator_\n",
    "\n",
    "def save_model(filename, model):\n",
    "    \"\"\"\n",
    "    filename: Filename to save the model\n",
    "    model:    Model weights to be saved\n",
    "    \"\"\"\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    print(\"Model Saved\")\n",
    "\n",
    "def load_model(filename):\n",
    "    \"\"\"\n",
    "    filename: Filename to load the model\n",
    "    return:   Model weights that are reloaded\n",
    "    \"\"\"\n",
    "    model_reloaded = pickle.load(open(filename, 'rb'))\n",
    "    return model_reloaded\n",
    "\n",
    "\n",
    "def MLP(train, save, test):\n",
    "    filename = \"./mlpBest.pkl\"\n",
    "    mlp = neural_network.MLPClassifier(random_state=999)\n",
    "    if train:\n",
    "        '''\n",
    "        Train\n",
    "        '''\n",
    "        params = {\"alpha\" : [0.0001],\n",
    "                \"learning_rate_init\" : [0.001],\n",
    "                \"batch_size\" : [32, 64, 128],\n",
    "                \"activation\" : [\"relu\"],\n",
    "                \"early_stopping\" : [True],\n",
    "                \"hidden_layer_sizes\" : [10, 50, 100],\n",
    "                }\n",
    "\n",
    "        mlpBest = train_and_tune(X, y,\n",
    "                                 mlp,\n",
    "                                 params,\n",
    "                                 scoring='f1_macro',\n",
    "                                 kfold=5)\n",
    "\n",
    "        if save:\n",
    "            save_model(filename, mlpBest)\n",
    "\n",
    "    if test:\n",
    "        '''\n",
    "        Test\n",
    "        '''\n",
    "        mlpBest_reloaded = load_model(filename)\n",
    "        pred = mlpBest_reloaded.predict(X)\n",
    "        acc  = mlpBest_reloaded.score(X, y)\n",
    "        \n",
    "        # cf_matrix = confusion_matrix(y, pred)\n",
    "        # df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in classes],\n",
    "        #                      columns = [i for i in classes])\n",
    "        # plt.figure(figsize = (12,10))\n",
    "        # sn.heatmap(df_cm, annot=True)\n",
    "        \n",
    "        print(\"Accuracy: \", acc)\n",
    "\n",
    "MLP(train=True, save=True, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "# le.transform(['M', 'B'])\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print('Test Accuracy: %.3f' % model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}