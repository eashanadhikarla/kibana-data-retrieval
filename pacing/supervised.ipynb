{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "# --- System ---\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# --- Utility ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# --- Plot ---\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Pytorch ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from lib.dataloader import PacingDataset\n",
    "from lib.classifier import PacingClassifier\n",
    "import lib.utils\n",
    "\n",
    "# random weight initialization\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()\n",
    "# ----------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "root_dir = os.getcwd()\n",
    "\n",
    "# data loading and preprocessing\n",
    "dataPath = \"data/statistics-5.csv\"\n",
    "df = pd.read_csv(dataPath)\n",
    "# ----------------------------------\n",
    "# Dropping columns that are not required at the moment\n",
    "df = df.drop(columns=['Unnamed: 0', 'UUID', 'HOSTNAME', 'TIMESTAMP', 'THROUGHPUT (Receiver)', 'LATENCY (mean)', 'CONGESTION (Receiver)', 'BYTES (Receiver)'])\n",
    "\n",
    "# Pre-processing\n",
    "pacing = df['PACING'].values\n",
    "for i, p in enumerate(pacing):\n",
    "    v, _ = p.split(\"gbit\")\n",
    "    pacing[i] = float(v) # int(v)\n",
    "\n",
    "df['PACING'] = pacing\n",
    "df['CONGESTION (Sender)'] = (df['CONGESTION (Sender)'] == 'cubic').astype(int)\n",
    "df['ALIAS'] = pd.factorize(df['ALIAS'])[0]\n",
    "\n",
    "num_of_classes = len(df['PACING'].unique())\n",
    "\n",
    "X = df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']].values\n",
    "y = df['PACING'].values\n",
    "y = y.astype('int')\n",
    "\n",
    "# Normalization\n",
    "minmax_scale = preprocessing.MinMaxScaler().fit(df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']])\n",
    "df_minmax = minmax_scale.transform(df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']])\n",
    "\n",
    "final_df = pd.DataFrame(df_minmax, columns=['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS'])\n",
    "X = final_df[['THROUGHPUT (Sender)', 'LATENCY (min.)', 'LATENCY (max.)', 'RETRANSMITS', 'STREAMS', 'CONGESTION (Sender)', 'ALIAS']].values\n",
    "# ----------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=1)\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "X_test  = torch.tensor(X_test)\n",
    "y_test  = torch.tensor(y_test)\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCH = 1000\n",
    "BATCH = 512\n",
    "LEARNING_RATE = 0.05\n",
    "\n",
    "INTERVAL = 50\n",
    "SAVE = False\n",
    "BESTLOSS = 10\n",
    "\n",
    "CE  = nn.CrossEntropyLoss()\n",
    "BCE = nn.BCELoss(reduction='mean')\n",
    "MSE = nn.MSELoss(reduction='mean') # 'mean', 'sum'. 'none'\n",
    "\n",
    "# Dataset w/o any tranformations\n",
    "traindata   = PacingDataset(tensors=(X_train, y_train), transform=None)\n",
    "trainloader = torch.utils.data.DataLoader(traindata, batch_size=BATCH)\n",
    "\n",
    "testdata    = PacingDataset(tensors=(X_test, y_test), transform=None)\n",
    "testloader = torch.utils.data.DataLoader(testdata, batch_size=1) # BATCH)\n",
    "\n",
    "inputFea = len(traindata[0][0])\n",
    "model = PacingClassifier (nc=num_of_classes, inputFeatures=inputFea)\n",
    "print(model)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "print(\"\\nBatch Size = %3d \" % BATCH)\n",
    "print(\"Loss = \" + str(CE))\n",
    "print(\"Optimizer = SGD\")\n",
    "print(\"Max Epochs = %3d \" % EPOCH)\n",
    "print(\"Learning Rate = %0.3f \" % LEARNING_RATE)\n",
    "print(\"Number of Classes = %d \" % num_of_classes)\n",
    "\n",
    "print(\"\\nStarting training with saved checkpoints\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(0, EPOCH):\n",
    "    torch.manual_seed(epoch+1) # recovery reproducibility\n",
    "    epoch_loss = 0             # for one full epoch\n",
    "\n",
    "    for (batch_idx, batch) in enumerate(trainloader):\n",
    "        (xs, ys) = batch                # (predictors, targets)\n",
    "        xs, ys = xs.float(), ys.float()\n",
    "        optimizer.zero_grad()           # prepare gradients\n",
    "\n",
    "        output = model(xs)              # predicted pacing rate\n",
    "        loss = CE(output, ys.long())    # avg per item in batch\n",
    "\n",
    "        epoch_loss += loss.item()       # accumulate averages\n",
    "        loss.backward()                 # compute gradients\n",
    "        optimizer.step()                # update weights\n",
    "\n",
    "    if epoch % INTERVAL == 0:\n",
    "        print(\"Epoch = %4d    Loss = %0.4f\" % (epoch, epoch_loss))\n",
    "\n",
    "        # save checkpoint\n",
    "        dt = time.strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "        fn = str(dt) + str(\"-\") + str(epoch) + \"_ckpt.pt\"\n",
    "\n",
    "        info_dict = {\n",
    "            'epoch' : epoch,\n",
    "            'model_state' : model.state_dict(),\n",
    "            'optimizer_state' : optimizer.state_dict()\n",
    "        }\n",
    "        if SAVE:\n",
    "            torch.save(info_dict, fn)\n",
    "\n",
    "print(\"\\nDone\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PacingClassifier(\n",
      "  (fc1): Linear(in_features=7, out_features=32, bias=True)\n",
      "  (drop1): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (drop2): Dropout(p=0.7, inplace=False)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (drop3): Dropout(p=0.7, inplace=False)\n",
      "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (drop4): Dropout(p=0.7, inplace=False)\n",
      "  (fc5): Linear(in_features=32, out_features=21, bias=True)\n",
      ")\n",
      "\n",
      "Batch Size = 512 \n",
      "Loss = CrossEntropyLoss()\n",
      "Optimizer = SGD\n",
      "Max Epochs = 1000 \n",
      "Learning Rate = 0.050 \n",
      "Number of Classes = 21 \n",
      "\n",
      "Starting training with saved checkpoints\n",
      "Epoch =    0    Loss = 24.5920\n",
      "Epoch =   50    Loss = 17.9660\n",
      "Epoch =  100    Loss = 16.5130\n",
      "Epoch =  150    Loss = 16.1677\n",
      "Epoch =  200    Loss = 15.8473\n",
      "Epoch =  250    Loss = 15.7375\n",
      "Epoch =  300    Loss = 15.4794\n",
      "Epoch =  350    Loss = 15.6110\n",
      "Epoch =  400    Loss = 15.4521\n",
      "Epoch =  450    Loss = 15.5056\n",
      "Epoch =  500    Loss = 15.3059\n",
      "Epoch =  550    Loss = 15.4462\n",
      "Epoch =  600    Loss = 15.3075\n",
      "Epoch =  650    Loss = 15.2869\n",
      "Epoch =  700    Loss = 15.2204\n",
      "Epoch =  750    Loss = 15.2463\n",
      "Epoch =  800    Loss = 15.2868\n",
      "Epoch =  850    Loss = 15.1561\n",
      "Epoch =  900    Loss = 15.1532\n",
      "Epoch =  950    Loss = 15.1698\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "correct, acc, total = 0, 0, 0\n",
    "with torch.no_grad():\n",
    "    for xs, ys in testloader:\n",
    "        xs, ys = xs.float(), ys.long()\n",
    "\n",
    "        output = model(xs)\n",
    "        \n",
    "        mse_loss = MSE(ys, output)\n",
    "        bce_loss = BCE(recon, xs)\n",
    "        loss = criterion(bce_loss, mu, log_var) + mse_loss\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total += ys.size(0)\n",
    "        pred = torch.max(output, 1)[1]\n",
    "        correct += (pred == ys).sum().item()\n",
    "    acc = (100 * correct / total)\n",
    "print(acc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('bbrv2': conda)"
  },
  "interpreter": {
   "hash": "f6e2000e74d83ab38a7a73c8353776f8595191be383670aed524d2a15b88663d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}